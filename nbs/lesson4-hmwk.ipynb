{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration: Embeddings\n",
    "\n",
    "## Lesson 4 Homework Assignment\n",
    "\n",
    "MovieLens dataset: https://grouplens.org/datasets/movielens/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "LESSON_HOME_DIR = current_dir + '/'\n",
    "DATA_HOME_DIR = LESSON_HOME_DIR + 'data/'\n",
    "\n",
    "#DATASET_DIR = DATA_HOME_DIR + 'ml-20m/'\n",
    "DATASET_DIR = DATA_HOME_DIR + 'ml-small/'\n",
    "MODEL_DIR = DATASET_DIR + 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(DATASET_DIR):\n",
    "    %cd $DATA_HOME_DIR\n",
    "    #!wget http://files.grouplens.org/datasets/movielens/ml-20m.zip\n",
    "    #!unzip ml-20m.zip\n",
    "    !wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "    !unzip ml-latest-small.zip && mv ml-latest-small ml-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_DIR): os.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1260759144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1029</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1061</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1129</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1260759185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1260759205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1       31     2.5  1260759144\n",
       "1       1     1029     3.0  1260759179\n",
       "2       1     1061     3.0  1260759182\n",
       "3       1     1129     2.0  1260759185\n",
       "4       1     1172     4.0  1260759205"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv(DATASET_DIR+'ratings.csv')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100004"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie names for a more user-friendly display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_names = pd.read_csv(DATASET_DIR+'movies.csv').set_index('movieId')['title'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = ratings.userId.unique()\n",
    "movies = ratings.movieId.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update movie and user ids so that they are contiguous integers, which we want when using embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userid2idx = {o:i for i,o in enumerate(users)}\n",
    "movieid2idx = {o:i for i,o in enumerate(movies)}\n",
    "\n",
    "ratings.movieId = ratings.movieId.apply(lambda x: movieid2idx[x])\n",
    "ratings.userId = ratings.userId.apply(lambda x: userid2idx[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 670, 0, 9065)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_min, user_max, movie_min, movie_max = (ratings.userId.min(), \n",
    "    ratings.userId.max(), ratings.movieId.min(), ratings.movieId.max())\n",
    "\n",
    "user_min, user_max, movie_min, movie_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(671, 9066)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = ratings.userId.nunique()\n",
    "n_movies = ratings.movieId.nunique()\n",
    "\n",
    "n_users, n_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of latent factors in each embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_factors = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slick way to randomly split data into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(ratings)) < 0.8\n",
    "trn = ratings[msk]\n",
    "val = ratings[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Product Model\n",
    "\n",
    "The most basic model is a dot product of a movie embedding and a user embedding. Let's see how well that works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, merge\n",
    "from keras.layers.core import Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_input(name, n_in, n_out, reg):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, W_regularizer=l2(reg))(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bias(inp, n_in):\n",
    "    x = Embedding(n_in, 1, input_length=1)(inp)\n",
    "    return Flatten()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_in, user_embed = embedding_input('user_in', n_users, n_factors, 1e-4)\n",
    "movie_in, movie_embed = embedding_input('movie_in', n_movies, n_factors, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = merge([user_embed, movie_embed], mode='dot')\n",
    "x = Flatten()(x)\n",
    "model = Model([user_in, movie_in], x)\n",
    "model.compile(Adam(0.001), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "user_in (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "movie_in (InputLayer)            (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1, 50)         33550       user_in[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 1, 50)         453300      movie_in[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 1, 1)          0           embedding_1[0][0]                \n",
      "                                                                   embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 1)             0           merge_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 486850\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010000000474974513"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr.get_value().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79956 samples, validate on 20048 samples\n",
      "Epoch 1/1\n",
      "79956/79956 [==============================] - 8s - loss: 10.0821 - val_loss: 3.8858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f24339e9dd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.userId, trn.movieId], trn.rating, batch_size=batch_size, nb_epoch=1, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's track predictions as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.9463322162628174]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = np.squeeze(model.predict([np.array([3]), np.array([6])])).item()\n",
    "\n",
    "predictions.append(predict)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79956 samples, validate on 20048 samples\n",
      "Epoch 1/3\n",
      "79956/79956 [==============================] - 8s - loss: 3.1649 - val_loss: 2.0108\n",
      "Epoch 2/3\n",
      "79956/79956 [==============================] - 8s - loss: 2.3998 - val_loss: 1.6374\n",
      "Epoch 3/3\n",
      "79956/79956 [==============================] - 8s - loss: 2.2209 - val_loss: 1.5198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2432e98f10>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.userId, trn.movieId], trn.rating, batch_size=batch_size, nb_epoch=3, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.9463322162628174, 4.9115986824035645]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = np.squeeze(model.predict([np.array([3]), np.array([6])])).item()\n",
    "\n",
    "predictions.append(predict)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79956 samples, validate on 20048 samples\n",
      "Epoch 1/6\n",
      "79956/79956 [==============================] - 8s - loss: 2.1537 - val_loss: 1.4681\n",
      "Epoch 2/6\n",
      "79956/79956 [==============================] - 8s - loss: 2.1172 - val_loss: 1.4442\n",
      "Epoch 3/6\n",
      "79956/79956 [==============================] - 8s - loss: 2.0909 - val_loss: 1.4412\n",
      "Epoch 4/6\n",
      "79956/79956 [==============================] - 8s - loss: 2.0680 - val_loss: 1.4319\n",
      "Epoch 5/6\n",
      "79956/79956 [==============================] - 8s - loss: 2.0480 - val_loss: 1.4287\n",
      "Epoch 6/6\n",
      "79956/79956 [==============================] - 8s - loss: 2.0286 - val_loss: 1.4311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2432e98f90>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.userId, trn.movieId], trn.rating, batch_size=batch_size, nb_epoch=6, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.9463322162628174, 4.9115986824035645, 4.935777187347412]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = np.squeeze(model.predict([np.array([3]), np.array([6])])).item()\n",
    "\n",
    "predictions.append(predict)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the course, the best benchmarks for `loss` are a bit over `0.9`, so this model doesn't seem to be working that well..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Bias\n",
    "\n",
    "Bias represents how positive or negative each user is, and how good each movie is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_in, user_embed = embedding_input('user_in', n_users, n_factors, 1e-4)\n",
    "movie_in, movie_embed = embedding_input('movie_in', n_movies, n_factors, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_bias = create_bias(user_in, n_users)\n",
    "movie_bias = create_bias(movie_in, n_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = merge([user_embed, movie_embed], mode='dot')\n",
    "x = Flatten()(x)\n",
    "x = merge([x, user_bias], mode='sum')\n",
    "x = merge([x, movie_bias], mode='sum')\n",
    "model = Model([user_in, movie_in], x)\n",
    "model.compile(Adam(0.001), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79956 samples, validate on 20048 samples\n",
      "Epoch 1/1\n",
      "79956/79956 [==============================] - 7s - loss: 2.9731 - val_loss: 1.7551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2431424f50>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.userId, trn.movieId], trn.rating, batch_size=batch_size, nb_epoch=1, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.878170013427734]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = np.squeeze(model.predict([np.array([3]), np.array([6])])).item()\n",
    "\n",
    "predictions.append(predict)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79956 samples, validate on 20048 samples\n",
      "Epoch 1/3\n",
      "79956/79956 [==============================] - 7s - loss: 2.1224 - val_loss: 1.4095\n",
      "Epoch 2/3\n",
      "79956/79956 [==============================] - 7s - loss: 1.8755 - val_loss: 1.2856\n",
      "Epoch 3/3\n",
      "79956/79956 [==============================] - 7s - loss: 1.7565 - val_loss: 1.2169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2431043b90>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.userId, trn.movieId], trn.rating, batch_size=batch_size, nb_epoch=3, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.878170013427734, 4.722662925720215]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = np.squeeze(model.predict([np.array([3]), np.array([6])])).item()\n",
    "\n",
    "predictions.append(predict)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79956 samples, validate on 20048 samples\n",
      "Epoch 1/6\n",
      "79956/79956 [==============================] - 7s - loss: 1.6706 - val_loss: 1.1755\n",
      "Epoch 2/6\n",
      "79956/79956 [==============================] - 7s - loss: 1.5994 - val_loss: 1.1403\n",
      "Epoch 3/6\n",
      "79956/79956 [==============================] - 7s - loss: 1.5308 - val_loss: 1.1092\n",
      "Epoch 4/6\n",
      "79956/79956 [==============================] - 7s - loss: 1.4675 - val_loss: 1.0785\n",
      "Epoch 5/6\n",
      "79956/79956 [==============================] - 7s - loss: 1.4039 - val_loss: 1.0499\n",
      "Epoch 6/6\n",
      "79956/79956 [==============================] - 7s - loss: 1.3419 - val_loss: 1.0269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2432e08290>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.userId, trn.movieId], trn.rating, batch_size=batch_size, nb_epoch=6, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.878170013427734, 4.722662925720215, 4.69352912902832]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = np.squeeze(model.predict([np.array([3]), np.array([6])])).item()\n",
    "\n",
    "predictions.append(predict)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With bias, we arrive at an upper-4 rating after the very first iteration (took more epochs without bias).\n",
    "\n",
    "Loss is also lower after the equivalent epochs cycle. So let's keep training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79956 samples, validate on 20048 samples\n",
      "Epoch 1/10\n",
      "79956/79956 [==============================] - 7s - loss: 1.2839 - val_loss: 1.0017\n",
      "Epoch 2/10\n",
      "79956/79956 [==============================] - 7s - loss: 1.2260 - val_loss: 0.9776\n",
      "Epoch 3/10\n",
      "79956/79956 [==============================] - 7s - loss: 1.1707 - val_loss: 0.9560\n",
      "Epoch 4/10\n",
      "79956/79956 [==============================] - 7s - loss: 1.1173 - val_loss: 0.9359\n",
      "Epoch 5/10\n",
      "79956/79956 [==============================] - 7s - loss: 1.0662 - val_loss: 0.9193\n",
      "Epoch 6/10\n",
      "79956/79956 [==============================] - 7s - loss: 1.0170 - val_loss: 0.9030\n",
      "Epoch 7/10\n",
      "79956/79956 [==============================] - 7s - loss: 0.9703 - val_loss: 0.8883\n",
      "Epoch 8/10\n",
      "79956/79956 [==============================] - 7s - loss: 0.9263 - val_loss: 0.8724\n",
      "Epoch 9/10\n",
      "79956/79956 [==============================] - 7s - loss: 0.8845 - val_loss: 0.8580\n",
      "Epoch 10/10\n",
      "79956/79956 [==============================] - 7s - loss: 0.8454 - val_loss: 0.8478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2431043e50>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.userId, trn.movieId], trn.rating, batch_size=batch_size, nb_epoch=10, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.878170013427734, 4.722662925720215, 4.69352912902832, 4.727512836456299]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = np.squeeze(model.predict([np.array([3]), np.array([6])])).item()\n",
    "\n",
    "predictions.append(predict)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79956 samples, validate on 20048 samples\n",
      "Epoch 1/5\n",
      "79956/79956 [==============================] - 7s - loss: 0.8089 - val_loss: 0.8366\n",
      "Epoch 2/5\n",
      "79956/79956 [==============================] - 7s - loss: 0.7749 - val_loss: 0.8281\n",
      "Epoch 3/5\n",
      "79956/79956 [==============================] - 7s - loss: 0.7441 - val_loss: 0.8198\n",
      "Epoch 4/5\n",
      "79956/79956 [==============================] - 7s - loss: 0.7157 - val_loss: 0.8124\n",
      "Epoch 5/5\n",
      "79956/79956 [==============================] - 7s - loss: 0.6897 - val_loss: 0.8065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2431a2a310>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.userId, trn.movieId], trn.rating, batch_size=64, nb_epoch=5, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.878170013427734,\n",
       " 4.722662925720215,\n",
       " 4.69352912902832,\n",
       " 4.727512836456299,\n",
       " 4.831404685974121]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = np.squeeze(model.predict([np.array([3]), np.array([6])])).item()\n",
    "\n",
    "predictions.append(predict)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is now quite a bit better than the `0.9` benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_DIR + 'bias.h5'):\n",
    "    model.save_weights(MODEL_DIR + 'bias.h5')\n",
    "model.load_weights(MODEL_DIR + 'bias.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network Model\n",
    "\n",
    "Rather than creating a special purpose architecture (like our dot-product with bias earlier), it's often both easier and more accurate to use a standard neural network. Let's try it! Here, we simply concatenate the user and movie embeddings into a single vector, which we feed into the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_in, user_embed = embedding_input('user_in', n_users, n_factors, 1e-4)\n",
    "movie_in, movie_embed = embedding_input('movie_in', n_movies, n_factors, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = merge([user_embed, movie_embed], mode='concat')\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(70, activation='relu')(x)\n",
    "x = Dropout(0.75)(x)\n",
    "x = Dense(1)(x)\n",
    "nn = Model([user_in, movie_in], x)\n",
    "nn.compile(Adam(0.001), loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same initial epoch cycle again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79956 samples, validate on 20048 samples\n",
      "Epoch 1/1\n",
      "79956/79956 [==============================] - 7s - loss: 0.6669 - val_loss: 0.8010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f242d7ab210>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.userId, trn.movieId], trn.rating, batch_size=batch_size, nb_epoch=1, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.839086532592773]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = np.squeeze(model.predict([np.array([3]), np.array([6])])).item()\n",
    "\n",
    "predictions.append(predict)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79956 samples, validate on 20048 samples\n",
      "Epoch 1/3\n",
      "79956/79956 [==============================] - 7s - loss: 0.6465 - val_loss: 0.7969\n",
      "Epoch 2/3\n",
      "79956/79956 [==============================] - 7s - loss: 0.6283 - val_loss: 0.7941\n",
      "Epoch 3/3\n",
      "79956/79956 [==============================] - 7s - loss: 0.6129 - val_loss: 0.7920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2431043e10>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.userId, trn.movieId], trn.rating, batch_size=batch_size, nb_epoch=3, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.839086532592773, 4.8729634284973145]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = np.squeeze(model.predict([np.array([3]), np.array([6])])).item()\n",
    "\n",
    "predictions.append(predict)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79956 samples, validate on 20048 samples\n",
      "Epoch 1/6\n",
      "79956/79956 [==============================] - 7s - loss: 0.5992 - val_loss: 0.7902\n",
      "Epoch 2/6\n",
      "79956/79956 [==============================] - 7s - loss: 0.5874 - val_loss: 0.7891\n",
      "Epoch 3/6\n",
      "79956/79956 [==============================] - 7s - loss: 0.5781 - val_loss: 0.7882\n",
      "Epoch 4/6\n",
      "79956/79956 [==============================] - 7s - loss: 0.5700 - val_loss: 0.7880\n",
      "Epoch 5/6\n",
      "79956/79956 [==============================] - 7s - loss: 0.5633 - val_loss: 0.7886\n",
      "Epoch 6/6\n",
      "79956/79956 [==============================] - 7s - loss: 0.5576 - val_loss: 0.7889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f243316d4d0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.userId, trn.movieId], trn.rating, batch_size=batch_size, nb_epoch=6, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.839086532592773, 4.8729634284973145, 4.852595329284668]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = np.squeeze(model.predict([np.array([3]), np.array([6])])).item()\n",
    "\n",
    "predictions.append(predict)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first epoch loss for the neural net was better than the 25th epoch loss for the dot product with bias model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Factor Analysis\n",
    "\n",
    "...of the top 2000 most popular movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = ratings.groupby('movieId')['rating'].count()\n",
    "topMovies = counts.sort_values(ascending=False)[:2000]\n",
    "topMovies = np.array(topMovies.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 57,  49,  99,  92, 143,  72, 402, 417,  79,  89])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arbitrary indices from counting movies with the most ratings\n",
    "topMovies[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[356, 296, 318, 593, 260, 480, 2571, 1, 527, 589]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MovieLens indices\n",
    "[movies[topMovies[i]] for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Forrest Gump (1994)',\n",
       " 'Pulp Fiction (1994)',\n",
       " 'Shawshank Redemption, The (1994)',\n",
       " 'Silence of the Lambs, The (1991)',\n",
       " 'Star Wars: Episode IV - A New Hope (1977)',\n",
       " 'Jurassic Park (1993)',\n",
       " 'Matrix, The (1999)',\n",
       " 'Toy Story (1995)',\n",
       " \"Schindler's List (1993)\",\n",
       " 'Terminator 2: Judgment Day (1991)']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Movie names\n",
    "[movie_names[movies[topMovies[i]]] for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at the movie embeddings. We create a 'model' - which in keras is simply a way of associating one or more inputs with one more more outputs, using the functional API. Here, our input is the movie id (a single id), and the output is the movie's embedding (an array of 50 latent factors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 50)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_movie_emb = Model(movie_in, movie_embed)\n",
    "movie_emb = np.squeeze(get_movie_emb.predict([topMovies]))\n",
    "movie_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it's hard to interpret 50 latent factors, we use [PCA](https://plot.ly/ipython-notebooks/principal-component-analysis/) to perform dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
