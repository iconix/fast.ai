{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 20 newsgroups topic analysis\n",
    "\n",
    "Instead of repeating the IMDB sentiment analysis from the lesson (because frankly, I'm a little bored with sentiment analysis), I will attempt to apply a similar approach to deep-learning NLP classification to a dataset a coworker has recently been messing around with in `scikit-learn`: `sklearn.datasets.fetch_20newsgroups`.\n",
    "\n",
    "http://people.csail.mit.edu/jrennie/20Newsgroups/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "LESSON_HOME_DIR = current_dir + '/'\n",
    "DATA_HOME_DIR = LESSON_HOME_DIR + 'data/'\n",
    "\n",
    "DATASET_DIR = DATA_HOME_DIR + '20_newsgroup/'\n",
    "MODEL_DIR = DATASET_DIR + 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(DATASET_DIR)\n",
    "    os.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "category_subset = [\n",
    "    'alt.atheism',\n",
    "    'comp.graphics',\n",
    "    'comp.os.ms-windows.misc',\n",
    "    'soc.religion.christian',\n",
    "]\n",
    "\n",
    "x_train = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories = category_subset,\n",
    "    shuffle = True,\n",
    "    remove = ('headers', 'footers', 'quotes'))\n",
    "\n",
    "x_test = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    categories = category_subset,\n",
    "    shuffle = True,\n",
    "    remove = ('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'soc.religion.christian']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`target_names` are as requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2254,), (2254,), 2254)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.filenames.shape, x_train.target.shape, len(x_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500,), (1500,), 1500)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.filenames.shape, x_test.target.shape, len(x_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras implements `get_word_index()` for the IMDB dataset, which returns an dictionary of word->index derived from a json file hosted on Amazon S3.\n",
    "\n",
    "This seems bizarre to me? Anyway, sklearn doesn't do this. So let's create our own index with `keras.preprocessing.text.Tokenizer` (https://keras.io/preprocessing/text/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/unidecode/__init__.py:46: RuntimeWarning: Argument <type 'str'> is not an unicode object. Passing an encoded string will likely have unexpected results.\n",
      "  _warn_if_not_unicode(string)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from unidecode import unidecode\n",
    "\n",
    "train_tokenizer = Tokenizer()\n",
    "unidecoded_x_train = [unidecode(text) for text in x_train.data]\n",
    "train_tokenizer.fit_on_texts(unidecoded_x_train) # builds the word index\n",
    "train_sequences = train_tokenizer.texts_to_sequences(unidecoded_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_word_index = train_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse the `word_index` with `idx2word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_idx2word = {v: k for k, v in train_word_index.iteritems()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first review, both as a list of indices and as text reconstructed from the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6, 47, 1529, 37, 84, 69, 963, 110, 2, 676, 445, 832, 1268, 1135, 198, 72, 445, 832, 8, 736, 450, 7, 6, 95, 189, 3, 28, 3, 1203, 5, 171, 69, 62, 133, 50862, 12, 8, 970, 7537, 4, 117, 1270, 4, 1268, 7, 84, 94, 3755, 18, 109, 236, 26, 542, 29, 206, 244, 117, 69, 4, 134, 176, 213, 199, 16359, 18501, 15497, 14450, 10736, 2404, 144, 35, 15644, 11379, 9545, 2404, 144'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(map(str, train_sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx2word[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i was wondering if any one knew how the various hard drive compression utilities work my hard drive is getting full and i don't want to have to buy a new one what i'm intrested in is speed ease of use amount of compression and any other aspect you think might be important as i've never use one of these things before thanks morgan bullard mb4008 coewl cen uiuc edu or mjbb uxa cso uiuc edu\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([train_idx2word[o] for o in train_sequences[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 'comp.os.ms-windows.misc')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.target[0], x_train.target_names[x_train.target[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce vocab size by setting rare words to max index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, sequence the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tokenizer = Tokenizer()\n",
    "unidecoded_x_test = [unidecode(text) for text in x_test.data]\n",
    "test_tokenizer.fit_on_texts(unidecoded_x_test) # builds the word index\n",
    "test_sequences = test_tokenizer.texts_to_sequences(unidecoded_x_test)\n",
    "test_word_index = test_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab_size = min(len(train_word_index), len(test_word_index))\n",
    "\n",
    "trn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in train_sequences]\n",
    "test = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in test_sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of the lengths of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16306, 0, 289.51863354037266)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array(map(len, trn))\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird that there are sentences with 0 sequences (words) in them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get indices of arrays that do NOT satisfy np.nonzero\n",
    "nonzero_indices = np.unique(np.nonzero(train_sequences)[0])\n",
    "zero_indices = set(range(len(train_sequences))).difference(nonzero_indices)\n",
    "len(zero_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 59 sentences with no words. E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], 'comp.graphics')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences[18], x_train.target_names[x_train.target[18]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove them (and their labels) from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn = np.delete(trn, list(zero_indices), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train.target = np.delete(x_train.target, list(zero_indices), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16306, 1, 297.30068337129842)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array(map(len, trn))\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so apparently there are also reviews with 1 word... we'll assume that's valid for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad (with zero) or truncate each sentence to make consistent length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "seq_len = 1000\n",
    "\n",
    "trn = sequence.pad_sequences(trn, maxlen=seq_len, value=0)\n",
    "test = sequence.pad_sequences(test, maxlen=seq_len, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,  9545,  2404,   144],\n",
       "       [    0,     0,     0, ...,  9485,    16,   546],\n",
       "       [    0,     0,     0, ...,   163,   490,   380],\n",
       "       ..., \n",
       "       [    0,     0,     0, ...,   104, 36013,   103],\n",
       "       [    0,     0,     0, ...,    26,  1263, 12586],\n",
       "       [    0,     0,     0, ...,  5867,  5785,  2465]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's turn the labels into categorical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "x_train.target = to_categorical(np.asarray(x_train.target))\n",
    "x_test.target = to_categorical(np.asarray(x_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2195, 1000), (2195, 4))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.shape, x_train.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 1000), (1500, 4))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape, x_test.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single hidden layer NN\n",
    "\n",
    "The simplest model that tends to give reasonable results is a single hidden layer net. So let's try that. Note that we can't expect to get any useful results by feeding word ids directly into a neural net - so instead we use an embedding to replace them with a vector of 32 (initially random) floats for each word in the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36014, 1000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# input_length => 500-word reviews, 32 floats per word\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(4, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 1000, 32)      1152448     embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 32000)         0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 100)           3200100     flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 100)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 4)             404         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 4352952\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2195 samples, validate on 1500 samples\n",
      "Epoch 1/2\n",
      "2195/2195 [==============================] - 0s - loss: 1.4089 - acc: 0.2856 - val_loss: 1.3732 - val_acc: 0.3227\n",
      "Epoch 2/2\n",
      "2195/2195 [==============================] - 0s - loss: 1.3090 - acc: 0.3595 - val_loss: 1.3583 - val_acc: 0.3460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eff05b27fd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, x_train.target, validation_data=(test, x_test.target), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good? Bad? Here are some accuracies [from an official `sklearn` example](http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html) that classifies documents by topics using a bag-of-words approach:\n",
    "\n",
    "```\n",
    "[('RidgeClassifier', 0.89726533628972649),\n",
    " ('Perceptron', 0.88543976348854403),\n",
    " ('PassiveAggressiveClassifier', 0.90613451589061345),\n",
    " ('KNeighborsClassifier', 0.85809312638580926),\n",
    " ('RandomForestClassifier', 0.83813747228381374),\n",
    " ('LinearSVC', 0.90022172949002222),\n",
    " ('SGDClassifier', 0.90096082779009612),\n",
    " ('LinearSVC', 0.87287509238728755),\n",
    " ('SGDClassifier', 0.88543976348854403),\n",
    " ('SGDClassifier', 0.89874353288987441),\n",
    " ('NearestCentroid', 0.85513673318551364),\n",
    " ('MultinomialNB', 0.90022172949002222),\n",
    " ('BernoulliNB', 0.88396156688839611),\n",
    " ('Pipeline', 0.8810051736881005)]\n",
    " \n",
    " mean: 0.88311688311688319\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... not a good result in comparison with a much simpler approach. Training accuracy is comparable, but testing accuracy is much poorer.\n",
    "\n",
    "It's possible that I'm doing something not-ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single conv layer with max pooling\n",
    "\n",
    "A CNN is likely to work better, since it's designed to take advantage of ordered data. We'll need to use a 1D CNN, since a sequence of words is 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)'''\n",
    "\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "\n",
    "'''conv1 = Sequential([\n",
    "    Embedding(vocab_size, 100, input_length=seq_len),\n",
    "    Convolution1D(128, 5, activation='relu'),\n",
    "    MaxPooling1D(5),\n",
    "    Convolution1D(128, 5, activation='relu'),\n",
    "    MaxPooling1D(5),\n",
    "    Convolution1D(128, 5, activation='relu'),\n",
    "    MaxPooling1D(5),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(4, activation='softmax')\n",
    "    ])'''\n",
    "\n",
    "conv1 = Sequential([\n",
    "    Embedding(vocab_size, 100, input_length=seq_len, dropout=0.4),\n",
    "    Dropout(0.4),\n",
    "    Convolution1D(128, 5, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    MaxPooling1D(5),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(4, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_4 (Embedding)          (None, 1000, 100)     3601400     embedding_input_4[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 1000, 100)     0           embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_2 (Convolution1D)  (None, 996, 128)      64128       dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 996, 128)      0           convolution1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_2 (MaxPooling1D)    (None, 199, 128)      0           dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 25472)         0           maxpooling1d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 128)           3260544     flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 128)           0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 4)             516         dropout_8[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 6926588\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "conv1.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "conv1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010000000474974513"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.optimizer.lr.get_value().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conv1.optimizer.lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2195 samples, validate on 1500 samples\n",
      "Epoch 1/4\n",
      "2195/2195 [==============================] - 3s - loss: 1.3920 - acc: 0.3294 - val_loss: 1.3635 - val_acc: 0.3347\n",
      "Epoch 2/4\n",
      "2195/2195 [==============================] - 3s - loss: 1.3133 - acc: 0.3658 - val_loss: 1.3563 - val_acc: 0.3373\n",
      "Epoch 3/4\n",
      "2195/2195 [==============================] - 3s - loss: 1.1403 - acc: 0.4328 - val_loss: 1.4872 - val_acc: 0.2833\n",
      "Epoch 4/4\n",
      "2195/2195 [==============================] - 3s - loss: 0.9381 - acc: 0.5198 - val_loss: 1.8670 - val_acc: 0.2747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efef5406b50>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.fit(trn, x_train.target, validation_data=(test, x_test.target), nb_epoch=4, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conv1.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2195 samples, validate on 1500 samples\n",
      "Epoch 1/4\n",
      "2195/2195 [==============================] - 3s - loss: 0.8164 - acc: 0.5321 - val_loss: 2.3665 - val_acc: 0.2260\n",
      "Epoch 2/4\n",
      "2195/2195 [==============================] - 3s - loss: 0.7905 - acc: 0.5317 - val_loss: 2.0969 - val_acc: 0.2553\n",
      "Epoch 3/4\n",
      "2195/2195 [==============================] - 3s - loss: 0.7724 - acc: 0.5380 - val_loss: 2.6437 - val_acc: 0.2527\n",
      "Epoch 4/4\n",
      "2195/2195 [==============================] - 3s - loss: 0.7698 - acc: 0.5549 - val_loss: 2.2627 - val_acc: 0.2613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efefb696590>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.fit(trn, x_train.target, validation_data=(test, x_test.target), nb_epoch=4, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
