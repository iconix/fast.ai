{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 20 newsgroups topic analysis\n",
    "\n",
    "Instead of repeating the IMDB sentiment analysis from the lesson (because frankly, I'm a little bored with sentiment analysis), I will attempt to apply a similar approach to deep-learning NLP classification to a dataset a coworker has recently been messing around with in `scikit-learn`: `sklearn.datasets.fetch_20newsgroups`.\n",
    "\n",
    "http://people.csail.mit.edu/jrennie/20Newsgroups/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "LESSON_HOME_DIR = current_dir + '/'\n",
    "DATA_HOME_DIR = LESSON_HOME_DIR + 'data/'\n",
    "\n",
    "DATASET_DIR = DATA_HOME_DIR + '20_newsgroup/'\n",
    "MODEL_DIR = DATASET_DIR + 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(DATASET_DIR)\n",
    "    os.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "category_subset = [\n",
    "    'alt.atheism',\n",
    "    'comp.graphics',\n",
    "    'comp.os.ms-windows.misc',\n",
    "    'soc.religion.christian',\n",
    "]\n",
    "\n",
    "x_train = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories = category_subset,\n",
    "    shuffle = True,\n",
    "    remove = ('headers', 'footers', 'quotes'))\n",
    "\n",
    "x_test = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    categories = category_subset,\n",
    "    shuffle = True,\n",
    "    remove = ('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'soc.religion.christian']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`target_names` are as requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2254,), (2254,), 2254)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.filenames.shape, x_train.target.shape, len(x_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500,), (1500,), 1500)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.filenames.shape, x_test.target.shape, len(x_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras implements `get_word_index()` for the IMDB dataset, which returns an dictionary of word->index derived from a json file hosted on Amazon S3.\n",
    "\n",
    "This seems bizarre to me? Anyway, sklearn doesn't do this. So let's create our own index with `keras.preprocessing.text.Tokenizer` (https://keras.io/preprocessing/text/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import keras.preprocessing.text\n",
    "import string\n",
    "\n",
    "# Workaround to add \"Unicode support for keras.preprocessing.text\"\n",
    "# (https://github.com/fchollet/keras/issues/1072#issuecomment-295470970)\n",
    "def text_to_word_sequence(text,\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True, split=\" \"):\n",
    "    if lower: text = text.lower()\n",
    "    if type(text) == unicode:\n",
    "        translate_table = {ord(c): ord(t) for c,t in zip(filters, split*len(filters)) }\n",
    "    else:\n",
    "        translate_table = string.maketrans(filters, split * len(filters))\n",
    "    text = text.translate(translate_table)\n",
    "    seq = text.split(split)\n",
    "    return [i for i in seq if i]\n",
    "    \n",
    "keras.preprocessing.text.text_to_word_sequence = text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "train_tokenizer = Tokenizer()\n",
    "train_tokenizer.fit_on_texts(x_train.data) # builds the word index\n",
    "train_sequences = train_tokenizer.texts_to_sequences(x_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_word_index = train_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'3ds2scn': 19125,\n",
       " u'wax3': 30528,\n",
       " u'fl46vzjp2': 19127,\n",
       " u'l1tbk': 19128,\n",
       " u'mbhi8bea': 19129,\n",
       " u'circuitry': 19130,\n",
       " u'pantheistic': 19131,\n",
       " u'mdbs': 19132,\n",
       " u'hanging': 8849,\n",
       " u'beqs': 19133,\n",
       " u'sation': 19134,\n",
       " u'disobeying': 13192,\n",
       " u\"'113s1t45\": 19135,\n",
       " u'sisrg': 19136,\n",
       " u\"ng2z'kk\": 19137,\n",
       " u\"ng2z'ki\": 19138,\n",
       " u'4bo0': 13211,\n",
       " u'trojan': 19140,\n",
       " u\"qq'jp\": 19141,\n",
       " u'yourdon': 19142,\n",
       " u'ua8cx': 10454,\n",
       " u'y74': 41966,\n",
       " u\"9l2'\": 19143,\n",
       " u'fractal': 3361,\n",
       " u'rlis': 6209,\n",
       " u'wednesday': 10455,\n",
       " u'woods': 19145,\n",
       " u'598n': 19146,\n",
       " u'amplifications': 19147,\n",
       " u'rlii': 19148,\n",
       " u'v6jylh': 19149,\n",
       " u'rlim': 10456,\n",
       " u'y8z': 19151,\n",
       " u's3u4578': 19152,\n",
       " u'matthean': 19153,\n",
       " u'rll': 12716,\n",
       " u'y8v': 19154,\n",
       " u'znb8flb8': 19155,\n",
       " u'270': 8850,\n",
       " u'271': 13196,\n",
       " u'272': 8851,\n",
       " u'273': 13197,\n",
       " u'274': 19156,\n",
       " u'275': 19157,\n",
       " u'v0z1t': 40355,\n",
       " u'sustaining': 9423,\n",
       " u'7bizw': 19159,\n",
       " u'y8c': 19360,\n",
       " u'targa': 2727,\n",
       " u'y8g': 19160,\n",
       " u'inanimate': 13199,\n",
       " u\"x'28\": 19161,\n",
       " u'errors': 1482,\n",
       " u'cooking': 19162,\n",
       " u'1achov': 19163,\n",
       " u'rle': 2832,\n",
       " u'm2r5a': 19165,\n",
       " u'usenet': 1171,\n",
       " u'1jpt78': 6210,\n",
       " u'designing': 6880,\n",
       " u'televangelists': 13200,\n",
       " u'd7qwz': 19166,\n",
       " u'g3ck': 19167,\n",
       " u'succumb': 19168,\n",
       " u'shocks': 19170,\n",
       " u'evolutionism': 13201,\n",
       " u'widget': 4868,\n",
       " u'27c': 19171,\n",
       " u'chins': 19172,\n",
       " u'27k': 19427,\n",
       " u'y86': 10457,\n",
       " u'27m': 6881,\n",
       " u'y85': 19173,\n",
       " u'hfnevz7': 42059,\n",
       " u'china': 10458,\n",
       " u'27u': 19175,\n",
       " u\"'g2rneu\": 24837,\n",
       " u'affiliated': 10459,\n",
       " u'quark': 47598,\n",
       " u'jjc8v': 19178,\n",
       " u'q6jeri4ku6h': 19179,\n",
       " u'natured': 19180,\n",
       " u'lmh': 34730,\n",
       " u'kids': 2615,\n",
       " u'dialog1': 19182,\n",
       " u'climbed': 19183,\n",
       " u'controversy': 5656,\n",
       " u'natures': 5232,\n",
       " u\"o5j'ax\": 10460,\n",
       " u'1jpt7s': 19184,\n",
       " u'xvauv': 19185,\n",
       " u'golden': 5233,\n",
       " u'topography': 19186,\n",
       " u'9st3qsecs8': 19187,\n",
       " u'projection': 8852,\n",
       " u'b0l47': 19188,\n",
       " u'16w3zvwk16m': 19189,\n",
       " u'lmo': 30541,\n",
       " u'rrrrr': 19191,\n",
       " u'rfc3': 36266,\n",
       " u'stern': 19194,\n",
       " u'satalink': 19195,\n",
       " u'p45': 19196,\n",
       " u'46862': 19197,\n",
       " u'p42': 19198,\n",
       " u'p41': 13203,\n",
       " u'dnd': 13275,\n",
       " u'dna': 8853,\n",
       " u'blashephemers': 13204,\n",
       " u'rmr9r': 19200,\n",
       " u'dnm': 10462,\n",
       " u'dnn': 19201,\n",
       " u'7fg': 19202,\n",
       " u'cannibal': 19203,\n",
       " u'e0': 2231,\n",
       " u'wo8ps': 19204,\n",
       " u'pkjznb8': 19205,\n",
       " u'music': 4274,\n",
       " u'therefore': 421,\n",
       " u'dns': 19206,\n",
       " u'distortions': 13205,\n",
       " u'77ut': 13206,\n",
       " u'sermons': 19207,\n",
       " u\"3l''2tcv9\": 19208,\n",
       " u'benedikt': 7726,\n",
       " u'n7zg': 19209,\n",
       " u'p4h': 19210,\n",
       " u'0rfumrd': 19213,\n",
       " u'mj4832s725jj1ne': 19214,\n",
       " u'j6eg': 19215,\n",
       " u'p4u': 966,\n",
       " u'p4t': 19216,\n",
       " u'unpack': 19217,\n",
       " u'circumstances': 3004,\n",
       " u'ecs1emt': 19218,\n",
       " u\"dn'\": 19219,\n",
       " u'morally': 4869,\n",
       " u'locked': 7727,\n",
       " u'arndt': 19220,\n",
       " u'7i5': 41983,\n",
       " u'dn0': 19222,\n",
       " u'm5junevz77utngvz': 19223,\n",
       " u'ludicrosity': 19224,\n",
       " u'7f7': 19225,\n",
       " u'sed4': 19226,\n",
       " u'cx7qa1f6': 19227,\n",
       " u'wang': 8854,\n",
       " u'unjust': 8855,\n",
       " u'kaprow': 19228,\n",
       " u'0pd': 30398,\n",
       " u'want': 189,\n",
       " u'absolute': 770,\n",
       " u\"'os7'\": 10463,\n",
       " u'6tmuv': 41987,\n",
       " u'csnlluf': 19232,\n",
       " u'whibbard': 47669,\n",
       " u'05lxm34': 19234,\n",
       " u'travel': 5651,\n",
       " u'constantinopolitan': 9754,\n",
       " u'ppvbud9': 19235,\n",
       " u'r3000': 10464,\n",
       " u'playback': 13210,\n",
       " u'grpxjq': 19237,\n",
       " u'cadence': 19239,\n",
       " u'miij': 19240,\n",
       " u'assimilated': 19241,\n",
       " u't45st44': 30546,\n",
       " u'apocrypha': 8856,\n",
       " u'2dm7jc': 36277,\n",
       " u'wrong': 297,\n",
       " u'puorpl': 19242,\n",
       " u'perutz': 19243,\n",
       " u'hzefw': 19893,\n",
       " u'vbto8': 19245,\n",
       " u'o569': 19246,\n",
       " u'ay79ug': 19247,\n",
       " u'qadian': 19248,\n",
       " u'sociologist': 41990,\n",
       " u'3isrg': 19250,\n",
       " u'menlo': 10465,\n",
       " u'fymkfy1': 19251,\n",
       " u\"effect'\": 19252,\n",
       " u'9xm3l': 19254,\n",
       " u'm89jgdy5': 19255,\n",
       " u'faddishness': 19256,\n",
       " u'welcomed': 13212,\n",
       " u'kuee': 19257,\n",
       " u'621': 19258,\n",
       " u\"'afdu\": 19259,\n",
       " u'zdz8': 19260,\n",
       " u'backings': 19261,\n",
       " u'rewarded': 13213,\n",
       " u'activating': 19262,\n",
       " u'74017': 36283,\n",
       " u'fir': 19264,\n",
       " u'uop': 19265,\n",
       " u'fiv': 10466,\n",
       " u'uov': 20006,\n",
       " u'fit': 1104,\n",
       " u'fiu': 19266,\n",
       " u'bringing': 4867,\n",
       " u'fix': 2032,\n",
       " u'uox': 19267,\n",
       " u'discourse': 8463,\n",
       " u'9we': 19269,\n",
       " u'fig': 7728,\n",
       " u'fij': 613,\n",
       " u'fik': 6899,\n",
       " u'fih': 7729,\n",
       " u'fii': 13352,\n",
       " u'u68by': 20044,\n",
       " u'fil': 19272,\n",
       " u'fim': 5665,\n",
       " u'fjby': 19274,\n",
       " u'vnzs6jd': 19275,\n",
       " u'urmq': 19276,\n",
       " u'urmw': 7730,\n",
       " u'urmt': 19277,\n",
       " u'recollections': 36286,\n",
       " u'effects': 2728,\n",
       " u'4pep': 19278,\n",
       " u'8afi89': 19279,\n",
       " u'multidimensional': 19280,\n",
       " u'sixteen': 13215,\n",
       " u'saddened': 19281,\n",
       " u'feid3ive3': 19282,\n",
       " u'fi2': 19283,\n",
       " u'rasterized': 7756,\n",
       " u'viaticum': 20101,\n",
       " u'fi5': 19285,\n",
       " u\"6doqvg'd3ii\": 13217,\n",
       " u'pzszb': 19286,\n",
       " u\"fi'\": 13218,\n",
       " u'arrow': 8894,\n",
       " u'etflkhfnkhf': 19288,\n",
       " u'55l34u': 19290,\n",
       " u'telescope': 10467,\n",
       " u'6q3': 19291,\n",
       " u'oi9d69': 19292,\n",
       " u'allah': 2389,\n",
       " u'parasites': 13372,\n",
       " u'393': 19294,\n",
       " u'390': 13220,\n",
       " u'397': 19295,\n",
       " u'fw8rlk': 19296,\n",
       " u'394': 13221,\n",
       " u'emunix': 19297,\n",
       " u'398': 19298,\n",
       " u'phas': 31117,\n",
       " u'uqei': 13222,\n",
       " u'golen': 19300,\n",
       " u'enviroment': 13223,\n",
       " u'gradation': 13224,\n",
       " u'mason': 13225,\n",
       " u'9f9f9': 3308,\n",
       " u'encourage': 3362,\n",
       " u'adapt': 17443,\n",
       " u'qkgol8a': 19304,\n",
       " u'aravx': 19305,\n",
       " u'misrepresentations': 19306,\n",
       " u'hxvf': 19307,\n",
       " u'sheep': 8064,\n",
       " u'39a': 19309,\n",
       " u'vd206': 19310,\n",
       " u'strath': 19311,\n",
       " u'39i': 13226,\n",
       " u'underfoot': 19312,\n",
       " u'39n': 13227,\n",
       " u'6qj': 19313,\n",
       " u'corrects': 20275,\n",
       " u'6ql': 3005,\n",
       " u'xyfgaj': 19315,\n",
       " u'7kljw': 13228,\n",
       " u'universally': 6882,\n",
       " u'syucgyx': 19316,\n",
       " u'husbands': 10468,\n",
       " u'9f9f9f9f9f9f9f9f9f9f9f9f9': 19317,\n",
       " u'chv4': 19318,\n",
       " u'ministries': 13229,\n",
       " u'disturbed': 13230,\n",
       " u'90ce': 19319,\n",
       " u'g9v3': 19320,\n",
       " u'g9v1': 10469,\n",
       " u'g9v4': 19321,\n",
       " u'kfq': 19322,\n",
       " u'pwhite': 13231,\n",
       " u'g9v8': 19323,\n",
       " u'g9v9': 20359,\n",
       " u\"y'p8\": 45931,\n",
       " u'maladies': 19325,\n",
       " u'yfv8': 19326,\n",
       " u'kff': 19327,\n",
       " u'24epul4': 19328,\n",
       " u'75vznkh': 19329,\n",
       " u\"g9v'\": 13232,\n",
       " u'megabytes': 8858,\n",
       " u'iycs': 19330,\n",
       " u'ua4': 47050,\n",
       " u'hme': 52372,\n",
       " u'q07': 19331,\n",
       " u'q04': 19332,\n",
       " u'wo47gph': 19333,\n",
       " u'service': 1554,\n",
       " u'kortink': 19334,\n",
       " u'lhzrlk8v': 19335,\n",
       " u'ry509y': 19336,\n",
       " u'needed': 1438,\n",
       " u'master': 4534,\n",
       " u'u0gujbv': 19337,\n",
       " u'genesis': 2033,\n",
       " u'c8emaselis': 19338,\n",
       " u'hiye': 19339,\n",
       " u'kf1': 13233,\n",
       " u\"adobe's\": 10470,\n",
       " u'ca85': 10471,\n",
       " u'q0x': 13234,\n",
       " u'g9va': 19340,\n",
       " u\"zri'\": 13434,\n",
       " u'g9ve': 13235,\n",
       " u'zcpajp9': 19342,\n",
       " u'inetnet': 19343,\n",
       " u'g9vo': 13236,\n",
       " u'positively': 19344,\n",
       " u'idlw': 19345,\n",
       " u\"m3j2b'qqfb\": 19346,\n",
       " u'qojjr': 19347,\n",
       " u'mekozdj': 13237,\n",
       " u\"'writing\": 19348,\n",
       " u'idly': 19349,\n",
       " u'nzc': 10472,\n",
       " u'idle': 13238,\n",
       " u\"havne't\": 19351,\n",
       " u'u2ku': 19352,\n",
       " u'bgogbhj': 19353,\n",
       " u'feeling': 2616,\n",
       " u'uorplq': 13239,\n",
       " u'8ha35': 19355,\n",
       " u'7ha1l': 47688,\n",
       " u'z38lb': 19356,\n",
       " u'lkhfnkjznk': 51897,\n",
       " u\"'e2\": 15219,\n",
       " u'chormc': 13241,\n",
       " u'sacco': 24872,\n",
       " u'277': 10473,\n",
       " u'spectrum': 4275,\n",
       " u'macine': 30565,\n",
       " u'279': 13198,\n",
       " u'deamon': 19361,\n",
       " u'dozen': 7732,\n",
       " u'vgl': 19362,\n",
       " u'affairs': 5234,\n",
       " u'vgn': 19363,\n",
       " u'o2y': 31520,\n",
       " u'mostly': 1930,\n",
       " u'vga': 873,\n",
       " u'9j8o0': 19365,\n",
       " u'vgx': 10474,\n",
       " u'4feg': 24973,\n",
       " u'l2s1': 51174,\n",
       " u'rhcp': 19366,\n",
       " u'shipments': 19367,\n",
       " u'committing': 6211,\n",
       " u'diminishing': 19368,\n",
       " u'metrics': 13242,\n",
       " u'simplify': 8913,\n",
       " u'divbdi': 19370,\n",
       " u'mouth': 2617,\n",
       " u'reverence': 8859,\n",
       " u'visualizations': 24876,\n",
       " u'8wjzous': 19371,\n",
       " u'1emw': 19372,\n",
       " u'expound': 19373,\n",
       " u'singer': 10475,\n",
       " u'6tm7vh': 19374,\n",
       " u'1urc': 19375,\n",
       " u'purges': 19376,\n",
       " u\"vg'\": 8860,\n",
       " u'vertices': 4007,\n",
       " u'dw4go': 19377,\n",
       " u'tech': 4883,\n",
       " u'cqdwj': 19379,\n",
       " u'vg4': 19380,\n",
       " u'scream': 10476,\n",
       " u'vg2': 19381,\n",
       " u'saying': 413,\n",
       " u'ylpryg': 19382,\n",
       " u'4bc76': 19383,\n",
       " u\"'82lk\": 50271,\n",
       " u'tempter': 19385,\n",
       " u'padded': 13244,\n",
       " u'nsqjd0': 19387,\n",
       " u'purccvm': 45401,\n",
       " u'ysag': 13245,\n",
       " u'tempted': 5652,\n",
       " u'cheaply': 19388,\n",
       " u'clicked': 19389,\n",
       " u\"'ou\": 16926,\n",
       " u'bonuses': 19390,\n",
       " u'rico': 19391,\n",
       " u'plqs4u': 19392,\n",
       " u'bliss': 19393,\n",
       " u'rick': 8861,\n",
       " u'rich': 4871,\n",
       " u\"5'wwiz\": 13246,\n",
       " u'gka': 52082,\n",
       " u'rice': 8862,\n",
       " u'ivafmy0k2475': 19394,\n",
       " u\"mqrs'oqxuqrp\": 19395,\n",
       " u\"eia9d43'dx0\": 19396,\n",
       " u't3s1t98': 49080,\n",
       " u\"p'5uv9z'ax\": 19398,\n",
       " u'plato': 19399,\n",
       " u'61dsg': 19400,\n",
       " u'iu824e6v': 42016,\n",
       " u'numeram': 19402,\n",
       " u'theodosios': 19403,\n",
       " u'altogether': 5653,\n",
       " u'2f8c': 19404,\n",
       " u'jaguar': 19405,\n",
       " u'distracting': 42018,\n",
       " u'nicely': 5235,\n",
       " u\"'aq046um\": 10478,\n",
       " u'patch': 3006,\n",
       " u\"th'other\": 19407,\n",
       " u'i098': 19408,\n",
       " u'ecr613s8': 19409,\n",
       " u'ddraf': 19410,\n",
       " u'6eh45u': 19411,\n",
       " u'clarified': 10479,\n",
       " u'sensitivity': 10480,\n",
       " u\"5wn'f\": 19412,\n",
       " u'tyndale': 19413,\n",
       " u'irx': 19414,\n",
       " u'10megs': 19415,\n",
       " u'lots': 1653,\n",
       " u'irq': 4872,\n",
       " u'irs': 19417,\n",
       " u'fymmr': 21009,\n",
       " u'xvi': 19418,\n",
       " u'iri': 19419,\n",
       " u'xvk': 19420,\n",
       " u'irk': 19421,\n",
       " u'irm': 19422,\n",
       " u'mffc1ja': 19423,\n",
       " u'irb': 15873,\n",
       " u'irc': 19425,\n",
       " u'xvf': 19426,\n",
       " u'wage': 17600,\n",
       " u'extend': 6884,\n",
       " u'nature': 492,\n",
       " u'optimist': 15221,\n",
       " u'superficial': 17163,\n",
       " u'brainwashed': 13248,\n",
       " u'extent': 2618,\n",
       " u'accessed': 7599,\n",
       " u'hdvz': 19429,\n",
       " u'mxte': 5805,\n",
       " u'tyranny': 10481,\n",
       " u'xv8': 19431,\n",
       " u'zrmas': 13250,\n",
       " u'ir1': 19432,\n",
       " u'ir3': 19433,\n",
       " u'heating': 19434,\n",
       " u'incense': 21126,\n",
       " u\"jbpjd'p8i\": 19436,\n",
       " u'sublime': 30581,\n",
       " u'melinda': 24892,\n",
       " u'mt471144': 19440,\n",
       " u\"'or\": 45807,\n",
       " u\"xv'\": 19441,\n",
       " u\"o'yre\": 19442,\n",
       " u'c803k': 13251,\n",
       " u\"r4'\": 19443,\n",
       " u'75u1f4': 32878,\n",
       " u'6w99sed7': 19445,\n",
       " u'bxesd': 19446,\n",
       " u'gopher': 6213,\n",
       " u\"d'etudes\": 19447,\n",
       " u'r44': 19448,\n",
       " u'minuses': 19449,\n",
       " u'mo44': 19450,\n",
       " u'j33n': 13252,\n",
       " u'27y': 13253,\n",
       " u'blonde': 21193,\n",
       " u'r48': 13564,\n",
       " u'l45u': 4873,\n",
       " u'godhead': 2493,\n",
       " u'mucz': 19453,\n",
       " u'fra': 19454,\n",
       " u'9jc': 19455,\n",
       " u'lnf5qp': 21208,\n",
       " u'1t471w86': 30585,\n",
       " u'ejbe': 19456,\n",
       " u'9jg': 19457,\n",
       " u'union': 6214,\n",
       " u'frj': 19458,\n",
       " u'mlur': 19459,\n",
       " u'2wk7': 19460,\n",
       " u'frn': 21226,\n",
       " u'bothers': 5654,\n",
       " u'i4s65e962n': 19461,\n",
       " u'much': 157,\n",
       " u\"'2fk\": 19462,\n",
       " u'fry': 19464,\n",
       " u'frx': 19465,\n",
       " u'h1rhse': 19466,\n",
       " u\"'tuorpuoqy\": 19467,\n",
       " u'r4e': 19468,\n",
       " u'lhzrfw': 30586,\n",
       " u\"v'ax\": 7926,\n",
       " u'r4c': 5655,\n",
       " u'davy': 19470,\n",
       " u'r4j': 21289,\n",
       " u'dave': 2146,\n",
       " u'spif': 19472,\n",
       " u'r4p': 19473,\n",
       " u'r4s': 6215,\n",
       " u'doubts': 6885,\n",
       " u'spin': 19474,\n",
       " u'hpuhr': 24900,\n",
       " u'l455': 21311,\n",
       " u'hqawl': 19477,\n",
       " u'vl1v': 25983,\n",
       " u'participatory': 13254,\n",
       " u'mukwn': 19479,\n",
       " u'employ': 10482,\n",
       " u'fr2': 13255,\n",
       " u'fr0': 19480,\n",
       " u\"i'8t\": 19481,\n",
       " u'fr6': 19482,\n",
       " u'9j6': 19483,\n",
       " u\"2'3tti\": 19484,\n",
       " u'ble2u': 19485,\n",
       " u'elaborate': 7733,\n",
       " u'0lv': 19486,\n",
       " u'0lu': 19487,\n",
       " u'kilcore': 19488,\n",
       " u'conditioned': 10483,\n",
       " u'0le': 19489,\n",
       " u'rw1031': 19490,\n",
       " u'0lm': 13256,\n",
       " u'oxymoron': 13202,\n",
       " u'valueless': 42032,\n",
       " u'memorial': 18851,\n",
       " u'0wkz': 19492,\n",
       " u'malefactors': 19493,\n",
       " u'vicars': 19495,\n",
       " u'mj5e94eg9hz': 19496,\n",
       " u'conformed': 12272,\n",
       " u'split': 2288,\n",
       " u\"bond's\": 19498,\n",
       " u'0wkg': 19499,\n",
       " u'ihmf1': 19500,\n",
       " u'principals': 13258,\n",
       " u'lqrp': 45952,\n",
       " u'i6op3a': 19501,\n",
       " u'0l0': 19502,\n",
       " u'mv3kcj4': 19503,\n",
       " u'ktikmh': 19504,\n",
       " u'southy': 7734,\n",
       " u'marched': 19506,\n",
       " u'flkhf': 19507,\n",
       " u'xegipgs': 41239,\n",
       " u'000usd': 13259,\n",
       " u'supper': 7735,\n",
       " u'zd3k8rlk': 19509,\n",
       " u'5ww4': 19510,\n",
       " u'ns9ln8': 19511,\n",
       " u'khflkh': 19512,\n",
       " u'khflki': 19513,\n",
       " u'daven': 46015,\n",
       " u\"'wwiz\": 3717,\n",
       " u'nffutils': 19515,\n",
       " u'qrplorplqrpl': 19516,\n",
       " u'kjzpk': 19517,\n",
       " u'vr56': 19518,\n",
       " u'academic': 5236,\n",
       " u\"03hord'\": 19519,\n",
       " u'145st466': 19520,\n",
       " u'nibby': 13260,\n",
       " u'u9126619': 19523,\n",
       " u'lhzd3ho0': 19524,\n",
       " u'corporate': 8864,\n",
       " u'ld6': 42038,\n",
       " u'zfbj': 19526,\n",
       " u\"t'0\": 21567,\n",
       " u'bhjnux': 4876,\n",
       " u'16usv': 19527,\n",
       " u'absurdities': 19528,\n",
       " u'dtax': 8865,\n",
       " u'fehat8p': 19529,\n",
       " u\"t'8\": 19530,\n",
       " u'spassky': 13261,\n",
       " u'edessa': 19531,\n",
       " u'isodata': 19532,\n",
       " u'nssdca': 10484,\n",
       " u'bevelizes': 19533,\n",
       " u'smva': 19534,\n",
       " u'portrayed': 19535,\n",
       " u'p3nnk': 13262,\n",
       " u'haj': 13264,\n",
       " u'hal': 6886,\n",
       " u'ham': 10485,\n",
       " u'hao': 13265,\n",
       " u'haa': 19536,\n",
       " u'had': 131,\n",
       " u'advancement': 10486,\n",
       " u'hag': 19537,\n",
       " u'infonode': 19538,\n",
       " u'fortran': 2619,\n",
       " u'mcnamara': 13266,\n",
       " u'beloved': 7736,\n",
       " u'has': 87,\n",
       " u'hat': 13267,\n",
       " u'hav': 13268,\n",
       " u\"t'r\": 19539,\n",
       " u\"t's\": 19540,\n",
       " u\"t'p\": 19541,\n",
       " u\"t'q\": 19542,\n",
       " u'dingebre': 19543,\n",
       " u'confidentially': 24915,\n",
       " u\"t'z\": 19545,\n",
       " u'elders': 8866,\n",
       " u'survival': 4008,\n",
       " u'ck8rfw': 19546,\n",
       " u\"'63dy\": 24916,\n",
       " u'unequivocally': 19548,\n",
       " u'shadow': 6216,\n",
       " u'xxpfdop1': 19549,\n",
       " u'wlrbn': 19550,\n",
       " u'masonic': 19551,\n",
       " u'semi': 5119,\n",
       " u'alice': 6887,\n",
       " u'u6wyzhff': 19553,\n",
       " u'ha9': 19554,\n",
       " u'ha1': 19555,\n",
       " u'hl94': 19556,\n",
       " u'warping': 8867,\n",
       " u'attorney': 13269,\n",
       " u'3hzrck': 13270,\n",
       " u'crowd': 6217,\n",
       " u'vovp': 19557,\n",
       " u'a046vyn': 19558,\n",
       " u'crown': 19559,\n",
       " u'postmaster': 8868,\n",
       " u'captive': 13271,\n",
       " u'defragmented': 12354,\n",
       " u'emphases': 19562,\n",
       " u'fiduciary': 13272,\n",
       " u'4m1x': 19563,\n",
       " u'ejl2xb': 19564,\n",
       " u'js5': 31251,\n",
       " u'jvxljw': 19565,\n",
       " u'xxkc6': 19566,\n",
       " u'dp1s7': 19567,\n",
       " u'bottom': 2494,\n",
       " u'45zi7u': 19568,\n",
       " u'4zvyk5lc': 19569,\n",
       " u'fu8rji': 19570,\n",
       " u'borrows': 24920,\n",
       " u'8ys': 8869,\n",
       " u'8yt': 13274,\n",
       " u'0qvql6s3b': 19199,\n",
       " u'8yx': 2853,\n",
       " u'8yz': 21872,\n",
       " u'sqsed5': 19573,\n",
       " u'mzzn': 19574,\n",
       " u'bmmq5': 19575,\n",
       " u'codeview': 19576,\n",
       " u'binder': 19577,\n",
       " u'8yd': 19578,\n",
       " u'8yg': 8870,\n",
       " u'anagram': 19579,\n",
       " u'8yj': 19580,\n",
       " u\"o'li\": 19581,\n",
       " u'0k83a': 19582,\n",
       " u\"uors'\": 19583,\n",
       " u'8yo': 13276,\n",
       " u'decisevely': 47732,\n",
       " u'xxkcx': 6218,\n",
       " u'cerrina': 47400,\n",
       " u'oneness': 6888,\n",
       " u'kilgore': 19585,\n",
       " u\"'aw83\": 19586,\n",
       " u'c24gc': 8871,\n",
       " u'letf75u': 19587,\n",
       " u\"0c'4k1g9ku\": 19588,\n",
       " u'mangoe': 13277,\n",
       " u'mbn': 3188,\n",
       " u'j5h8rl3': 19589,\n",
       " u'mbh': 19590,\n",
       " u'mbk': 13278,\n",
       " u'creatio': 19591,\n",
       " u'mbe': 13279,\n",
       " u'marshall': 10488,\n",
       " u'marshalk': 19592,\n",
       " u'9l0qax': 3663,\n",
       " u'administer': 10490,\n",
       " u'beings': 1737,\n",
       " u'xiqv1': 19593,\n",
       " u'mby': 19594,\n",
       " u'mbz': 19595,\n",
       " u'js1': 44018,\n",
       " u'unrecognisable': 30166,\n",
       " u'shoots': 16042,\n",
       " u'mbw': 19598,\n",
       " u'mbp': 13280,\n",
       " u'mbq': 19599,\n",
       " u'mbs': 19600,\n",
       " u'64x32': 19601,\n",
       " u'dnu': 19602,\n",
       " u'am17': 19603,\n",
       " u'despised': 10491,\n",
       " u'suffice': 8872,\n",
       " u'13676': 19604,\n",
       " u'raped': 13281,\n",
       " u'az8': 19605,\n",
       " u'grasping': 19606,\n",
       " u'lhzd3k8rck': 19607,\n",
       " u'greatness': 13282,\n",
       " u'5onfj': 19608,\n",
       " u'tamu': 5657,\n",
       " u's8b': 19609,\n",
       " u'precendence': 19610,\n",
       " u'serenelli': 19611,\n",
       " u'q45k15o': 19613,\n",
       " u'k2fu': 42052,\n",
       " u'thesaurus': 19615,\n",
       " u'30u': 19616,\n",
       " u's8p': 19617,\n",
       " u'myhc': 19618,\n",
       " u's8r': 19619,\n",
       " u's8v': 4536,\n",
       " u's8w': 19621,\n",
       " u'mb4': 19622,\n",
       " u'2yyk9': 19623,\n",
       " u's8z': 13724,\n",
       " u'mb7': 19625,\n",
       " u'mb0': 19626,\n",
       " u'mb1': 13283,\n",
       " u'verdi': 19627,\n",
       " u'azi': 19628,\n",
       " u'passsage': 19629,\n",
       " u'humbled': 13285,\n",
       " u'toolkits': 8873,\n",
       " u\"else's\": 6219,\n",
       " u'arrays': 10492,\n",
       " u'aza': 19630,\n",
       " u'azd': 19631,\n",
       " u'smashes': 19632,\n",
       " u'humbles': 19633,\n",
       " u'd4c1': 19634,\n",
       " u'tzxb': 19635,\n",
       " u\"naytnazz'kjz\": 46954,\n",
       " u'univesa': 10493,\n",
       " u'7ex2p': 19637,\n",
       " u'dues': 19638,\n",
       " u'771tnevrnl': 19639,\n",
       " u'azw': 19640,\n",
       " u'5wq8m': 19641,\n",
       " u'disgrace': 19642,\n",
       " u'cj1d': 8936,\n",
       " u'30o': 36290,\n",
       " u'bhv5k': 19644,\n",
       " u'qvjpwu': 30616,\n",
       " u'rosenkreutz': 19646,\n",
       " u'fanzine': 13288,\n",
       " u'x5w285': 50852,\n",
       " u'cj1v': 10495,\n",
       " u'triangles': 5658,\n",
       " u'dpcd': 36342,\n",
       " u\"d'e5e9e5g9v\": 19648,\n",
       " u'eventual': 19649,\n",
       " u'xkc': 7737,\n",
       " u'5yal': 42759,\n",
       " u'xkg': 13290,\n",
       " u'cambodia': 19650,\n",
       " u'pasadena': 7738,\n",
       " u'role': 2034,\n",
       " u'16v6': 10496,\n",
       " u'air3qjwm': 19651,\n",
       " u'xkq': 19652,\n",
       " u'roll': 6889,\n",
       " u'emended': 19653,\n",
       " u't3s1ecr6': 19654,\n",
       " u'intend': 2854,\n",
       " u'palms': 10497,\n",
       " u\"kr'w\": 19656,\n",
       " u'9191': 19657,\n",
       " u'jjjjjkbxn': 13291,\n",
       " u'cumulative': 28144,\n",
       " u'intent': 2495,\n",
       " u'frustation': 19658,\n",
       " u'variable': 4537,\n",
       " u'aaplay': 19659,\n",
       " u'contadictorily': 19660,\n",
       " u'8rck': 19661,\n",
       " u'80387': 13292,\n",
       " u'ordination': 47739,\n",
       " u'8rcs': 19663,\n",
       " u'fczn': 13293,\n",
       " u'xk8': 19664,\n",
       " u'6956': 39500,\n",
       " u'jmn': 19665,\n",
       " u'osy': 13294,\n",
       " u'b0dax': 8874,\n",
       " u'chaim': 19667,\n",
       " u'chain': 5634,\n",
       " u'whoever': 5238,\n",
       " u'osi': 19669,\n",
       " u'vxj': 38556,\n",
       " u'3sn': 19670,\n",
       " u'j8jy': 19671,\n",
       " u'fyl5': 19672,\n",
       " u'm104ysz': 19673,\n",
       " u'chair': 13295,\n",
       " u'3sa': 19674,\n",
       " u'osf': 7739,\n",
       " u'mt471': 30681,\n",
       " u'themseles': 19675,\n",
       " u'p4s': 19676,\n",
       " u'chistmas': 19677,\n",
       " u'machz': 19678,\n",
       " u'balled': 19679,\n",
       " u'a0000': 13298,\n",
       " u'a2dpn': 24940,\n",
       " u'kfij': 19681,\n",
       " u'p4p': 19682,\n",
       " u\"w8v'\": 19685,\n",
       " u'downloading': 5239,\n",
       " u'72plnki': 19686,\n",
       " u'3s8': 6220,\n",
       " u'jerk': 10499,\n",
       " u'fyld': 19687,\n",
       " u'fylk': 19688,\n",
       " u'os3': 19689,\n",
       " u'os2': 4538,\n",
       " u'3s1': 10500,\n",
       " u'tatja': 19690,\n",
       " u't466t3p': 19691,\n",
       " u'choice': 928,\n",
       " u'netlib': 8876,\n",
       " u'embark': 19692,\n",
       " u'm5e96': 19693,\n",
       " u'rationality': 6221,\n",
       " u'fylt': 19694,\n",
       " u'9giz': 19695,\n",
       " u'5gta': 44950,\n",
       " u'exact': 2035,\n",
       " u'minute': 3572,\n",
       " u'bbbbb': 10501,\n",
       " u\"simrd'\": 19696,\n",
       " u'souf': 49715,\n",
       " u'zkr6c': 19698,\n",
       " u'boudary': 19699,\n",
       " u\"'tu72pl\": 19700,\n",
       " u'skewed': 10502,\n",
       " u'fikpikzsfk': 10899,\n",
       " u'5g2j59': 19701,\n",
       " u'defaults': 13299,\n",
       " u'dismally': 42070,\n",
       " u'multiprocessing': 13300,\n",
       " u'0qb': 19704,\n",
       " u'shatim': 13301,\n",
       " u'ufi': 17172,\n",
       " u'4jdjnhe': 19706,\n",
       " u'0qq': 6890,\n",
       " u'chopping': 8877,\n",
       " u'0qu': 13795,\n",
       " u'0qv': 19708,\n",
       " u'0qw': 19709,\n",
       " u'patient': 12277,\n",
       " u'winecj': 19711,\n",
       " u'bagging': 19712,\n",
       " u'ww4e': 10504,\n",
       " u'antwerp': 19713,\n",
       " u'celebrated': 10505,\n",
       " u'zrg': 13302,\n",
       " u'300': 2464,\n",
       " u'newwhj': 19714,\n",
       " u'zrd': 6223,\n",
       " u'038o8v': 19715,\n",
       " u'zrb': 4539,\n",
       " u'z72pl': 19717,\n",
       " u'zrm': 5660,\n",
       " u'ground': 3772,\n",
       " u'boost': 10506,\n",
       " u'302': 42073,\n",
       " u'zri': 3773,\n",
       " u'zrh': 13303,\n",
       " u'drafted': 19718,\n",
       " u'siemon': 10507,\n",
       " u't0mx': 19719,\n",
       " u'honour': 19720,\n",
       " u'khaliphate': 19721,\n",
       " u's635': 50876,\n",
       " u'lii03i': 19722,\n",
       " u'0q2': 19723,\n",
       " u'ucs3ea8': 19724,\n",
       " u'address': 464,\n",
       " u'mx4q4dv': 19725,\n",
       " u\"h'lv\": 19727,\n",
       " u'ufy': 42075,\n",
       " u'rpc7dgr': 19730,\n",
       " u\"k'ka\": 19731,\n",
       " u'96jsv2n': 19732,\n",
       " u'queue': 11045,\n",
       " u'accomplished': 6891,\n",
       " u'ww46': 19734,\n",
       " u'ww47': 10509,\n",
       " u'ww44': 13304,\n",
       " u'ww45': 19735,\n",
       " u'6ei0mfq': 7741,\n",
       " u\"m'1tl\": 19736,\n",
       " u'windowing': 10510,\n",
       " u'vituperation': 19737,\n",
       " u'lkerdi': 19738,\n",
       " u'mbecg3hi': 19739,\n",
       " u'raise': 2924,\n",
       " u'exitting': 19741,\n",
       " u\"hdvbf'ax\": 19742,\n",
       " u'1k1nz': 19743,\n",
       " u\"'j7b\": 42076,\n",
       " u'8oob': 41948,\n",
       " u'bigal': 19746,\n",
       " u\"wv'\": 19747,\n",
       " u'darnell': 19748,\n",
       " u'undergone': 19749,\n",
       " u\"satan's\": 52221,\n",
       " u'working': 780,\n",
       " u'perished': 10512,\n",
       " u'ss2fus': 19750,\n",
       " u'optimize': 12280,\n",
       " u'opposed': 1972,\n",
       " u'w4ew27': 13306,\n",
       " u'czs': 36359,\n",
       " u'k0h': 13307,\n",
       " u'k0j': 19754,\n",
       " u'familar': 19755,\n",
       " u'consoled': 19756,\n",
       " u'alastair': 7742,\n",
       " u'ewxte': 19757,\n",
       " u'assimilation': 13860,\n",
       " u'6ecs8': 19759,\n",
       " u'consoles': 19760,\n",
       " u'g9v38v': 19761,\n",
       " u'6fph': 19762,\n",
       " u'inquistion': 19763,\n",
       " u'k04': 19764,\n",
       " u'originally': 2036,\n",
       " u'rutherford': 19766,\n",
       " u'abortion': 4877,\n",
       " u'xyz2': 10513,\n",
       " u'mt3s1t': 19767,\n",
       " u'bdxrbgn': 19768,\n",
       " u'albright': 41606,\n",
       " u'following': 394,\n",
       " u\"aokz'ax\": 24958,\n",
       " u'2lm': 13308,\n",
       " u'2ll': 19771,\n",
       " u\"ua'8\": 42082,\n",
       " u'2lk': 2390,\n",
       " u'mirrors': 2398,\n",
       " u'2le': 10514,\n",
       " u'mailboxes': 19774,\n",
       " u'locks': 9020,\n",
       " u'p99li': 10773,\n",
       " u'incremental': 8879,\n",
       " u'2ly': 19776,\n",
       " u'hzrlj1': 36361,\n",
       " u'2lu': 19778,\n",
       " u'2lt': 13309,\n",
       " u'1ir2m': 19779,\n",
       " u'usna': 19780,\n",
       " u'2ls': 6224,\n",
       " u'listens': 13310,\n",
       " u'hs9l': 29933,\n",
       " u'xqpfzq1aw': 19781,\n",
       " u'kjr8': 19782,\n",
       " u'believeing': 13311,\n",
       " u'1f9f9f9f9f': 19783,\n",
       " u'b36wi': 19784,\n",
       " u'kjr7': 19785,\n",
       " u'kxo': 30639,\n",
       " u'atmospherique': 19786,\n",
       " u'subroutines': 5240,\n",
       " u'philosphies': 19788,\n",
       " u'feq65c': 19789,\n",
       " u'7uenq': 19790,\n",
       " u'cxxkcx': 13313,\n",
       " u'60qf07l': 19791,\n",
       " u'z4x6': 19792,\n",
       " u'pellissippi': 19793,\n",
       " u'2l9': 8880,\n",
       " u'2l8': 13315,\n",
       " u'ecs1t3p': 19794,\n",
       " u'2l1': 7743,\n",
       " u'conscious': 4009,\n",
       " u'warring': 38935,\n",
       " u\"jw4'\": 19797,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse the `word_index` with `idx2word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_idx2word = {v: k for k, v in train_word_index.iteritems()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first review, both as a list of indices and as text reconstructed from the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6, 47, 1529, 37, 84, 69, 963, 110, 2, 676, 445, 832, 1268, 1135, 198, 72, 445, 832, 8, 736, 450, 7, 6, 95, 189, 3, 28, 3, 1203, 5, 171, 69, 62, 133, 50864, 12, 8, 970, 7537, 4, 117, 1270, 4, 1268, 7, 84, 94, 3755, 18, 109, 236, 26, 542, 29, 206, 244, 117, 69, 4, 134, 176, 213, 199, 16358, 18502, 15497, 14450, 10736, 2404, 144, 35, 15644, 11379, 9545, 2404, 144'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(map(str, train_sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'i'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx2word[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"i was wondering if any one knew how the various hard drive compression utilities work my hard drive is getting full and i don't want to have to buy a new one what i'm intrested in is speed ease of use amount of compression and any other aspect you think might be important as i've never use one of these things before thanks morgan bullard mb4008 coewl cen uiuc edu or mjbb uxa cso uiuc edu\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([train_idx2word[o] for o in train_sequences[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 'comp.os.ms-windows.misc')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.target[0], x_train.target_names[x_train.target[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce vocab size by setting rare words to max index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, sequence the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tokenizer = Tokenizer()\n",
    "test_tokenizer.fit_on_texts(x_test.data)\n",
    "test_sequences = test_tokenizer.texts_to_sequences(x_test.data)\n",
    "test_word_index = test_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#vocab_size = min(len(train_word_index), len(test_word_index))\n",
    "vocab_size = 5000\n",
    "\n",
    "trn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in train_sequences]\n",
    "test = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in test_sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of the lengths of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16306, 0, 289.51863354037266)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array(map(len, trn))\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39458, 0, 253.43933333333334)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array(map(len, test))\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird that there are sentences with 0 sequences (words) in them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get indices of arrays that do NOT satisfy np.nonzero\n",
    "nonzero_indices = np.unique(np.nonzero(train_sequences)[0])\n",
    "zero_indices = set(range(len(train_sequences))).difference(nonzero_indices)\n",
    "len(zero_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 59 sentences with no words. E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], 'comp.graphics')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences[18], x_train.target_names[x_train.target[18]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove them (and their labels) from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trn = np.delete(trn, list(zero_indices), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train.target = np.delete(x_train.target, list(zero_indices), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16306, 0, 289.51863354037266)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array(map(len, trn))\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so apparently there are also reviews with 1 word... we'll assume that's valid for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad (with zero) or truncate each sentence to make consistent length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "seq_len = 500\n",
    "\n",
    "trn = sequence.pad_sequences(trn, maxlen=seq_len, value=0)\n",
    "test = sequence.pad_sequences(test, maxlen=seq_len, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 4999, 2404,  144],\n",
       "       [   0,    0,    0, ..., 4999,   16,  546],\n",
       "       [  26,  104, 4999, ...,  163,  490,  380],\n",
       "       ..., \n",
       "       [   0,    0,    0, ...,  104, 4999,  103],\n",
       "       [   0,    0,    0, ...,   26, 1263, 4999],\n",
       "       [   0,    0,    0, ..., 4999, 4999, 2465]], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's turn the labels into categorical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "x_train.target = to_categorical(np.asarray(x_train.target))\n",
    "x_test.target = to_categorical(np.asarray(x_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2254, 500), (2254, 4))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.shape, x_train.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 500), (1500, 4))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape, x_test.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from keras.datasets import imdb\\nidx = imdb.get_word_index()\\n\\nfrom keras.utils.data_utils import get_file\\nimport pickle\\npath = get_file('imdb_full.pkl',\\n                origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\\n                md5_hash='d091312047c43cf9e4e38fef92437263')\\nf = open(path, 'rb')\\n(x_train, labels_train), (x_test, labels_test) = pickle.load(f)\\n\\nvocab_size = 5000\\n\\ntrn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_train]\\ntest = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_test]\\n\\ntrn = sequence.pad_sequences(trn, maxlen=seq_len, value=0)\\ntest = sequence.pad_sequences(test, maxlen=seq_len, value=0)\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from keras.datasets import imdb\n",
    "idx = imdb.get_word_index()\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "import pickle\n",
    "path = get_file('imdb_full.pkl',\n",
    "                origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n",
    "                md5_hash='d091312047c43cf9e4e38fef92437263')\n",
    "f = open(path, 'rb')\n",
    "(x_train, labels_train), (x_test, labels_test) = pickle.load(f)\n",
    "\n",
    "vocab_size = 5000\n",
    "\n",
    "trn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_train]\n",
    "test = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_test]\n",
    "\n",
    "trn = sequence.pad_sequences(trn, maxlen=seq_len, value=0)\n",
    "test = sequence.pad_sequences(test, maxlen=seq_len, value=0)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single hidden layer NN\n",
    "\n",
    "The simplest model that tends to give reasonable results is a single hidden layer net. So let's try that. Note that we can't expect to get any useful results by feeding word ids directly into a neural net - so instead we use an embedding to replace them with a vector of 32 (initially random) floats for each word in the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 500)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# input_length => 500-word reviews, 32 floats per word\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(4, activation='sigmoid')])\n",
    "    #Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 500, 32)       160000      embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 16000)         0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           1600100     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 100)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 4)             404         dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1760504\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "#model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2254 samples, validate on 1500 samples\n",
      "Epoch 1/2\n",
      "2254/2254 [==============================] - 0s - loss: 1.3632 - acc: 0.3270 - val_loss: 1.3488 - val_acc: 0.3393\n",
      "Epoch 2/2\n",
      "2254/2254 [==============================] - 0s - loss: 1.2441 - acc: 0.3953 - val_loss: 1.3826 - val_acc: 0.2847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1224b39350>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, x_train.target, validation_data=(test, x_test.target), nb_epoch=2, batch_size=64)\n",
    "#model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good? Bad? Here are some accuracies [from an official `sklearn` example](http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html) that classifies documents by topics using a bag-of-words approach:\n",
    "\n",
    "```\n",
    "[('RidgeClassifier', 0.89726533628972649),\n",
    " ('Perceptron', 0.88543976348854403),\n",
    " ('PassiveAggressiveClassifier', 0.90613451589061345),\n",
    " ('KNeighborsClassifier', 0.85809312638580926),\n",
    " ('RandomForestClassifier', 0.83813747228381374),\n",
    " ('LinearSVC', 0.90022172949002222),\n",
    " ('SGDClassifier', 0.90096082779009612),\n",
    " ('LinearSVC', 0.87287509238728755),\n",
    " ('SGDClassifier', 0.88543976348854403),\n",
    " ('SGDClassifier', 0.89874353288987441),\n",
    " ('NearestCentroid', 0.85513673318551364),\n",
    " ('MultinomialNB', 0.90022172949002222),\n",
    " ('BernoulliNB', 0.88396156688839611),\n",
    " ('Pipeline', 0.8810051736881005)]\n",
    " \n",
    " mean: 0.88311688311688319\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... not a good result in comparison with a much simpler approach. Training accuracy is comparable, but testing accuracy is much poorer.\n",
    "\n",
    "Since my model is barely training, I'm most likely doing something incorrectly. `pretrained_word_embeddings.ipynb` from Keras's examples repository was able to achieve `0.8734` acc, `0.7257` val_acc after 10 epochs - still not comparable to the 'shallow', bag-of-words models, but viable at least."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single conv layer with max pooling\n",
    "\n",
    "A CNN is likely to work better, since it's designed to take advantage of ordered data. We'll need to use a 1D CNN, since a sequence of words is 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)'''\n",
    "\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "\n",
    "'''conv1 = Sequential([\n",
    "    Embedding(vocab_size, 100, input_length=seq_len),\n",
    "    Convolution1D(128, 5, activation='relu'),\n",
    "    MaxPooling1D(5),\n",
    "    Convolution1D(128, 5, activation='relu'),\n",
    "    MaxPooling1D(5),\n",
    "    Convolution1D(128, 5, activation='relu'),\n",
    "    MaxPooling1D(5),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(4, activation='softmax')\n",
    "    ])'''\n",
    "\n",
    "conv1 = Sequential([\n",
    "    Embedding(vocab_size, 100, input_length=seq_len, dropout=0.4),\n",
    "    Dropout(0.4),\n",
    "    Convolution1D(128, 5, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    MaxPooling1D(5),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(4, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 500, 100)      500000      embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 500, 100)      0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_1 (Convolution1D)  (None, 496, 128)      64128       dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 496, 128)      0           convolution1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_1 (MaxPooling1D)    (None, 99, 128)       0           dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 12672)         0           maxpooling1d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 128)           1622144     flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 128)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 4)             516         dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2186788\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "conv1.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "conv1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010000000474974513"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.optimizer.lr.get_value().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conv1.optimizer.lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2254 samples, validate on 1500 samples\n",
      "Epoch 1/4\n",
      "2254/2254 [==============================] - 2s - loss: 1.3667 - acc: 0.3336 - val_loss: 1.3583 - val_acc: 0.3313\n",
      "Epoch 2/4\n",
      "2254/2254 [==============================] - 1s - loss: 1.3293 - acc: 0.3647 - val_loss: 1.3634 - val_acc: 0.3373\n",
      "Epoch 3/4\n",
      "2254/2254 [==============================] - 1s - loss: 1.1944 - acc: 0.4161 - val_loss: 1.5065 - val_acc: 0.2727\n",
      "Epoch 4/4\n",
      "2254/2254 [==============================] - 1s - loss: 0.9573 - acc: 0.5071 - val_loss: 2.2944 - val_acc: 0.2640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f121ef0b910>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.fit(trn, x_train.target, validation_data=(test, x_test.target), nb_epoch=4, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conv1.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2254 samples, validate on 1500 samples\n",
      "Epoch 1/4\n",
      "2254/2254 [==============================] - 2s - loss: 0.8390 - acc: 0.5120 - val_loss: 2.5100 - val_acc: 0.2433\n",
      "Epoch 2/4\n",
      "2254/2254 [==============================] - 1s - loss: 0.7947 - acc: 0.5355 - val_loss: 2.8222 - val_acc: 0.2473\n",
      "Epoch 3/4\n",
      "2254/2254 [==============================] - 1s - loss: 0.7888 - acc: 0.5368 - val_loss: 2.8804 - val_acc: 0.2500\n",
      "Epoch 4/4\n",
      "2254/2254 [==============================] - 1s - loss: 0.7794 - acc: 0.5297 - val_loss: 2.7423 - val_acc: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f121b01a310>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.fit(trn, x_train.target, validation_data=(test, x_test.target), nb_epoch=4, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pre-trained vectors\n",
    "\n",
    "You may want to look at wordvectors.ipynb before moving on.\n",
    "\n",
    "In this section, we replicate the previous CNN, but using pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "def get_glove_dataset(dataset):\n",
    "    \"\"\"Download the requested glove dataset from files.fast.ai\n",
    "    and return a location that can be passed to load_vectors.\n",
    "    \"\"\"\n",
    "    # see wordvectors.ipynb for info on how these files were\n",
    "    # generated from the original glove data.\n",
    "    md5sums = {'6B.50d': '8e1557d1228decbda7db6dfd81cd9909',\n",
    "               '6B.100d': 'c92dbbeacde2b0384a43014885a60b2c',\n",
    "               '6B.200d': 'af271b46c04b0b2e41a84d8cd806178d',\n",
    "               '6B.300d': '30290210376887dcc6d0a5a6374d8255'}\n",
    "    glove_path = os.path.abspath('data/glove/results')\n",
    "    %mkdir -p $glove_path\n",
    "    return get_file(dataset,\n",
    "                    'http://files.fast.ai/models/glove/' + dataset + '.tgz',\n",
    "                    cache_subdir=glove_path,\n",
    "                    md5_hash=md5sums.get(dataset, None),\n",
    "                    untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import load_array\n",
    "import pickle\n",
    "\n",
    "def load_vectors(loc):\n",
    "    return (load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untaring file...\n"
     ]
    }
   ],
   "source": [
    "vecs, words, wordidx = load_vectors(get_glove_dataset('6B.50d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The glove word ids and imdb word ids use different indexes. So we create a simple function that creates an embedding matrix using the indexes from imdb, and the embeddings from glove (where they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from numpy.random import normal\n",
    "\n",
    "def create_emb():\n",
    "    n_fact = vecs.shape[1]\n",
    "    emb = np.zeros((vocab_size, n_fact))\n",
    "\n",
    "    for i in range(1,len(emb)):\n",
    "        word = train_idx2word[i]\n",
    "        if word and re.match(r\"^[a-zA-Z0-9\\-]*$\", word) and word in wordidx:\n",
    "            src_idx = wordidx[word]\n",
    "            emb[i] = vecs[src_idx]\n",
    "        else:\n",
    "            # If we can't find the word in glove, randomly initialize\n",
    "            emb[i] = normal(scale=0.6, size=(n_fact,))\n",
    "\n",
    "    # This is our \"rare word\" id - we want to randomly initialize\n",
    "    emb[-1] = normal(scale=0.6, size=(n_fact,))\n",
    "    emb/=3\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
       "       [-0.0736, -0.243 ,  0.0836, ..., -0.1244,  0.3455,  0.2754],\n",
       "       [ 0.1393,  0.0832, -0.1375, ..., -0.0614, -0.0384, -0.2619],\n",
       "       ..., \n",
       "       [-0.5187,  0.2875,  0.0487, ..., -0.202 , -0.0205, -0.163 ],\n",
       "       [ 0.5126, -0.0599,  0.275 , ...,  0.2665, -0.1906,  0.2124],\n",
       "       [-0.2356, -0.4336,  0.1991, ..., -0.1291, -0.458 , -0.0296]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emb_model = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.2, \n",
    "              weights=[emb], trainable=False),\n",
    "    Dropout(0.25),\n",
    "    Convolution1D(64, 5, border_mode='same', activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(4, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_3 (Embedding)          (None, 500, 50)       0           embedding_input_3[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 500, 50)       0           embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_2 (Convolution1D)  (None, 500, 64)       16064       dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 500, 64)       0           convolution1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_2 (MaxPooling1D)    (None, 250, 64)       0           dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 16000)         0           maxpooling1d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 100)           1600100     flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 100)           0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 4)             404         dropout_7[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1616568\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2254 samples, validate on 1500 samples\n",
      "Epoch 1/4\n",
      "2254/2254 [==============================] - 1s - loss: 1.3446 - acc: 0.3429 - val_loss: 1.3730 - val_acc: 0.3460\n",
      "Epoch 2/4\n",
      "2254/2254 [==============================] - 1s - loss: 1.0494 - acc: 0.4787 - val_loss: 2.3117 - val_acc: 0.2713\n",
      "Epoch 3/4\n",
      "2254/2254 [==============================] - 1s - loss: 0.9280 - acc: 0.5106 - val_loss: 2.3087 - val_acc: 0.2727\n",
      "Epoch 4/4\n",
      "2254/2254 [==============================] - 1s - loss: 0.8899 - acc: 0.5235 - val_loss: 2.4015 - val_acc: 0.2760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f11fe821a10>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model.fit(trn, x_train.target, validation_data=(test, x_test.target), nb_epoch=4, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
