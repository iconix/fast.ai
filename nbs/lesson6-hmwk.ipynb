{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Setup\n",
    "\n",
    "We're going to download the collected plays of Shakespeare to use as our data.\n",
    "\n",
    "Source: http://www.gutenberg.org/cache/epub/100/pg100.txt\n",
    "\n",
    "The original source was preprocessed to remove sonnets and non-Shakesperean text added by Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = BASE_DIR + '/data/shakespeare/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_path = DATA_DIR + 'models/'\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('corpus length:', 5291227)\n"
     ]
    }
   ],
   "source": [
    "data = DATA_DIR + 'gutenberg_shakespeare_modified.txt' # preprocessed\n",
    "\n",
    "with open(data, 'r') as f:\n",
    "    text = f.read()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total chars:', 88)\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's useful to have a zero value in the dataset, e.g. for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars.insert(0, \"\\0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00\\n\\r !\"&\\'(),-.0123456789:;<?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_`abcdefghijklmnopqrstuvwxyz|}\\xbb\\xbf\\xef'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map chars to indices and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\x00': 0, ' ': 3, '(': 8, ',': 10, '0': 13, '4': 17, '8': 21, '\\xbb': 85, '<': 25, '\\xbf': 86, 'D': 30, 'H': 34, 'L': 38, 'P': 42, 'T': 46, 'X': 50, '`': 56, 'd': 60, 'h': 64, 'l': 68, '\\xef': 87, 'p': 72, 't': 76, 'x': 80, '|': 83, \"'\": 7, '3': 16, '7': 20, ';': 24, '?': 26, 'C': 29, 'G': 33, 'K': 37, 'O': 41, 'S': 45, 'W': 49, '[': 53, '_': 55, 'c': 59, 'g': 63, 'k': 67, 'o': 71, 's': 75, 'w': 79, '\\n': 1, '\"': 5, '&': 6, '.': 12, '2': 15, '6': 19, ':': 23, 'B': 28, 'F': 32, 'J': 36, 'N': 40, 'R': 44, 'V': 48, 'Z': 52, 'b': 58, 'f': 62, 'j': 66, 'n': 70, 'r': 74, 'v': 78, 'z': 82, '\\r': 2, '!': 4, ')': 9, '-': 11, '1': 14, '5': 18, '9': 22, 'A': 27, 'E': 31, 'I': 35, 'M': 39, 'Q': 43, 'U': 47, 'Y': 51, ']': 54, 'a': 57, 'e': 61, 'i': 65, 'm': 69, 'q': 73, 'u': 77, 'y': 81, '}': 84}\n"
     ]
    }
   ],
   "source": [
    "print(char_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*idx* converts the Shakepearean text to character indices (based on the *char_indices* mapping above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = [char_indices[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[87, 85, 86, 45, 29, 31, 40, 31, 23, 2, 1, 44, 71, 77, 75, 65, 68, 68, 71, 70, 24, 3, 42, 57, 74, 65, 75, 24, 3, 32, 68, 71, 74, 61, 70, 59, 61, 24, 3, 39, 57, 74, 75, 61, 65, 68, 68, 61, 75, 2, 1, 2, 1, 2, 1, 27, 29, 46, 3, 35, 12, 3, 45, 29, 31, 40, 31, 3, 14, 12]\n"
     ]
    }
   ],
   "source": [
    "print(idx[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xef\\xbb\\xbfSCENE:\\r\\nRousillon; Paris; Florence; Marseilles\\r\\n\\r\\n\\r\\nACT I. SCENE 1.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(indices_char[i] for i in idx[:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 char model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBALS needed from this point on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, merge, SimpleRNN, TimeDistributed\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fac = 42 # number of latent factors (size of embedding matrix)\n",
    "n_hidden = 256 # hyperparameter: size of hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inputs\n",
    "\n",
    "Create a list of every 4th character, starting at the 0th, 1st, 2nd, then 3rd characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nc = 3 # num chars\n",
    "c1_dat = [idx[i] for i in xrange(0, len(idx)-1-nc, nc)]\n",
    "c2_dat = [idx[i+1] for i in xrange(0, len(idx)-1-nc, nc)]\n",
    "c3_dat = [idx[i+2] for i in xrange(0, len(idx)-1-nc, nc)]\n",
    "c4_dat = [idx[i+3] for i in xrange(0, len(idx)-1-nc, nc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5291223, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0, len(idx)-1-nc, nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1763741, 1763741)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c1_dat), len(c4_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Out inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = np.stack(c1_dat)\n",
    "x2 = np.stack(c2_dat)\n",
    "x3 = np.stack(c3_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.stack(c4_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1763741,), (1763741,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create inputs and embedding outputs for each of our 3 character inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embedding_input(name, n_in, n_out):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name+'_in')\n",
    "    emb = Embedding(n_in, n_out, input_length=1, name=name+'_emb')(inp)\n",
    "    return inp, Flatten()(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c1_in, c1_emb = embedding_input('c1', vocab_size, n_fac)\n",
    "c2_in, c2_emb = embedding_input('c2', vocab_size, n_fac)\n",
    "c3_in, c3_emb = embedding_input('c3', vocab_size, n_fac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![3char](./3char.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dense_in` is the 'green arrow' in the diagram - the layer operation from input to hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dense_in = Dense(n_hidden, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first hidden activation is simply this function applied to the result of the embedding of the first character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c1_hidden = dense_in(c1_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dense_hidden` is the 'orange arrow' from our diagram - the layer operation from hidden to hidden\n",
    "\n",
    "_Note:_ unsure why the activation for this is `tanh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_hidden = Dense(n_hidden, activation='tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second and third activations sum up the previous hidden state (after applying `dense_hidden`) to the new input state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge([new input state, orange arrow from previous hidden state])\n",
    "c2_hidden = merge([dense_in(c2_emb), dense_hidden(c1_hidden)])\n",
    "c3_hidden = merge([dense_in(c3_emb), dense_hidden(c2_hidden)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dense_out` is the 'blue arrow' from our diagram - the layer operation from hidden to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_out = Dense(vocab_size, activation='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third hidden state is the input to our output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c4_out = dense_out(c3_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Model([c1_in, c2_in, c3_in], c4_out)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.optimizer.lr=0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "c1_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c2_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c1_emb (Embedding)               (None, 1, 42)         3696        c1_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "c2_emb (Embedding)               (None, 1, 42)         3696        c2_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 42)            0           c1_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 256)           11008       flatten_1[0][0]                  \n",
      "                                                                   flatten_2[0][0]                  \n",
      "                                                                   flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "c3_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 42)            0           c2_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c3_emb (Embedding)               (None, 1, 42)         3696        c3_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 256)           65792       dense_1[0][0]                    \n",
      "                                                                   merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 42)            0           c3_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 256)           0           dense_1[1][0]                    \n",
      "                                                                   dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 256)           0           dense_1[2][0]                    \n",
      "                                                                   dense_2[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 88)            22616       merge_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 110504\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1763741/1763741 [==============================] - 126s - loss: 3.6386   \n",
      "Epoch 2/4\n",
      "1763741/1763741 [==============================] - 128s - loss: 3.1218   \n",
      "Epoch 3/4\n",
      "1763741/1763741 [==============================] - 122s - loss: 3.0424   \n",
      "Epoch 4/4\n",
      "1763741/1763741 [==============================] - 126s - loss: 2.9566   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a1b992990>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x1, x2, x3], y, batch_size=64, nb_epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1763741/1763741 [==============================] - 128s - loss: 2.8626   \n",
      "Epoch 2/4\n",
      "1763741/1763741 [==============================] - 126s - loss: 2.7697   \n",
      "Epoch 3/4\n",
      "1763741/1763741 [==============================] - 128s - loss: 2.6858   \n",
      "Epoch 4/4\n",
      "1763741/1763741 [==============================] - 122s - loss: 2.6146   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a1a351e50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x1, x2, x3], y, batch_size=64, nb_epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1763741/1763741 [==============================] - 123s - loss: 2.5550   \n",
      "Epoch 2/4\n",
      "1763741/1763741 [==============================] - 126s - loss: 2.5057   \n",
      "Epoch 3/4\n",
      "1763741/1763741 [==============================] - 131s - loss: 2.4651   \n",
      "Epoch 4/4\n",
      "1763741/1763741 [==============================] - 123s - loss: 2.4316   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a1b838550>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x1, x2, x3], y, batch_size=64, nb_epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1763741/1763741 [==============================] - 131s - loss: 2.4037   \n",
      "Epoch 2/4\n",
      "1763741/1763741 [==============================] - 125s - loss: 2.3800   \n",
      "Epoch 3/4\n",
      "1763741/1763741 [==============================] - 126s - loss: 2.3595   \n",
      "Epoch 4/4\n",
      "1763741/1763741 [==============================] - 130s - loss: 2.3416   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a1eaa5550>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x1, x2, x3], y, batch_size=64, nb_epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1763741/1763741 [==============================] - 126s - loss: 2.3256   \n",
      "Epoch 2/10\n",
      "1763741/1763741 [==============================] - 128s - loss: 2.3111   \n",
      "Epoch 3/10\n",
      "1763741/1763741 [==============================] - 126s - loss: 2.2980   \n",
      "Epoch 4/10\n",
      "1763741/1763741 [==============================] - 121s - loss: 2.2859   \n",
      "Epoch 5/10\n",
      "1763741/1763741 [==============================] - 128s - loss: 2.2748   \n",
      "Epoch 6/10\n",
      "1763741/1763741 [==============================] - 124s - loss: 2.2645   \n",
      "Epoch 7/10\n",
      "1763741/1763741 [==============================] - 125s - loss: 2.2550   \n",
      "Epoch 8/10\n",
      "1763741/1763741 [==============================] - 125s - loss: 2.2462   \n",
      "Epoch 9/10\n",
      "1763741/1763741 [==============================] - 128s - loss: 2.2380   \n",
      "Epoch 10/10\n",
      "1763741/1763741 [==============================] - 128s - loss: 2.2304   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a1b838750>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x1, x2, x3], y, batch_size=64, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save1_path = model_path + 'save1.h5'\n",
    "if not os.path.exists(save1_path):\n",
    "    model.save_weights(save1_path)\n",
    "model.load_weights(save1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"`newaxis` is used to increase the dimension of the existing array by one more dimension, when used once\" - [source](https://stackoverflow.com/questions/29241056/the-use-of-numpy-newaxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next(m, inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arrs = [np.array(i)[np.newaxis] for i in idxs]\n",
    "    p = m.predict(arrs)\n",
    "    i = np.argmax(p)\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(model, 'phi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(model, ' th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(model, ' an')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first RNN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBALS needed from this point on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nc = 8 # numChars == size of our unrolled RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xs` (+ `c_in_dat`), `y` (+ `c_out_dat`), `cs` (+ `embedding_input()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inputs\n",
    "\n",
    "Now let's try predicting char 9 using chars 1-8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of 0 through 7, create a list of every 8th character with that starting point. These will be the 8 inputs to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_in_dat = [[idx[i+n] for i in xrange(0, len(idx)-1-nc, nc)]\n",
    "           for n in range(nc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a list of the next character in each of these series. This will be the labels for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_out_dat = [idx[i+nc] for i in xrange(0, len(idx)-1-nc, nc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs = [np.stack(c) for c in c_in_dat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, (661403,))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xs), xs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.stack(c_out_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each column below is one series of 8 characters from the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([87, 23, 68, 74, 74, 57, 75, 29]),\n",
       " array([85,  2, 68, 65, 61, 74,  2, 46]),\n",
       " array([86,  1, 71, 75, 70, 75,  1,  3]),\n",
       " array([45, 44, 70, 24, 59, 61,  2, 35]),\n",
       " array([29, 71, 24,  3, 61, 65,  1, 12]),\n",
       " array([31, 77,  3, 32, 24, 68,  2,  3]),\n",
       " array([40, 75, 42, 68,  3, 68,  1, 45]),\n",
       " array([31, 65, 57, 71, 39, 61, 27, 29])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[xs[n][:nc] for n in range(nc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and this is the next character after each sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 68, 74, 74, 57, 75, 29, 31])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:nc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_input(name, n_in, n_out):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name+'_in')\n",
    "    emb = Embedding(n_in, n_out, input_length=1, name=name+'_emb')(inp)\n",
    "    return inp, Flatten()(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cs = [embedding_input('c'+str(n), vocab_size, n_fac) for n in range(nc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"I'd suggest trying the trick I mentioned in the lesson for simple RNNs: using an identity matrix to initialize your hidden state, and use relu instead of tanh.\" - [Jeremy on forums](http://forums.fast.ai/t/purpose-of-rnns-and-theano/242/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dense_in = Dense(n_hidden, activation='relu')\n",
    "dense_hidden = Dense(n_hidden, activation='relu', init='identity')\n",
    "dense_out = Dense(vocab_size, activation='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding of the first character of each sequence goes through `dense_in` to create our first hidden activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden = dense_in(cs[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for each successive layer, we combine the output of `dense_in` on the next character with the output of `dense_hidden` on the current hidden state to create the new hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1, nc):\n",
    "    dense = dense_in(cs[i][1])\n",
    "    hidden = dense_hidden(hidden)\n",
    "    hidden = merge([dense, hidden])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the final hidden state through `dense_out` gives us our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = dense_out(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "c0_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c1_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c0_emb (Embedding)               (None, 1, 42)         3696        c0_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "c1_emb (Embedding)               (None, 1, 42)         3696        c1_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 42)            0           c0_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 256)           11008       flatten_4[0][0]                  \n",
      "                                                                   flatten_5[0][0]                  \n",
      "                                                                   flatten_6[0][0]                  \n",
      "                                                                   flatten_7[0][0]                  \n",
      "                                                                   flatten_8[0][0]                  \n",
      "                                                                   flatten_9[0][0]                  \n",
      "                                                                   flatten_10[0][0]                 \n",
      "                                                                   flatten_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "c2_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 42)            0           c1_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 256)           65792       dense_4[0][0]                    \n",
      "                                                                   merge_3[0][0]                    \n",
      "                                                                   merge_4[0][0]                    \n",
      "                                                                   merge_5[0][0]                    \n",
      "                                                                   merge_6[0][0]                    \n",
      "                                                                   merge_7[0][0]                    \n",
      "                                                                   merge_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "c2_emb (Embedding)               (None, 1, 42)         3696        c2_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "c3_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 42)            0           c2_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_3 (Merge)                  (None, 256)           0           dense_4[1][0]                    \n",
      "                                                                   dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "c3_emb (Embedding)               (None, 1, 42)         3696        c3_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "c4_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)              (None, 42)            0           c3_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_4 (Merge)                  (None, 256)           0           dense_4[2][0]                    \n",
      "                                                                   dense_5[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "c4_emb (Embedding)               (None, 1, 42)         3696        c4_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "c5_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)              (None, 42)            0           c4_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_5 (Merge)                  (None, 256)           0           dense_4[3][0]                    \n",
      "                                                                   dense_5[2][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "c5_emb (Embedding)               (None, 1, 42)         3696        c5_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "c6_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)              (None, 42)            0           c5_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_6 (Merge)                  (None, 256)           0           dense_4[4][0]                    \n",
      "                                                                   dense_5[3][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "c6_emb (Embedding)               (None, 1, 42)         3696        c6_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "c7_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)             (None, 42)            0           c6_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_7 (Merge)                  (None, 256)           0           dense_4[5][0]                    \n",
      "                                                                   dense_5[4][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "c7_emb (Embedding)               (None, 1, 42)         3696        c7_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)             (None, 42)            0           c7_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_8 (Merge)                  (None, 256)           0           dense_4[6][0]                    \n",
      "                                                                   dense_5[5][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "merge_9 (Merge)                  (None, 256)           0           dense_4[7][0]                    \n",
      "                                                                   dense_5[6][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 88)            22616       merge_9[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 128984\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([c[0] for c in cs], out)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "661403/661403 [==============================] - 89s - loss: 2.0080    \n",
      "Epoch 2/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.8291    \n",
      "Epoch 3/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.7723    \n",
      "Epoch 4/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.7404    \n",
      "Epoch 5/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.7189    \n",
      "Epoch 6/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.7034    \n",
      "Epoch 7/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.6912    \n",
      "Epoch 8/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6812    \n",
      "Epoch 9/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.6738    \n",
      "Epoch 10/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6671    \n",
      "Epoch 11/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6615    \n",
      "Epoch 12/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6561    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a036c9b10>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xs, y, batch_size=64, nb_epoch=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next(m, inp):\n",
    "    arrs = [np.array(char_indices[c])[np.newaxis] for c in inp]\n",
    "    p = m.predict(arrs)\n",
    "    return chars[np.argmax(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(model, 'for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(model, 'part of ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(model, 'queens a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a helper function for generating `k` additional words (separated by whitespace) in a starter sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_seq(m, inp, k):\n",
    "    k_count = 0\n",
    "    seq = inp\n",
    "    while k_count < k+1:\n",
    "        pc = get_next(m, inp)\n",
    "        seq += pc\n",
    "        inp = inp[1:] + pc\n",
    "        if (pc == ' '):\n",
    "            k_count += 1\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queens and the son the roper the roper the roper the roper '"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'queens a', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'part of the roper the roper the roper the roper the roper the '"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'part of ', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for thos  a dount of the roper the roper the roper '"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'for thos', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model currently seems to 'fixate' phrases like: \"the some sore\" or \"the roper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "661403/661403 [==============================] - 93s - loss: 1.6514    \n",
      "Epoch 2/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6475    \n",
      "Epoch 3/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6451    \n",
      "Epoch 4/12\n",
      "661403/661403 [==============================] - 90s - loss: 1.6412    \n",
      "Epoch 5/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.6385    \n",
      "Epoch 6/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6363    \n",
      "Epoch 7/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6338    \n",
      "Epoch 8/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6311    \n",
      "Epoch 9/12\n",
      "661403/661403 [==============================] - 94s - loss: 1.6292    \n",
      "Epoch 10/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6272    \n",
      "Epoch 11/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.6256    \n",
      "Epoch 12/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6249    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a036c9fd0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xs, y, batch_size=64, nb_epoch=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queens and the sor of the sor of the sor of the '"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'queens a', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'part of the sor of the sor of the sor of the sor '"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'part of ', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for thos  h and the sor of the sor of the '"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'for thos', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6231    \n",
      "Epoch 2/12\n",
      "661403/661403 [==============================] - 96s - loss: 1.6210    \n",
      "Epoch 3/12\n",
      "661403/661403 [==============================] - 99s - loss: 1.6199    \n",
      "Epoch 4/12\n",
      "661403/661403 [==============================] - 93s - loss: 1.6187    \n",
      "Epoch 5/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6178    \n",
      "Epoch 6/12\n",
      "661403/661403 [==============================] - 95s - loss: 1.6167    \n",
      "Epoch 7/12\n",
      "661403/661403 [==============================] - 95s - loss: 1.6169    \n",
      "Epoch 8/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.6154    \n",
      "Epoch 9/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.6145    \n",
      "Epoch 10/12\n",
      "661403/661403 [==============================] - 93s - loss: 1.6136    \n",
      "Epoch 11/12\n",
      "661403/661403 [==============================] - 90s - loss: 1.6126    \n",
      "Epoch 12/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.6121    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a036c9e10>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xs, y, batch_size=64, nb_epoch=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queens and the hand the hand the hand the hand the hand '"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'queens a', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'part of the gonder the sore the gonder the sore the gonder the '"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'part of ', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for thos of the gonder the sore the gonder the sore the '"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'for thos', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different 'fixation' phrases like: \"the best with\", \"the gonder the sore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save2_path = model_path + 'save2.h5'\n",
    "if not os.path.exists(save2_path):\n",
    "    model.save_weights(save2_path)\n",
    "model.load_weights(save2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first RNN with keras!\n",
    "\n",
    "This is nearly equivalent to the RNN we built ourselves in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 8, 42)         3696        embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_1 (SimpleRNN)          (None, 256)           76544       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 88)            22616       simplernn_1[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 102856\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=nc),\n",
    "        SimpleRNN(n_hidden, activation='relu', inner_init='identity'),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid `IndexError: axis 1 out of bounds [0, 1)`: http://forums.fast.ai/t/lesson-6-discussion/245/70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.3725    \n",
      "Epoch 2/12\n",
      "661403/661403 [==============================] - 87s - loss: 1.3684    \n",
      "Epoch 3/12\n",
      "661403/661403 [==============================] - 87s - loss: 1.3657    \n",
      "Epoch 4/12\n",
      "661403/661403 [==============================] - 87s - loss: 1.3619    \n",
      "Epoch 5/12\n",
      "661403/661403 [==============================] - 87s - loss: 1.3594    \n",
      "Epoch 6/12\n",
      "661403/661403 [==============================] - 87s - loss: 1.3570    \n",
      "Epoch 7/12\n",
      "661403/661403 [==============================] - 87s - loss: 1.3547    \n",
      "Epoch 8/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.3532    \n",
      "Epoch 9/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.3516    \n",
      "Epoch 10/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.3496    \n",
      "Epoch 11/12\n",
      "661403/661403 [==============================] - 90s - loss: 1.3524    \n",
      "Epoch 12/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.3484    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49f2b4f090>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.fit(np.concatenate([x[np.newaxis] for x in xs]).T, y, batch_size=64, nb_epoch=12)\n",
    "model.fit(np.concatenate(xs, axis=1), y, batch_size=64, nb_epoch=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_keras(m, inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arrs = np.array(idxs)[np.newaxis,:]\n",
    "    p = m.predict(arrs)[0]\n",
    "    return chars[np.argmax(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_keras_seq(m, inp, k):\n",
    "    k_count = 0\n",
    "    seq = inp\n",
    "    while k_count < k+1:\n",
    "        pc = get_next_keras(m, inp)\n",
    "        seq += pc\n",
    "        inp = inp[1:] + pc\n",
    "        if (pc == ' '):\n",
    "            k_count += 1\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queens and the shall be the shall be the shall be the '"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_keras_seq(model, 'queens a', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'part of the shall be the shall be the shall be the shall '"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_keras_seq(model, 'part of ', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for those the shall be the shall be the shall be the '"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_keras_seq(model, 'for thos', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Fixations_: \"the sent\", \"the shall be\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.3467    \n",
      "Epoch 2/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.3468    \n",
      "Epoch 3/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.3444    \n",
      "Epoch 4/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.3438    \n",
      "Epoch 5/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.3424    \n",
      "Epoch 6/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.3426    \n",
      "Epoch 7/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.3420    \n",
      "Epoch 8/12\n",
      "661403/661403 [==============================] - 90s - loss: 1.3420    \n",
      "Epoch 9/12\n",
      "661403/661403 [==============================] - 90s - loss: 1.3404    \n",
      "Epoch 10/12\n",
      "661403/661403 [==============================] - 92s - loss: 1.3402    \n",
      "Epoch 11/12\n",
      "661403/661403 [==============================] - 90s - loss: 1.3393    \n",
      "Epoch 12/12\n",
      "661403/661403 [==============================] - 95s - loss: 1.3403    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49f1b72110>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.fit(np.concatenate([x[np.newaxis] for x in xs]).T, y, batch_size=64, nb_epoch=12)\n",
    "model.fit(np.concatenate(xs, axis=1), y, batch_size=64, nb_epoch=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queens and the strength the strength the strength the strength the strength '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_keras_seq(model, 'queens a', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'part of the strength the strength the strength the strength the strength the '"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_keras_seq(model, 'part of ', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for those thou shalt the strength the strength the strength the strength '"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_keras_seq(model, 'for thos', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Fixations_: \"the serve me\", \"the strength\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save3_path = model_path + 'save3.h5'\n",
    "if not os.path.exists(save3_path):\n",
    "    model.save_weights(save3_path)\n",
    "model.load_weights(save3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBALS needed from this point on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ys` (+ `c_out_dat`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inputs\n",
    "\n",
    "To use a sequence model, we can leave our input unchanged - but we have to change our output to a sequence.\n",
    "\n",
    "Here, `c_out_dat` is identical to `c_in_dat`, but moved across 1 character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_out_dat = [[idx[i+n] for i in xrange(1, len(idx)-nc, nc)]\n",
    "            for n in range(nc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ys = [np.stack(c) for c in c_out_dat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading down each column shows one set of inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[87],\n",
       "        [23],\n",
       "        [68],\n",
       "        [74],\n",
       "        [74],\n",
       "        [57],\n",
       "        [75],\n",
       "        [29]]), array([[85],\n",
       "        [ 2],\n",
       "        [68],\n",
       "        [65],\n",
       "        [61],\n",
       "        [74],\n",
       "        [ 2],\n",
       "        [46]]), array([[86],\n",
       "        [ 1],\n",
       "        [71],\n",
       "        [75],\n",
       "        [70],\n",
       "        [75],\n",
       "        [ 1],\n",
       "        [ 3]]), array([[45],\n",
       "        [44],\n",
       "        [70],\n",
       "        [24],\n",
       "        [59],\n",
       "        [61],\n",
       "        [ 2],\n",
       "        [35]]), array([[29],\n",
       "        [71],\n",
       "        [24],\n",
       "        [ 3],\n",
       "        [61],\n",
       "        [65],\n",
       "        [ 1],\n",
       "        [12]]), array([[31],\n",
       "        [77],\n",
       "        [ 3],\n",
       "        [32],\n",
       "        [24],\n",
       "        [68],\n",
       "        [ 2],\n",
       "        [ 3]]), array([[40],\n",
       "        [75],\n",
       "        [42],\n",
       "        [68],\n",
       "        [ 3],\n",
       "        [68],\n",
       "        [ 1],\n",
       "        [45]]), array([[31],\n",
       "        [65],\n",
       "        [57],\n",
       "        [71],\n",
       "        [39],\n",
       "        [61],\n",
       "        [27],\n",
       "        [29]])]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[xs[n][:nc] for n in range(nc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([85,  2, 68, 65, 61, 74,  2, 46]),\n",
       " array([86,  1, 71, 75, 70, 75,  1,  3]),\n",
       " array([45, 44, 70, 24, 59, 61,  2, 35]),\n",
       " array([29, 71, 24,  3, 61, 65,  1, 12]),\n",
       " array([31, 77,  3, 32, 24, 68,  2,  3]),\n",
       " array([40, 75, 42, 68,  3, 68,  1, 45]),\n",
       " array([31, 65, 57, 71, 39, 61, 27, 29]),\n",
       " array([23, 68, 74, 74, 57, 75, 29, 31])]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ys[n][:nc] for n in range(nc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_in = Dense(n_hidden, activation='relu')\n",
    "dense_hidden = Dense(n_hidden, activation='relu', init='identity')\n",
    "dense_out = Dense(vocab_size, activation='softmax', name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to pass a vectcor of all zeros as our starting point - here's our input layers for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp1 = Input(shape=(n_fac,), name='zeros')\n",
    "hidden = dense_in(inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outs = []\n",
    "\n",
    "for i in range(nc):\n",
    "    dense = dense_in(cs[i][1])\n",
    "    hidden = dense_hidden(hidden)\n",
    "    hidden = merge([dense, hidden], mode='sum')\n",
    "    # every layer now has an output\n",
    "    outs.append(dense_out(hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "c0_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c0_emb (Embedding)               (None, 1, 42)         3696        c0_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "zeros (InputLayer)               (None, 42)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 256)           11008       zeros[0][0]                      \n",
      "                                                                   flatten_4[0][0]                  \n",
      "                                                                   flatten_5[0][0]                  \n",
      "                                                                   flatten_6[0][0]                  \n",
      "                                                                   flatten_7[0][0]                  \n",
      "                                                                   flatten_8[0][0]                  \n",
      "                                                                   flatten_9[0][0]                  \n",
      "                                                                   flatten_10[0][0]                 \n",
      "                                                                   flatten_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 42)            0           c0_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c1_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 256)           65792       dense_8[0][0]                    \n",
      "                                                                   merge_10[0][0]                   \n",
      "                                                                   merge_11[0][0]                   \n",
      "                                                                   merge_12[0][0]                   \n",
      "                                                                   merge_13[0][0]                   \n",
      "                                                                   merge_14[0][0]                   \n",
      "                                                                   merge_15[0][0]                   \n",
      "                                                                   merge_16[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "c1_emb (Embedding)               (None, 1, 42)         3696        c1_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "merge_10 (Merge)                 (None, 256)           0           dense_8[1][0]                    \n",
      "                                                                   dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 42)            0           c1_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c2_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c2_emb (Embedding)               (None, 1, 42)         3696        c2_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "merge_11 (Merge)                 (None, 256)           0           dense_8[2][0]                    \n",
      "                                                                   dense_9[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 42)            0           c2_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c3_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c3_emb (Embedding)               (None, 1, 42)         3696        c3_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "merge_12 (Merge)                 (None, 256)           0           dense_8[3][0]                    \n",
      "                                                                   dense_9[2][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)              (None, 42)            0           c3_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c4_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c4_emb (Embedding)               (None, 1, 42)         3696        c4_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "merge_13 (Merge)                 (None, 256)           0           dense_8[4][0]                    \n",
      "                                                                   dense_9[3][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)              (None, 42)            0           c4_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c5_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c5_emb (Embedding)               (None, 1, 42)         3696        c5_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "merge_14 (Merge)                 (None, 256)           0           dense_8[5][0]                    \n",
      "                                                                   dense_9[4][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)              (None, 42)            0           c5_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c6_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c6_emb (Embedding)               (None, 1, 42)         3696        c6_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "c7_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "merge_15 (Merge)                 (None, 256)           0           dense_8[6][0]                    \n",
      "                                                                   dense_9[5][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)             (None, 42)            0           c6_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c7_emb (Embedding)               (None, 1, 42)         3696        c7_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)             (None, 42)            0           c7_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_16 (Merge)                 (None, 256)           0           dense_8[7][0]                    \n",
      "                                                                   dense_9[6][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "merge_17 (Merge)                 (None, 256)           0           dense_8[8][0]                    \n",
      "                                                                   dense_9[7][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 88)            22616       merge_10[0][0]                   \n",
      "                                                                   merge_11[0][0]                   \n",
      "                                                                   merge_12[0][0]                   \n",
      "                                                                   merge_13[0][0]                   \n",
      "                                                                   merge_14[0][0]                   \n",
      "                                                                   merge_15[0][0]                   \n",
      "                                                                   merge_16[0][0]                   \n",
      "                                                                   merge_17[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 128984\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([inp1] + [c[0] for c in cs], outs)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(661403, 42)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = np.tile(np.zeros(n_fac), (len(xs[0]), 1))\n",
    "zeros.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "661403/661403 [==============================] - 216s - loss: 16.4263 - output_loss_1: 2.4570 - output_loss_2: 2.2423 - output_loss_3: 2.0530 - output_loss_4: 1.9849 - output_loss_5: 1.9383 - output_loss_6: 1.9218 - output_loss_7: 1.9160 - output_loss_8: 1.9132   \n",
      "Epoch 2/12\n",
      "661403/661403 [==============================] - 212s - loss: 15.2679 - output_loss_1: 2.4154 - output_loss_2: 2.1897 - output_loss_3: 1.9418 - output_loss_4: 1.8231 - output_loss_5: 1.7499 - output_loss_6: 1.7231 - output_loss_7: 1.7139 - output_loss_8: 1.7109   \n",
      "Epoch 3/12\n",
      "661403/661403 [==============================] - 214s - loss: 15.0269 - output_loss_1: 2.4129 - output_loss_2: 2.1850 - output_loss_3: 1.9264 - output_loss_4: 1.7911 - output_loss_5: 1.7072 - output_loss_6: 1.6760 - output_loss_7: 1.6657 - output_loss_8: 1.6626   \n",
      "Epoch 4/12\n",
      "661403/661403 [==============================] - 217s - loss: 14.9106 - output_loss_1: 2.4114 - output_loss_2: 2.1828 - output_loss_3: 1.9198 - output_loss_4: 1.7756 - output_loss_5: 1.6871 - output_loss_6: 1.6527 - output_loss_7: 1.6422 - output_loss_8: 1.6390   \n",
      "Epoch 5/12\n",
      "661403/661403 [==============================] - 218s - loss: 14.8421 - output_loss_1: 2.4105 - output_loss_2: 2.1811 - output_loss_3: 1.9157 - output_loss_4: 1.7672 - output_loss_5: 1.6755 - output_loss_6: 1.6393 - output_loss_7: 1.6283 - output_loss_8: 1.6244   \n",
      "Epoch 6/12\n",
      "661403/661403 [==============================] - 211s - loss: 14.7946 - output_loss_1: 2.4099 - output_loss_2: 2.1804 - output_loss_3: 1.9127 - output_loss_4: 1.7608 - output_loss_5: 1.6675 - output_loss_6: 1.6296 - output_loss_7: 1.6190 - output_loss_8: 1.6147   \n",
      "Epoch 7/12\n",
      "661403/661403 [==============================] - 211s - loss: 14.7578 - output_loss_1: 2.4092 - output_loss_2: 2.1791 - output_loss_3: 1.9104 - output_loss_4: 1.7559 - output_loss_5: 1.6616 - output_loss_6: 1.6232 - output_loss_7: 1.6109 - output_loss_8: 1.6074   \n",
      "Epoch 8/12\n",
      "661403/661403 [==============================] - 213s - loss: 14.7301 - output_loss_1: 2.4088 - output_loss_2: 2.1785 - output_loss_3: 1.9080 - output_loss_4: 1.7529 - output_loss_5: 1.6567 - output_loss_6: 1.6179 - output_loss_7: 1.6053 - output_loss_8: 1.6020   \n",
      "Epoch 9/12\n",
      "661403/661403 [==============================] - 215s - loss: 14.7073 - output_loss_1: 2.4085 - output_loss_2: 2.1782 - output_loss_3: 1.9069 - output_loss_4: 1.7503 - output_loss_5: 1.6533 - output_loss_6: 1.6126 - output_loss_7: 1.6011 - output_loss_8: 1.5965   \n",
      "Epoch 10/12\n",
      "661403/661403 [==============================] - 225s - loss: 14.6883 - output_loss_1: 2.4084 - output_loss_2: 2.1775 - output_loss_3: 1.9053 - output_loss_4: 1.7481 - output_loss_5: 1.6497 - output_loss_6: 1.6095 - output_loss_7: 1.5967 - output_loss_8: 1.5930   \n",
      "Epoch 11/12\n",
      "661403/661403 [==============================] - 216s - loss: 14.6720 - output_loss_1: 2.4082 - output_loss_2: 2.1774 - output_loss_3: 1.9048 - output_loss_4: 1.7460 - output_loss_5: 1.6466 - output_loss_6: 1.6063 - output_loss_7: 1.5936 - output_loss_8: 1.5892   \n",
      "Epoch 12/12\n",
      "661403/661403 [==============================] - 212s - loss: 14.6579 - output_loss_1: 2.4080 - output_loss_2: 2.1767 - output_loss_3: 1.9038 - output_loss_4: 1.7442 - output_loss_5: 1.6449 - output_loss_6: 1.6034 - output_loss_7: 1.5907 - output_loss_8: 1.5863   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49f6f81f50>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([zeros]+xs, ys, batch_size=64, nb_epoch=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nexts(m, inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arrs = [np.array(i)[np.newaxis] for i in idxs]\n",
    "    p = model.predict([np.zeros(n_fac)[np.newaxis,:]] + arrs)\n",
    "    print(list(inp))\n",
    "    return [chars[np.argmax(o)] for o in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 't', 'h', 'i', 's', ' ', 'i', 's']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ', 'h', 'e', 't', ' ', 'm', 's', ' ']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts(model, ' this is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'p', 'a', 'r', 't', ' ', 'o', 'f']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ', 'o', 'r', 'e', 'o', 'o', 'f', ' ']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts(model, ' part of')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['q', 'u', 'e', 'e', 'n', 's', ' ', 'a']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['u', 'i', 'e', 'n', ' ', ' ', 't', 'n']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts(model, 'queens a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBALS needed from this point on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(661403, 1)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_rnn = np.stack(np.squeeze(xs), axis=1)\n",
    "y_rnn = np.atleast_3d(np.stack(ys, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((661403, 8), (661403, 8, 1))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_rnn.shape, y_rnn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence model with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert our previous keras model into a sequence model, simply add the `return_sequences=True` parameter, and add `TimeDistributed` around our dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 8, 42)         3696        embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_2 (SimpleRNN)          (None, 8, 256)        76544       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribute(None, 8, 88)         22616       simplernn_2[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 102856\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=nc),\n",
    "        SimpleRNN(n_hidden, return_sequences=True, activation='relu', inner_init='identity'),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "    ])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "661403/661403 [==============================] - 125s - loss: 1.8411   \n",
      "Epoch 2/8\n",
      "661403/661403 [==============================] - 124s - loss: 1.6598   \n",
      "Epoch 3/8\n",
      "661403/661403 [==============================] - 126s - loss: 1.6290   \n",
      "Epoch 4/8\n",
      "661403/661403 [==============================] - 124s - loss: 1.6138   \n",
      "Epoch 5/8\n",
      "661403/661403 [==============================] - 124s - loss: 1.6043   \n",
      "Epoch 6/8\n",
      "661403/661403 [==============================] - 124s - loss: 1.5976   \n",
      "Epoch 7/8\n",
      "661403/661403 [==============================] - 124s - loss: 1.5925   \n",
      "Epoch 8/8\n",
      "661403/661403 [==============================] - 123s - loss: 1.5884   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49ce7a1190>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_rnn, y_rnn, batch_size=64, nb_epoch=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "661403/661403 [==============================] - 127s - loss: 1.5852   \n",
      "Epoch 2/4\n",
      "661403/661403 [==============================] - 124s - loss: 1.5825   \n",
      "Epoch 3/4\n",
      "661403/661403 [==============================] - 124s - loss: 1.5803   \n",
      "Epoch 4/4\n",
      "661403/661403 [==============================] - 124s - loss: 1.5783   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49e5eec450>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_rnn, y_rnn, batch_size=64, nb_epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nexts_keras(m, inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arrs = np.array(idxs)[np.newaxis,:]\n",
    "    p = m.predict(arrs)[0]\n",
    "    print(list(inp))\n",
    "    return [chars[np.argmax(o)] for o in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 't', 'h', 'i', 's', ' ', 'i', 's']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ', 'h', 'e', 's', ' ', 's', 's', ' ']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts_keras(model, ' this is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'p', 'a', 'r', 't', ' ', 'o', 'f']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ', 'r', 'r', 't', ' ', 'o', 'f', ' ']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts_keras(model, ' part of')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['q', 'u', 'e', 'e', 'n', 's', ' ', 'a']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['u', 'i', 'e', 'n', ',', ' ', 'o', 'n']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts_keras(model, 'queens a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save4_path = model_path + 'save4.h5'\n",
    "if not os.path.exists(save4_path):\n",
    "    model.save_weights(save4_path)\n",
    "model.load_weights(save4_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful model with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs = 64\n",
    "nc = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_in_dat = [[idx[i+n] for i in xrange(0, len(idx)-1-nc, nc)]\n",
    "           for n in range(nc)]\n",
    "c_out_dat = [[idx[i+n] for i in xrange(1, len(idx)-nc, nc)]\n",
    "            for n in range(nc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs = [np.stack(c) for c in c_in_dat]\n",
    "xs = np.concatenate([[np.array(o)] for o in xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ys = [np.stack(c) for c in c_out_dat]\n",
    "ys = np.concatenate([[np.array(o)] for o in ys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 132280), (40, 132280))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_rnn = np.stack(np.squeeze(xs), axis=1)\n",
    "y_rnn = np.atleast_3d(np.stack(ys, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((132280, 40), (132280, 40, 1))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_rnn.shape, y_rnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_model(batch_size_override=None):\n",
    "    if batch_size_override is None:\n",
    "        batch_size_override = bs\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=nc, batch_input_shape=(batch_size_override,nc)),\n",
    "        LSTM(n_hidden, input_dim=n_fac, return_sequences=True, stateful=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        Dropout(0.2),\n",
    "        LSTM(n_hidden, input_dim=n_fac, return_sequences=True, stateful=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        Dropout(0.2),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "    ])\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_example(m, seed, gen_length=320):\n",
    "    pred_m = make_model(batch_size_override=1) # batch_size_override is the important bit\n",
    "    for layer, pred_layer in zip(m.layers, pred_m.layers):\n",
    "        pred_layer.set_weights(layer.get_weights())\n",
    "    \n",
    "    output = seed\n",
    "    for i in range(gen_length):\n",
    "        x = np.array([char_indices[c] for c in output[-nc:]])[np.newaxis,:]\n",
    "        preds = pred_m.predict(x, verbose=0, batch_size=1)[0][-1]\n",
    "        preds = preds / np.sum(preds)\n",
    "        output += np.random.choice(chars, p=preds)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epochs(m, num_epochs=12, seed='In the cathedral church of Westminster, '):\n",
    "    for i in range(num_epochs):\n",
    "        m.fit(x_rnn[:mx], y_rnn[:mx], batch_size=bs, nb_epoch=1, shuffle=False)\n",
    "        print_example(m, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=nc, batch_input_shape=(bs,nc)),\n",
    "        LSTM(n_hidden, input_dim=n_fac, return_sequences=True, stateful=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        Dropout(0.2),\n",
    "        LSTM(n_hidden, input_dim=n_fac, return_sequences=True, stateful=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        Dropout(0.2),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_3 (Embedding)          (64, 40, 42)          3696        embedding_input_3[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (64, 40, 256)         306176      embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (64, 40, 256)         0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (64, 40, 256)         525312      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (64, 40, 256)         0           lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_2 (TimeDistribute(64, 40, 88)          22616       dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 857800\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using a fixed batch shape, we have to ensure our inputs and outputs are an even multiple of the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mx = len(x_rnn)//bs*bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 234s - loss: 2.1258   \n",
      "In the cathedral church of Westminster, you day,\n",
      "    Though mettert of gave his plow of will plois. There's my neemon to it not me;\n",
      "    Mehechon- my reblecgn.\n",
      "\n",
      "             Exeunt\n",
      "\n",
      "Aen inety it frriene.\n",
      "  SPERYEN. Nore; why, to gordand\n",
      "    Sou t    Exition]TES. So known beingor I may bring somest the compuse we have belamouch.\n",
      "    I live you was pra\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 234s - loss: 1.7051   \n",
      "In the cathedral church of Westminster, on my now to beseech; most not but\n",
      "    faith, impect dear dir good I warry cruetion\n",
      "    are be like of fullow now, with he have to see\n",
      "    A warries the seed it; no please your sneed- the giull on\n",
      "    Unprovest.\n",
      "  SERVAND. Of he speech for they has never re that it with their here into was whom, sorrow this nut ta\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 235s - loss: 1.6198   \n",
      "In the cathedral church of Westminster, ifing of lady. The Kerys; night are your look-and\n",
      "    The unartion.             Exeunt PLOOPELLES\n",
      "\n",
      "Enter REMNNC\n",
      "\n",
      "  THORD. No, dread him       Ender THORMOLA. Hone herds to that boy, it do,\n",
      "    As with a father.\n",
      "  CLOWN. Look gardity drumber; priva; ex the fentle grave of the pasting\n",
      "    in she's lett'r,\n",
      "    My\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 236s - loss: 1.5778   \n",
      "In the cathedral church of Westminster, sir,\n",
      "    By the full me melcontring with sir, 'then sake all by a honour good all in French\n",
      "    On.\n",
      "  TILLIA. Sir, for fortune,\n",
      "    War.\n",
      "  PAULINA. By charge.\n",
      "  CLOREZRIO. One of serve learness or I'll says which make me you have citues, but how bress a desirn e't state one\n",
      "    Umplays thee.\n",
      "    How be the daug\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 235s - loss: 1.5528   \n",
      "In the cathedral church of Westminster, and in hours and the King letter, but not her , as you are to attontures.\n",
      "  SHERMALA. No, for but me, not know thee be a fire it.\n",
      "    I am the old heavy show to ud,\n",
      "    The change wethat know upon his lifes.\n",
      "\n",
      "                                  See the adam bear your more to us't;\n",
      "    I shall nave the the dried lea\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 236s - loss: 1.5354   \n",
      "In the cathedral church of Westminster, give me, fecter'd are hirsember of leave is soch to a honour hoss.\n",
      "    The heart; it?\n",
      "  CLOWN. Which had conterphick, in the shaper of it;\n",
      "    Kere perbute the wise,\n",
      "    And those behold-season's profige's crown, as I am I thanks of us,  know a wife\n",
      "    bear to our more bittince waste at than\n",
      "    besore to birdne\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 235s - loss: 1.5234   \n",
      "In the cathedral church of Westminster, and when the good; then them are with see the four for th' eyes I am stir.\n",
      "  FLORIZEL. Be the gate part of eyes and then; my fair\n",
      "    Or not repead-fellow towards he love.\n",
      "  SPEEDRES. Ahe a such a now\n",
      "    as our house hath his house I cannot gentleman!\n",
      "  MARIA. This growning\n",
      "    My gentleman in green\n",
      "    To her.\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 237s - loss: 1.5139   \n",
      "In the cathedral church of Westminster, sir, but they have\n",
      "    her accick\n",
      "    We not and presentle right him\n",
      "    But yet they shall the thoughtss of fair men,\n",
      "    So. Now, good thou Havns be did your appear of a STY, the laugh now he blush me shall present it to a wonder of the knrange you are betodch'd these find my ratress\n",
      "    With a queen becime hono\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 238s - loss: 1.5068   \n",
      "In the cathedral church of Westminster, I say:\n",
      "    Give us the scorn eyes\n",
      "    inll thoughts as you are the daughter, of it, what entereus\n",
      "    anseesion the sword shall the own pagand of the reason.\n",
      "   And I rest not are I thank accas'd to all\n",
      "    with a like this, costs the pastoice much; so disease. It say the will dischief\n",
      "    letter, mother, she's\n",
      "\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 235s - loss: 1.5003   \n",
      "In the cathedral church of Westminster, and in me.\n",
      "    Him stay pursution have e us'd to me.\n",
      "    And his sickly death in the rages him a kingdom in\n",
      "    for all the rape of I'll the\n",
      "    fair any in the our own hide and yea, my lord,\n",
      "    And what said my chamber, great bent, a\n",
      "    glited, and singles to-night, and thou prospar as you\n",
      "    i Sme? I will t\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 237s - loss: 1.4954   \n",
      "In the cathedral church of Westminster, to hear this ornaw proud bear him, a too merts begin of you to something! But see you much still, but even lege\n",
      "    I'll love it and music down your give to him dries\n",
      "    Uncherry there of my own world well.\n",
      "    More Times, my Lord, you not my poor day!\n",
      "  LEONTES. I cannot protess, any what apt sweet\n",
      "    Which mar\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 237s - loss: 1.4909   \n",
      "In the cathedral church of Westminster, in hancle.\n",
      "    But canst thou art at e'er this catch give him\n",
      "    To paint, my master, the fool of pance commit my father,\n",
      "    But be bold, sir; all it greed\n",
      "    After you\n",
      "    more and frebomes have of his spirit here\n",
      "    That I'll\n",
      "    proud. No, Love, merty fair tongue\n",
      "    To he is not a boy. 'Tis before thy l\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 238s - loss: 1.4872   \n",
      "In the cathedral church of Westminster, now\n",
      "    But my peace, taintures to be\n",
      "    dispostate court your hand; and we did not few, you were we\n",
      "    I'll be tefee for my night,\n",
      " '   That should thou seam! What should see again.\n",
      "    Come, I she's not prisoners better we know.\n",
      "    To-old lords, nor I must not fury,\n",
      "    I see him go, have you shall be the s\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 237s - loss: 1.4840   \n",
      "In the cathedral church of Westminster, and my care be rusted,\n",
      "    Good of you withen, and Lady, was these here!\n",
      "    My lord; this who underself for Apome\n",
      "    to know to the wan is evil. What hath she be believe my fivol\n",
      "    Then we proad memitue: a deeds, but comes her answer upon the\n",
      "    it humour's a fear'st through\n",
      "    buriing lay for bience more g\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 240s - loss: 1.4812   \n",
      "In the cathedral church of Westminster, shall I sayes them\n",
      "    And in whileness like men brief any born is her, tell\n",
      "    him?\n",
      "  VALENTINE.  [Aside] days fortune for you.\n",
      "                                        Thou shalt prison'd to three. He\n",
      "    have greet    Perding the open  here stand his worst to see thee as gallain me comes to past, and I am doth \n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 239s - loss: 1.4784   \n",
      "In the cathedral church of Westminster, now with him, I have aught furthers\n",
      "    she.\n",
      "  LAUNCE. Y boy's\n",
      "    I hilf on his come and more predent it.\n",
      "  LEONTES. None says the point: my lord- he not sucend and letter.\n",
      "  SPEED. What's note,\n",
      "    Of re is more\n",
      "    she shall no presently your enemies, and I art accus'd me; and thou art a\n",
      "    Shall have hope \n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 235s - loss: 1.4754   \n",
      "In the cathedral church of Westminster, princes.\n",
      "  PAROLLES. I will no obfafty.\n",
      "  JULIA. Good it, let's not break with my father.\n",
      "  CRINCE. O, thi does the clifper. Make you when humour'd like to them\n",
      "    But she are an answer have her]\n",
      "    Cleantue, not what she's\n",
      "    How\n",
      "    Pirch'd now, my service. So court\n",
      "    True is to her and following-\n",
      "  AUT\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 238s - loss: 1.4734   \n",
      "In the cathedral church of Westminster, good King:\n",
      "    And lords, as my heart honesty, us. Something is not thou a boy?\n",
      "  CLOWN. Faith, O though where helches' uncle.\n",
      "  SPEED. 'Tis bid a new gockro'st, whereof ht my body\n",
      "    deed themselves.\n",
      "  OLIVIA. He shall they came I know not here?\n",
      "    Why is not but made the earth I have rememb'red my company, an\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 238s - loss: 1.4714   \n",
      "In the cathedral church of Westminster, in the dain\n",
      "    Pastation and die; which this through tide me\n",
      "    my leave.\n",
      "                          Speak; I'll be travel of my abper.      Exit\n",
      "  SILVIUS. But, is curse-famous o'er day; and more, you sway of\n",
      "    sometime it no further, but you must under't.\n",
      "  PATROCLUS. Think, where from the griefs and father \n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 237s - loss: 1.4693   \n",
      "In the cathedral church of Westminster, 'tis more fresh'd\n",
      "    This father\n",
      "    but ever be board a predictors with\n",
      "    In my pince, but such a company!\n",
      "    But there shall bear the bloody of offend false.                             Exit\n",
      "  POLIXENES. Gavollus, for madam, nxocellent recovil we had\n",
      "    That I dord hnr brasty forest\n",
      "    Has hear in you,\n",
      "\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 237s - loss: 1.4672   \n",
      "In the cathedral church of Westminster, out of and love\n",
      "    Be not your master?\n",
      "  VIOLA. Peacely forg, tress;\n",
      "    If thou says after to can at walk, it proud. O, fie, and\n",
      "    I will not I know not the moves yet\n",
      "    As she have heard:\n",
      "    Dust they must pales to a peril ng.\n",
      "  SECOND LADY. What shall die this carries; and who hath to him\n",
      "    A man, I s\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 237s - loss: 1.4657   \n",
      "In the cathedral church of Westminster, lady,\n",
      "    In the edge; she are a passions to wait it be,\n",
      "    Thou hast chhall be suffer-fittle.\n",
      "    Thou hast abyremiemation that's well things,\n",
      "    And excellent but decorded in y fashing how it: come.\n",
      "  AGUECHEEK. Why, get you on After Proteus better; for that fill me no.\n",
      "  SARORLUS. 'A would such surely colour\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 236s - loss: 1.4639   \n",
      "In the cathedral church of Westminster, I am cheek which\n",
      "    makes you, thou's a\n",
      "    gallant he's a bloody lady he would not fair\n",
      "    More of Guck againing evy to live by her; there parted him- a is the ave sail\n",
      "    The hot and where he know you bemore the ou, he would break their greech; they will nit you, sir; from him\n",
      "    Therefore well-basely so cil\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 236s - loss: 1.4624   \n",
      "In the cathedral church of Westminster, but\n",
      "    which place of kands be one calls de'd well.\n",
      "    For 'tis at the no more.\n",
      "\n",
      "  FLUELLEN. Heur caluze, not on the TROILUS\n",
      "  DEMETRIUS. O,\n",
      "    Axpridence!\n",
      "  IOLAX. But I did well will\n",
      "    which I enfrain of heavy 't has been the rumber,\n",
      "    Than the was off stands as that is commosey,\n",
      "    Unto the gentlem\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 240s - loss: 1.4613   \n",
      "In the cathedral church of Westminster, hang, my father's sheep\n",
      "    This is follow of lady, to merry, graciously a,\n",
      "    Words of that accorn'd like you! What was now with his paxe me\n",
      "    Break your tears such a true employ of here, good smell's man your battle\n",
      "    Which the minister ten, before you now\n",
      "    That I have weak it quarrellow too. For the hea\n"
     ]
    }
   ],
   "source": [
    "run_epochs(model, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save5_path = model_path + 'save5.h5'\n",
    "if not os.path.exists(save5_path):\n",
    "    model.save_weights(save5_path)\n",
    "model.load_weights(save5_path)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
