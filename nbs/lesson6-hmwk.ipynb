{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Setup\n",
    "\n",
    "We're going to download the collected plays of Shakespeare to use as our data.\n",
    "\n",
    "Source: http://www.gutenberg.org/cache/epub/100/pg100.txt\n",
    "\n",
    "The original source was preprocessed to remove sonnets and non-Shakesperean text added by Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = BASE_DIR + '/data/shakespeare/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_path = DATA_DIR + 'models/'\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('corpus length:', 5291227)\n"
     ]
    }
   ],
   "source": [
    "data = DATA_DIR + 'gutenberg_shakespeare_modified.txt' # preprocessed\n",
    "\n",
    "with open(data, 'r') as f:\n",
    "    text = f.read()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total chars:', 88)\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's useful to have a zero value in the dataset, e.g. for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars.insert(0, \"\\0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00\\n\\r !\"&\\'(),-.0123456789:;<?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_`abcdefghijklmnopqrstuvwxyz|}\\xbb\\xbf\\xef'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map chars to indices and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\x00': 0, ' ': 3, '(': 8, ',': 10, '0': 13, '4': 17, '8': 21, '\\xbb': 85, '<': 25, '\\xbf': 86, 'D': 30, 'H': 34, 'L': 38, 'P': 42, 'T': 46, 'X': 50, '`': 56, 'd': 60, 'h': 64, 'l': 68, '\\xef': 87, 'p': 72, 't': 76, 'x': 80, '|': 83, \"'\": 7, '3': 16, '7': 20, ';': 24, '?': 26, 'C': 29, 'G': 33, 'K': 37, 'O': 41, 'S': 45, 'W': 49, '[': 53, '_': 55, 'c': 59, 'g': 63, 'k': 67, 'o': 71, 's': 75, 'w': 79, '\\n': 1, '\"': 5, '&': 6, '.': 12, '2': 15, '6': 19, ':': 23, 'B': 28, 'F': 32, 'J': 36, 'N': 40, 'R': 44, 'V': 48, 'Z': 52, 'b': 58, 'f': 62, 'j': 66, 'n': 70, 'r': 74, 'v': 78, 'z': 82, '\\r': 2, '!': 4, ')': 9, '-': 11, '1': 14, '5': 18, '9': 22, 'A': 27, 'E': 31, 'I': 35, 'M': 39, 'Q': 43, 'U': 47, 'Y': 51, ']': 54, 'a': 57, 'e': 61, 'i': 65, 'm': 69, 'q': 73, 'u': 77, 'y': 81, '}': 84}\n"
     ]
    }
   ],
   "source": [
    "print(char_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*idx* converts the Shakepearean text to character indices (based on the *char_indices* mapping above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = [char_indices[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[87, 85, 86, 45, 29, 31, 40, 31, 23, 2, 1, 44, 71, 77, 75, 65, 68, 68, 71, 70, 24, 3, 42, 57, 74, 65, 75, 24, 3, 32, 68, 71, 74, 61, 70, 59, 61, 24, 3, 39, 57, 74, 75, 61, 65, 68, 68, 61, 75, 2, 1, 2, 1, 2, 1, 27, 29, 46, 3, 35, 12, 3, 45, 29, 31, 40, 31, 3, 14, 12]\n"
     ]
    }
   ],
   "source": [
    "print(idx[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xef\\xbb\\xbfSCENE:\\r\\nRousillon; Paris; Florence; Marseilles\\r\\n\\r\\n\\r\\nACT I. SCENE 1.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(indices_char[i] for i in idx[:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 char model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBALS needed from this point on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, merge, SimpleRNN, TimeDistributed\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fac = 42 # number of latent factors (size of embedding matrix)\n",
    "n_hidden = 256 # hyperparameter: size of hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inputs\n",
    "\n",
    "Create a list of every 4th character, starting at the 0th, 1st, 2nd, then 3rd characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nc = 3 # num chars\n",
    "c1_dat = [idx[i] for i in xrange(0, len(idx)-1-nc, nc)]\n",
    "c2_dat = [idx[i+1] for i in xrange(0, len(idx)-1-nc, nc)]\n",
    "c3_dat = [idx[i+2] for i in xrange(0, len(idx)-1-nc, nc)]\n",
    "c4_dat = [idx[i+3] for i in xrange(0, len(idx)-1-nc, nc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5291223, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0, len(idx)-1-nc, nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1763741, 1763741)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c1_dat), len(c4_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Out inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = np.stack(c1_dat)\n",
    "x2 = np.stack(c2_dat)\n",
    "x3 = np.stack(c3_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.stack(c4_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1763741,), (1763741,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create inputs and embedding outputs for each of our 3 character inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embedding_input(name, n_in, n_out):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name+'_in')\n",
    "    emb = Embedding(n_in, n_out, input_length=1, name=name+'_emb')(inp)\n",
    "    return inp, Flatten()(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c1_in, c1_emb = embedding_input('c1', vocab_size, n_fac)\n",
    "c2_in, c2_emb = embedding_input('c2', vocab_size, n_fac)\n",
    "c3_in, c3_emb = embedding_input('c3', vocab_size, n_fac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![3char](./3char.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dense_in` is the 'green arrow' in the diagram - the layer operation from input to hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dense_in = Dense(n_hidden, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first hidden activation is simply this function applied to the result of the embedding of the first character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c1_hidden = dense_in(c1_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dense_hidden` is the 'orange arrow' from our diagram - the layer operation from hidden to hidden\n",
    "\n",
    "_Note:_ unsure why the activation for this is `tanh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_hidden = Dense(n_hidden, activation='tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second and third activations sum up the previous hidden state (after applying `dense_hidden`) to the new input state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge([new input state, orange arrow from previous hidden state])\n",
    "c2_hidden = merge([dense_in(c2_emb), dense_hidden(c1_hidden)])\n",
    "c3_hidden = merge([dense_in(c3_emb), dense_hidden(c2_hidden)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dense_out` is the 'blue arrow' from our diagram - the layer operation from hidden to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_out = Dense(vocab_size, activation='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third hidden state is the input to our output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c4_out = dense_out(c3_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Model([c1_in, c2_in, c3_in], c4_out)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.optimizer.lr=0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "c1_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c2_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c1_emb (Embedding)               (None, 1, 42)         3696        c1_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "c2_emb (Embedding)               (None, 1, 42)         3696        c2_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 42)            0           c1_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 256)           11008       flatten_1[0][0]                  \n",
      "                                                                   flatten_2[0][0]                  \n",
      "                                                                   flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "c3_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 42)            0           c2_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c3_emb (Embedding)               (None, 1, 42)         3696        c3_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 256)           65792       dense_1[0][0]                    \n",
      "                                                                   merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 42)            0           c3_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 256)           0           dense_1[1][0]                    \n",
      "                                                                   dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 256)           0           dense_1[2][0]                    \n",
      "                                                                   dense_2[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 88)            22616       merge_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 110504\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1763741/1763741 [==============================] - 126s - loss: 3.6386   \n",
      "Epoch 2/4\n",
      "1763741/1763741 [==============================] - 128s - loss: 3.1218   \n",
      "Epoch 3/4\n",
      "1763741/1763741 [==============================] - 122s - loss: 3.0424   \n",
      "Epoch 4/4\n",
      "1763741/1763741 [==============================] - 126s - loss: 2.9566   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a1b992990>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x1, x2, x3], y, batch_size=64, nb_epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1763741/1763741 [==============================] - 128s - loss: 2.8626   \n",
      "Epoch 2/4\n",
      "1763741/1763741 [==============================] - 126s - loss: 2.7697   \n",
      "Epoch 3/4\n",
      "1763741/1763741 [==============================] - 128s - loss: 2.6858   \n",
      "Epoch 4/4\n",
      "1763741/1763741 [==============================] - 122s - loss: 2.6146   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a1a351e50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x1, x2, x3], y, batch_size=64, nb_epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1763741/1763741 [==============================] - 123s - loss: 2.5550   \n",
      "Epoch 2/4\n",
      "1763741/1763741 [==============================] - 126s - loss: 2.5057   \n",
      "Epoch 3/4\n",
      "1763741/1763741 [==============================] - 131s - loss: 2.4651   \n",
      "Epoch 4/4\n",
      "1763741/1763741 [==============================] - 123s - loss: 2.4316   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a1b838550>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x1, x2, x3], y, batch_size=64, nb_epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1763741/1763741 [==============================] - 131s - loss: 2.4037   \n",
      "Epoch 2/4\n",
      "1763741/1763741 [==============================] - 125s - loss: 2.3800   \n",
      "Epoch 3/4\n",
      "1763741/1763741 [==============================] - 126s - loss: 2.3595   \n",
      "Epoch 4/4\n",
      "1763741/1763741 [==============================] - 130s - loss: 2.3416   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a1eaa5550>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x1, x2, x3], y, batch_size=64, nb_epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1763741/1763741 [==============================] - 126s - loss: 2.3256   \n",
      "Epoch 2/10\n",
      "1763741/1763741 [==============================] - 128s - loss: 2.3111   \n",
      "Epoch 3/10\n",
      "1763741/1763741 [==============================] - 126s - loss: 2.2980   \n",
      "Epoch 4/10\n",
      "1763741/1763741 [==============================] - 121s - loss: 2.2859   \n",
      "Epoch 5/10\n",
      "1763741/1763741 [==============================] - 128s - loss: 2.2748   \n",
      "Epoch 6/10\n",
      "1763741/1763741 [==============================] - 124s - loss: 2.2645   \n",
      "Epoch 7/10\n",
      "1763741/1763741 [==============================] - 125s - loss: 2.2550   \n",
      "Epoch 8/10\n",
      "1763741/1763741 [==============================] - 125s - loss: 2.2462   \n",
      "Epoch 9/10\n",
      "1763741/1763741 [==============================] - 128s - loss: 2.2380   \n",
      "Epoch 10/10\n",
      "1763741/1763741 [==============================] - 128s - loss: 2.2304   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a1b838750>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x1, x2, x3], y, batch_size=64, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save1_path = model_path + 'save1.h5'\n",
    "if not os.path.exists(save1_path):\n",
    "    model.save_weights(save1_path)\n",
    "model.load_weights(save1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"`newaxis` is used to increase the dimension of the existing array by one more dimension, when used once\" - [source](https://stackoverflow.com/questions/29241056/the-use-of-numpy-newaxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next(m, inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arrs = [np.array(i)[np.newaxis] for i in idxs]\n",
    "    p = m.predict(arrs)\n",
    "    i = np.argmax(p)\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(model, 'phi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(model, ' th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(model, ' an')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first RNN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBALS needed from this point on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nc = 8 # numChars == size of our unrolled RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xs` (+ `c_in_dat`), `y` (+ `c_out_dat`), `cs` (+ `embedding_input()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inputs\n",
    "\n",
    "Now let's try predicting char 9 using chars 1-8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of 0 through 7, create a list of every 8th character with that starting point. These will be the 8 inputs to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_in_dat = [[idx[i+n] for i in xrange(0, len(idx)-1-nc, nc)]\n",
    "           for n in range(nc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a list of the next character in each of these series. This will be the labels for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_out_dat = [idx[i+nc] for i in xrange(0, len(idx)-1-nc, nc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs = [np.stack(c) for c in c_in_dat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, (661403,))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xs), xs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.stack(c_out_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each column below is one series of 8 characters from the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([87, 23, 68, 74, 74, 57, 75, 29]),\n",
       " array([85,  2, 68, 65, 61, 74,  2, 46]),\n",
       " array([86,  1, 71, 75, 70, 75,  1,  3]),\n",
       " array([45, 44, 70, 24, 59, 61,  2, 35]),\n",
       " array([29, 71, 24,  3, 61, 65,  1, 12]),\n",
       " array([31, 77,  3, 32, 24, 68,  2,  3]),\n",
       " array([40, 75, 42, 68,  3, 68,  1, 45]),\n",
       " array([31, 65, 57, 71, 39, 61, 27, 29])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[xs[n][:nc] for n in range(nc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and this is the next character after each sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 68, 74, 74, 57, 75, 29, 31])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:nc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_input(name, n_in, n_out):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name+'_in')\n",
    "    emb = Embedding(n_in, n_out, input_length=1, name=name+'_emb')(inp)\n",
    "    return inp, Flatten()(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cs = [embedding_input('c'+str(n), vocab_size, n_fac) for n in range(nc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"I'd suggest trying the trick I mentioned in the lesson for simple RNNs: using an identity matrix to initialize your hidden state, and use relu instead of tanh.\" - [Jeremy on forums](http://forums.fast.ai/t/purpose-of-rnns-and-theano/242/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dense_in = Dense(n_hidden, activation='relu')\n",
    "dense_hidden = Dense(n_hidden, activation='relu', init='identity')\n",
    "dense_out = Dense(vocab_size, activation='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding of the first character of each sequence goes through `dense_in` to create our first hidden activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden = dense_in(cs[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for each successive layer, we combine the output of `dense_in` on the next character with the output of `dense_hidden` on the current hidden state to create the new hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1, nc):\n",
    "    dense = dense_in(cs[i][1])\n",
    "    hidden = dense_hidden(hidden)\n",
    "    hidden = merge([dense, hidden])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the final hidden state through `dense_out` gives us our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = dense_out(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "c0_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c1_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c0_emb (Embedding)               (None, 1, 42)         3696        c0_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "c1_emb (Embedding)               (None, 1, 42)         3696        c1_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 42)            0           c0_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 256)           11008       flatten_4[0][0]                  \n",
      "                                                                   flatten_5[0][0]                  \n",
      "                                                                   flatten_6[0][0]                  \n",
      "                                                                   flatten_7[0][0]                  \n",
      "                                                                   flatten_8[0][0]                  \n",
      "                                                                   flatten_9[0][0]                  \n",
      "                                                                   flatten_10[0][0]                 \n",
      "                                                                   flatten_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "c2_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 42)            0           c1_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 256)           65792       dense_4[0][0]                    \n",
      "                                                                   merge_3[0][0]                    \n",
      "                                                                   merge_4[0][0]                    \n",
      "                                                                   merge_5[0][0]                    \n",
      "                                                                   merge_6[0][0]                    \n",
      "                                                                   merge_7[0][0]                    \n",
      "                                                                   merge_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "c2_emb (Embedding)               (None, 1, 42)         3696        c2_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "c3_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 42)            0           c2_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_3 (Merge)                  (None, 256)           0           dense_4[1][0]                    \n",
      "                                                                   dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "c3_emb (Embedding)               (None, 1, 42)         3696        c3_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "c4_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)              (None, 42)            0           c3_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_4 (Merge)                  (None, 256)           0           dense_4[2][0]                    \n",
      "                                                                   dense_5[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "c4_emb (Embedding)               (None, 1, 42)         3696        c4_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "c5_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)              (None, 42)            0           c4_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_5 (Merge)                  (None, 256)           0           dense_4[3][0]                    \n",
      "                                                                   dense_5[2][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "c5_emb (Embedding)               (None, 1, 42)         3696        c5_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "c6_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)              (None, 42)            0           c5_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_6 (Merge)                  (None, 256)           0           dense_4[4][0]                    \n",
      "                                                                   dense_5[3][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "c6_emb (Embedding)               (None, 1, 42)         3696        c6_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "c7_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)             (None, 42)            0           c6_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_7 (Merge)                  (None, 256)           0           dense_4[5][0]                    \n",
      "                                                                   dense_5[4][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "c7_emb (Embedding)               (None, 1, 42)         3696        c7_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)             (None, 42)            0           c7_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_8 (Merge)                  (None, 256)           0           dense_4[6][0]                    \n",
      "                                                                   dense_5[5][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "merge_9 (Merge)                  (None, 256)           0           dense_4[7][0]                    \n",
      "                                                                   dense_5[6][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 88)            22616       merge_9[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 128984\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([c[0] for c in cs], out)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "661403/661403 [==============================] - 89s - loss: 2.0080    \n",
      "Epoch 2/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.8291    \n",
      "Epoch 3/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.7723    \n",
      "Epoch 4/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.7404    \n",
      "Epoch 5/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.7189    \n",
      "Epoch 6/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.7034    \n",
      "Epoch 7/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.6912    \n",
      "Epoch 8/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6812    \n",
      "Epoch 9/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.6738    \n",
      "Epoch 10/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6671    \n",
      "Epoch 11/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6615    \n",
      "Epoch 12/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6561    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a036c9b10>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xs, y, batch_size=64, nb_epoch=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next(m, inp):\n",
    "    arrs = [np.array(char_indices[c])[np.newaxis] for c in inp]\n",
    "    p = m.predict(arrs)\n",
    "    return chars[np.argmax(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(model, 'for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(model, 'part of ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(model, 'queens a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a helper function for generating `k` additional words (separated by whitespace) in a starter sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_seq(m, inp, k):\n",
    "    k_count = 0\n",
    "    seq = inp\n",
    "    while k_count < k+1:\n",
    "        pc = get_next(m, inp)\n",
    "        seq += pc\n",
    "        inp = inp[1:] + pc\n",
    "        if (pc == ' '):\n",
    "            k_count += 1\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queens and the son the roper the roper the roper the roper '"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'queens a', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'part of the roper the roper the roper the roper the roper the '"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'part of ', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for thos  a dount of the roper the roper the roper '"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'for thos', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model currently seems to 'fixate' phrases like: \"the some sore\" or \"the roper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "661403/661403 [==============================] - 93s - loss: 1.6514    \n",
      "Epoch 2/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6475    \n",
      "Epoch 3/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6451    \n",
      "Epoch 4/12\n",
      "661403/661403 [==============================] - 90s - loss: 1.6412    \n",
      "Epoch 5/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.6385    \n",
      "Epoch 6/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6363    \n",
      "Epoch 7/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6338    \n",
      "Epoch 8/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6311    \n",
      "Epoch 9/12\n",
      "661403/661403 [==============================] - 94s - loss: 1.6292    \n",
      "Epoch 10/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6272    \n",
      "Epoch 11/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.6256    \n",
      "Epoch 12/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6249    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a036c9fd0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xs, y, batch_size=64, nb_epoch=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queens and the sor of the sor of the sor of the '"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'queens a', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'part of the sor of the sor of the sor of the sor '"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'part of ', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for thos  h and the sor of the sor of the '"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'for thos', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6231    \n",
      "Epoch 2/12\n",
      "661403/661403 [==============================] - 96s - loss: 1.6210    \n",
      "Epoch 3/12\n",
      "661403/661403 [==============================] - 99s - loss: 1.6199    \n",
      "Epoch 4/12\n",
      "661403/661403 [==============================] - 93s - loss: 1.6187    \n",
      "Epoch 5/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.6178    \n",
      "Epoch 6/12\n",
      "661403/661403 [==============================] - 95s - loss: 1.6167    \n",
      "Epoch 7/12\n",
      "661403/661403 [==============================] - 95s - loss: 1.6169    \n",
      "Epoch 8/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.6154    \n",
      "Epoch 9/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.6145    \n",
      "Epoch 10/12\n",
      "661403/661403 [==============================] - 93s - loss: 1.6136    \n",
      "Epoch 11/12\n",
      "661403/661403 [==============================] - 90s - loss: 1.6126    \n",
      "Epoch 12/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.6121    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a036c9e10>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xs, y, batch_size=64, nb_epoch=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queens and the hand the hand the hand the hand the hand '"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'queens a', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'part of the gonder the sore the gonder the sore the gonder the '"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'part of ', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for thos of the gonder the sore the gonder the sore the '"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq(model, 'for thos', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different 'fixation' phrases like: \"the best with\", \"the gonder the sore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save2_path = model_path + 'save2.h5'\n",
    "if not os.path.exists(save2_path):\n",
    "    model.save_weights(save2_path)\n",
    "model.load_weights(save2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first RNN with keras!\n",
    "\n",
    "This is nearly equivalent to the RNN we built ourselves in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 8, 42)         3696        embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_1 (SimpleRNN)          (None, 256)           76544       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 88)            22616       simplernn_1[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 102856\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=nc),\n",
    "        SimpleRNN(n_hidden, activation='relu', inner_init='identity'),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid `IndexError: axis 1 out of bounds [0, 1)`: http://forums.fast.ai/t/lesson-6-discussion/245/70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.3725    \n",
      "Epoch 2/12\n",
      "661403/661403 [==============================] - 87s - loss: 1.3684    \n",
      "Epoch 3/12\n",
      "661403/661403 [==============================] - 87s - loss: 1.3657    \n",
      "Epoch 4/12\n",
      "661403/661403 [==============================] - 87s - loss: 1.3619    \n",
      "Epoch 5/12\n",
      "661403/661403 [==============================] - 87s - loss: 1.3594    \n",
      "Epoch 6/12\n",
      "661403/661403 [==============================] - 87s - loss: 1.3570    \n",
      "Epoch 7/12\n",
      "661403/661403 [==============================] - 87s - loss: 1.3547    \n",
      "Epoch 8/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.3532    \n",
      "Epoch 9/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.3516    \n",
      "Epoch 10/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.3496    \n",
      "Epoch 11/12\n",
      "661403/661403 [==============================] - 90s - loss: 1.3524    \n",
      "Epoch 12/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.3484    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49f2b4f090>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.fit(np.concatenate([x[np.newaxis] for x in xs]).T, y, batch_size=64, nb_epoch=12)\n",
    "model.fit(np.concatenate(xs, axis=1), y, batch_size=64, nb_epoch=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_keras(m, inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arrs = np.array(idxs)[np.newaxis,:]\n",
    "    p = m.predict(arrs)[0]\n",
    "    return chars[np.argmax(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_keras_seq(m, inp, k):\n",
    "    k_count = 0\n",
    "    seq = inp\n",
    "    while k_count < k+1:\n",
    "        pc = get_next_keras(m, inp)\n",
    "        seq += pc\n",
    "        inp = inp[1:] + pc\n",
    "        if (pc == ' '):\n",
    "            k_count += 1\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queens and the shall be the shall be the shall be the '"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_keras_seq(model, 'queens a', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'part of the shall be the shall be the shall be the shall '"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_keras_seq(model, 'part of ', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for those the shall be the shall be the shall be the '"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_keras_seq(model, 'for thos', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Fixations_: \"the sent\", \"the shall be\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.3467    \n",
      "Epoch 2/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.3468    \n",
      "Epoch 3/12\n",
      "661403/661403 [==============================] - 88s - loss: 1.3444    \n",
      "Epoch 4/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.3438    \n",
      "Epoch 5/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.3424    \n",
      "Epoch 6/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.3426    \n",
      "Epoch 7/12\n",
      "661403/661403 [==============================] - 89s - loss: 1.3420    \n",
      "Epoch 8/12\n",
      "661403/661403 [==============================] - 90s - loss: 1.3420    \n",
      "Epoch 9/12\n",
      "661403/661403 [==============================] - 90s - loss: 1.3404    \n",
      "Epoch 10/12\n",
      "661403/661403 [==============================] - 92s - loss: 1.3402    \n",
      "Epoch 11/12\n",
      "661403/661403 [==============================] - 90s - loss: 1.3393    \n",
      "Epoch 12/12\n",
      "661403/661403 [==============================] - 95s - loss: 1.3403    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49f1b72110>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.fit(np.concatenate([x[np.newaxis] for x in xs]).T, y, batch_size=64, nb_epoch=12)\n",
    "model.fit(np.concatenate(xs, axis=1), y, batch_size=64, nb_epoch=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queens and the strength the strength the strength the strength the strength '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_keras_seq(model, 'queens a', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'part of the strength the strength the strength the strength the strength the '"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_keras_seq(model, 'part of ', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for those thou shalt the strength the strength the strength the strength '"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_keras_seq(model, 'for thos', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Fixations_: \"the serve me\", \"the strength\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save3_path = model_path + 'save3.h5'\n",
    "if not os.path.exists(save3_path):\n",
    "    model.save_weights(save3_path)\n",
    "model.load_weights(save3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBALS needed from this point on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ys` (+ `c_out_dat`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inputs\n",
    "\n",
    "To use a sequence model, we can leave our input unchanged - but we have to change our output to a sequence.\n",
    "\n",
    "Here, `c_out_dat` is identical to `c_in_dat`, but moved across 1 character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_out_dat = [[idx[i+n] for i in xrange(1, len(idx)-nc, nc)]\n",
    "            for n in range(nc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ys = [np.stack(c) for c in c_out_dat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading down each column shows one set of inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[87],\n",
       "        [23],\n",
       "        [68],\n",
       "        [74],\n",
       "        [74],\n",
       "        [57],\n",
       "        [75],\n",
       "        [29]]), array([[85],\n",
       "        [ 2],\n",
       "        [68],\n",
       "        [65],\n",
       "        [61],\n",
       "        [74],\n",
       "        [ 2],\n",
       "        [46]]), array([[86],\n",
       "        [ 1],\n",
       "        [71],\n",
       "        [75],\n",
       "        [70],\n",
       "        [75],\n",
       "        [ 1],\n",
       "        [ 3]]), array([[45],\n",
       "        [44],\n",
       "        [70],\n",
       "        [24],\n",
       "        [59],\n",
       "        [61],\n",
       "        [ 2],\n",
       "        [35]]), array([[29],\n",
       "        [71],\n",
       "        [24],\n",
       "        [ 3],\n",
       "        [61],\n",
       "        [65],\n",
       "        [ 1],\n",
       "        [12]]), array([[31],\n",
       "        [77],\n",
       "        [ 3],\n",
       "        [32],\n",
       "        [24],\n",
       "        [68],\n",
       "        [ 2],\n",
       "        [ 3]]), array([[40],\n",
       "        [75],\n",
       "        [42],\n",
       "        [68],\n",
       "        [ 3],\n",
       "        [68],\n",
       "        [ 1],\n",
       "        [45]]), array([[31],\n",
       "        [65],\n",
       "        [57],\n",
       "        [71],\n",
       "        [39],\n",
       "        [61],\n",
       "        [27],\n",
       "        [29]])]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[xs[n][:nc] for n in range(nc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([85,  2, 68, 65, 61, 74,  2, 46]),\n",
       " array([86,  1, 71, 75, 70, 75,  1,  3]),\n",
       " array([45, 44, 70, 24, 59, 61,  2, 35]),\n",
       " array([29, 71, 24,  3, 61, 65,  1, 12]),\n",
       " array([31, 77,  3, 32, 24, 68,  2,  3]),\n",
       " array([40, 75, 42, 68,  3, 68,  1, 45]),\n",
       " array([31, 65, 57, 71, 39, 61, 27, 29]),\n",
       " array([23, 68, 74, 74, 57, 75, 29, 31])]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ys[n][:nc] for n in range(nc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_in = Dense(n_hidden, activation='relu')\n",
    "dense_hidden = Dense(n_hidden, activation='relu', init='identity')\n",
    "dense_out = Dense(vocab_size, activation='softmax', name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to pass a vectcor of all zeros as our starting point - here's our input layers for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp1 = Input(shape=(n_fac,), name='zeros')\n",
    "hidden = dense_in(inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outs = []\n",
    "\n",
    "for i in range(nc):\n",
    "    dense = dense_in(cs[i][1])\n",
    "    hidden = dense_hidden(hidden)\n",
    "    hidden = merge([dense, hidden], mode='sum')\n",
    "    # every layer now has an output\n",
    "    outs.append(dense_out(hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "c0_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c0_emb (Embedding)               (None, 1, 42)         3696        c0_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "zeros (InputLayer)               (None, 42)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 256)           11008       zeros[0][0]                      \n",
      "                                                                   flatten_4[0][0]                  \n",
      "                                                                   flatten_5[0][0]                  \n",
      "                                                                   flatten_6[0][0]                  \n",
      "                                                                   flatten_7[0][0]                  \n",
      "                                                                   flatten_8[0][0]                  \n",
      "                                                                   flatten_9[0][0]                  \n",
      "                                                                   flatten_10[0][0]                 \n",
      "                                                                   flatten_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 42)            0           c0_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c1_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 256)           65792       dense_8[0][0]                    \n",
      "                                                                   merge_10[0][0]                   \n",
      "                                                                   merge_11[0][0]                   \n",
      "                                                                   merge_12[0][0]                   \n",
      "                                                                   merge_13[0][0]                   \n",
      "                                                                   merge_14[0][0]                   \n",
      "                                                                   merge_15[0][0]                   \n",
      "                                                                   merge_16[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "c1_emb (Embedding)               (None, 1, 42)         3696        c1_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "merge_10 (Merge)                 (None, 256)           0           dense_8[1][0]                    \n",
      "                                                                   dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 42)            0           c1_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c2_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c2_emb (Embedding)               (None, 1, 42)         3696        c2_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "merge_11 (Merge)                 (None, 256)           0           dense_8[2][0]                    \n",
      "                                                                   dense_9[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 42)            0           c2_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c3_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c3_emb (Embedding)               (None, 1, 42)         3696        c3_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "merge_12 (Merge)                 (None, 256)           0           dense_8[3][0]                    \n",
      "                                                                   dense_9[2][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)              (None, 42)            0           c3_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c4_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c4_emb (Embedding)               (None, 1, 42)         3696        c4_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "merge_13 (Merge)                 (None, 256)           0           dense_8[4][0]                    \n",
      "                                                                   dense_9[3][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)              (None, 42)            0           c4_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c5_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c5_emb (Embedding)               (None, 1, 42)         3696        c5_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "merge_14 (Merge)                 (None, 256)           0           dense_8[5][0]                    \n",
      "                                                                   dense_9[4][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)              (None, 42)            0           c5_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c6_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "c6_emb (Embedding)               (None, 1, 42)         3696        c6_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "c7_in (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "merge_15 (Merge)                 (None, 256)           0           dense_8[6][0]                    \n",
      "                                                                   dense_9[5][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)             (None, 42)            0           c6_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "c7_emb (Embedding)               (None, 1, 42)         3696        c7_in[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)             (None, 42)            0           c7_emb[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_16 (Merge)                 (None, 256)           0           dense_8[7][0]                    \n",
      "                                                                   dense_9[6][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "merge_17 (Merge)                 (None, 256)           0           dense_8[8][0]                    \n",
      "                                                                   dense_9[7][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 88)            22616       merge_10[0][0]                   \n",
      "                                                                   merge_11[0][0]                   \n",
      "                                                                   merge_12[0][0]                   \n",
      "                                                                   merge_13[0][0]                   \n",
      "                                                                   merge_14[0][0]                   \n",
      "                                                                   merge_15[0][0]                   \n",
      "                                                                   merge_16[0][0]                   \n",
      "                                                                   merge_17[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 128984\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([inp1] + [c[0] for c in cs], outs)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(661403, 42)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = np.tile(np.zeros(n_fac), (len(xs[0]), 1))\n",
    "zeros.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "661403/661403 [==============================] - 216s - loss: 16.4263 - output_loss_1: 2.4570 - output_loss_2: 2.2423 - output_loss_3: 2.0530 - output_loss_4: 1.9849 - output_loss_5: 1.9383 - output_loss_6: 1.9218 - output_loss_7: 1.9160 - output_loss_8: 1.9132   \n",
      "Epoch 2/12\n",
      "661403/661403 [==============================] - 212s - loss: 15.2679 - output_loss_1: 2.4154 - output_loss_2: 2.1897 - output_loss_3: 1.9418 - output_loss_4: 1.8231 - output_loss_5: 1.7499 - output_loss_6: 1.7231 - output_loss_7: 1.7139 - output_loss_8: 1.7109   \n",
      "Epoch 3/12\n",
      "661403/661403 [==============================] - 214s - loss: 15.0269 - output_loss_1: 2.4129 - output_loss_2: 2.1850 - output_loss_3: 1.9264 - output_loss_4: 1.7911 - output_loss_5: 1.7072 - output_loss_6: 1.6760 - output_loss_7: 1.6657 - output_loss_8: 1.6626   \n",
      "Epoch 4/12\n",
      "661403/661403 [==============================] - 217s - loss: 14.9106 - output_loss_1: 2.4114 - output_loss_2: 2.1828 - output_loss_3: 1.9198 - output_loss_4: 1.7756 - output_loss_5: 1.6871 - output_loss_6: 1.6527 - output_loss_7: 1.6422 - output_loss_8: 1.6390   \n",
      "Epoch 5/12\n",
      "661403/661403 [==============================] - 218s - loss: 14.8421 - output_loss_1: 2.4105 - output_loss_2: 2.1811 - output_loss_3: 1.9157 - output_loss_4: 1.7672 - output_loss_5: 1.6755 - output_loss_6: 1.6393 - output_loss_7: 1.6283 - output_loss_8: 1.6244   \n",
      "Epoch 6/12\n",
      "661403/661403 [==============================] - 211s - loss: 14.7946 - output_loss_1: 2.4099 - output_loss_2: 2.1804 - output_loss_3: 1.9127 - output_loss_4: 1.7608 - output_loss_5: 1.6675 - output_loss_6: 1.6296 - output_loss_7: 1.6190 - output_loss_8: 1.6147   \n",
      "Epoch 7/12\n",
      "661403/661403 [==============================] - 211s - loss: 14.7578 - output_loss_1: 2.4092 - output_loss_2: 2.1791 - output_loss_3: 1.9104 - output_loss_4: 1.7559 - output_loss_5: 1.6616 - output_loss_6: 1.6232 - output_loss_7: 1.6109 - output_loss_8: 1.6074   \n",
      "Epoch 8/12\n",
      "661403/661403 [==============================] - 213s - loss: 14.7301 - output_loss_1: 2.4088 - output_loss_2: 2.1785 - output_loss_3: 1.9080 - output_loss_4: 1.7529 - output_loss_5: 1.6567 - output_loss_6: 1.6179 - output_loss_7: 1.6053 - output_loss_8: 1.6020   \n",
      "Epoch 9/12\n",
      "661403/661403 [==============================] - 215s - loss: 14.7073 - output_loss_1: 2.4085 - output_loss_2: 2.1782 - output_loss_3: 1.9069 - output_loss_4: 1.7503 - output_loss_5: 1.6533 - output_loss_6: 1.6126 - output_loss_7: 1.6011 - output_loss_8: 1.5965   \n",
      "Epoch 10/12\n",
      "661403/661403 [==============================] - 225s - loss: 14.6883 - output_loss_1: 2.4084 - output_loss_2: 2.1775 - output_loss_3: 1.9053 - output_loss_4: 1.7481 - output_loss_5: 1.6497 - output_loss_6: 1.6095 - output_loss_7: 1.5967 - output_loss_8: 1.5930   \n",
      "Epoch 11/12\n",
      "661403/661403 [==============================] - 216s - loss: 14.6720 - output_loss_1: 2.4082 - output_loss_2: 2.1774 - output_loss_3: 1.9048 - output_loss_4: 1.7460 - output_loss_5: 1.6466 - output_loss_6: 1.6063 - output_loss_7: 1.5936 - output_loss_8: 1.5892   \n",
      "Epoch 12/12\n",
      "661403/661403 [==============================] - 212s - loss: 14.6579 - output_loss_1: 2.4080 - output_loss_2: 2.1767 - output_loss_3: 1.9038 - output_loss_4: 1.7442 - output_loss_5: 1.6449 - output_loss_6: 1.6034 - output_loss_7: 1.5907 - output_loss_8: 1.5863   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49f6f81f50>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([zeros]+xs, ys, batch_size=64, nb_epoch=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nexts(m, inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arrs = [np.array(i)[np.newaxis] for i in idxs]\n",
    "    p = model.predict([np.zeros(n_fac)[np.newaxis,:]] + arrs)\n",
    "    print(list(inp))\n",
    "    return [chars[np.argmax(o)] for o in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 't', 'h', 'i', 's', ' ', 'i', 's']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ', 'h', 'e', 't', ' ', 'm', 's', ' ']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts(model, ' this is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'p', 'a', 'r', 't', ' ', 'o', 'f']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ', 'o', 'r', 'e', 'o', 'o', 'f', ' ']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts(model, ' part of')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['q', 'u', 'e', 'e', 'n', 's', ' ', 'a']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['u', 'i', 'e', 'n', ' ', ' ', 't', 'n']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts(model, 'queens a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBALS needed from this point on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(661403,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_rnn = np.stack(np.squeeze(xs), axis=1)\n",
    "y_rnn = np.atleast_3d(np.stack(ys, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((661403, 8), (661403, 8, 1))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_rnn.shape, y_rnn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence model with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert our previous keras model into a sequence model, simply add the `return_sequences=True` parameter, and add `TimeDistributed` around our dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 8, 42)         3696        embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_2 (SimpleRNN)          (None, 8, 256)        76544       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribute(None, 8, 88)         22616       simplernn_2[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 102856\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=nc),\n",
    "        SimpleRNN(n_hidden, return_sequences=True, activation='relu', inner_init='identity'),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "    ])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "661403/661403 [==============================] - 125s - loss: 1.8411   \n",
      "Epoch 2/8\n",
      "661403/661403 [==============================] - 124s - loss: 1.6598   \n",
      "Epoch 3/8\n",
      "661403/661403 [==============================] - 126s - loss: 1.6290   \n",
      "Epoch 4/8\n",
      "661403/661403 [==============================] - 124s - loss: 1.6138   \n",
      "Epoch 5/8\n",
      "661403/661403 [==============================] - 124s - loss: 1.6043   \n",
      "Epoch 6/8\n",
      "661403/661403 [==============================] - 124s - loss: 1.5976   \n",
      "Epoch 7/8\n",
      "661403/661403 [==============================] - 124s - loss: 1.5925   \n",
      "Epoch 8/8\n",
      "661403/661403 [==============================] - 123s - loss: 1.5884   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49ce7a1190>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_rnn, y_rnn, batch_size=64, nb_epoch=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "661403/661403 [==============================] - 127s - loss: 1.5852   \n",
      "Epoch 2/4\n",
      "661403/661403 [==============================] - 124s - loss: 1.5825   \n",
      "Epoch 3/4\n",
      "661403/661403 [==============================] - 124s - loss: 1.5803   \n",
      "Epoch 4/4\n",
      "661403/661403 [==============================] - 124s - loss: 1.5783   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49e5eec450>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_rnn, y_rnn, batch_size=64, nb_epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nexts_keras(m, inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arrs = np.array(idxs)[np.newaxis,:]\n",
    "    p = m.predict(arrs)[0]\n",
    "    print(list(inp))\n",
    "    return [chars[np.argmax(o)] for o in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 't', 'h', 'i', 's', ' ', 'i', 's']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ', 'h', 'e', 's', ' ', 's', 's', ' ']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts_keras(model, ' this is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'p', 'a', 'r', 't', ' ', 'o', 'f']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ', 'r', 'r', 't', ' ', 'o', 'f', ' ']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts_keras(model, ' part of')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['q', 'u', 'e', 'e', 'n', 's', ' ', 'a']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['u', 'i', 'e', 'n', ',', ' ', 'o', 'n']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts_keras(model, 'queens a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save4_path = model_path + 'save4.h5'\n",
    "if not os.path.exists(save4_path):\n",
    "    model.save_weights(save4_path)\n",
    "model.load_weights(save4_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful model with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs = 64\n",
    "nc = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_in_dat = [[idx[i+n] for i in xrange(0, len(idx)-1-nc, nc)]\n",
    "           for n in range(nc)]\n",
    "c_out_dat = [[idx[i+n] for i in xrange(1, len(idx)-nc, nc)]\n",
    "            for n in range(nc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs = [np.stack(c) for c in c_in_dat]\n",
    "xs = np.concatenate([[np.array(o)] for o in xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ys = [np.stack(c) for c in c_out_dat]\n",
    "ys = np.concatenate([[np.array(o)] for o in ys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 132280), (40, 132280))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_rnn = np.stack(np.squeeze(xs), axis=1)\n",
    "y_rnn = np.atleast_3d(np.stack(ys, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((132280, 40), (132280, 40, 1))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_rnn.shape, y_rnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_model(batch_size_override=None):\n",
    "    if batch_size_override is None:\n",
    "        batch_size_override = bs\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=nc, batch_input_shape=(batch_size_override,nc)),\n",
    "        BatchNormalization(),\n",
    "        LSTM(n_hidden, input_dim=n_fac, return_sequences=True, stateful=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        LSTM(n_hidden, input_dim=n_fac, return_sequences=True, stateful=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        TimeDistributed(Dense(n_hidden, activation='relu')),\n",
    "        Dropout(0.2),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "    ])\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_example(m, seed, gen_length=320):\n",
    "    pred_m = make_model(batch_size_override=1) # batch_size_override is the important bit\n",
    "    for layer, pred_layer in zip(m.layers, pred_m.layers):\n",
    "        pred_layer.set_weights(layer.get_weights())\n",
    "    \n",
    "    output = seed\n",
    "    for i in range(gen_length):\n",
    "        x = np.array([char_indices[c] for c in output[-nc:]])[np.newaxis,:]\n",
    "        preds = pred_m.predict(x, verbose=0, batch_size=1)[0][-1]\n",
    "        preds = preds / np.sum(preds)\n",
    "        output += np.random.choice(chars, p=preds)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epochs(m, num_epochs=12, seed='In the cathedral church of Westminster, '):\n",
    "    for i in range(num_epochs):\n",
    "        m.reset_states()\n",
    "        m.fit(x_rnn[:mx], y_rnn[:mx], batch_size=bs, nb_epoch=1, shuffle=False)\n",
    "        print_example(m, seed)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=nc, batch_input_shape=(bs,nc)),\n",
    "        BatchNormalization(),\n",
    "        LSTM(n_hidden, input_dim=n_fac, return_sequences=True, stateful=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        LSTM(n_hidden, input_dim=n_fac, return_sequences=True, stateful=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        TimeDistributed(Dense(n_hidden, activation='relu')),\n",
    "        Dropout(0.2),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_4 (Embedding)          (64, 40, 42)          3696        embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNormal(64, 40, 42)          84          embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                    (64, 40, 256)         306176      batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                    (64, 40, 256)         525312      lstm_7[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_3 (TimeDistribute(64, 40, 256)         65792       lstm_8[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (64, 40, 256)         0           timedistributed_3[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_4 (TimeDistribute(64, 40, 88)          22616       dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 923676\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using a fixed batch shape, we have to ensure our inputs and outputs are an even multiple of the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mx = len(x_rnn)//bs*bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 263s - loss: 1.8217   \n",
      "In the cathedral church of Westminster, to\n",
      "    She will then, what hath of this\n",
      "    good your back not;\n",
      "    Pelive so good nigh deed diest her is basit sh                          [Manion, parthing of with be partena sacit'd thou'll do you cordents resorn this,\n",
      "    And upon chang'd- In sicns, as bid your blood! Call hands her gorthouss!\n",
      "  PLORY. or'igne\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 263s - loss: 1.5381   \n",
      "In the cathedral church of Westminster, he that they may be r'd-didd\n",
      "    With their head all an every; sweet,\n",
      "    Me; like to have this, and dokethang the revolt uncompanyly.\n",
      "  AUTOLYCUS. Your lord who done. Hath see their passices will be harm on my devil-would,\n",
      "    The stomy charge againly's painstractions-\n",
      "  MARTIZEL. My lord, sweet time is't the 'on\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 265s - loss: 1.4750   \n",
      "In the cathedral church of Westminster, we'll cannot a husbander, what hath a blood that I know form again,\n",
      "    Up you as no heaven;\n",
      "    Which freed like untall in a well make him,\n",
      "    see in her foolish, do you not all\n",
      "    life]  Now, piper of your formict-boy,\n",
      "    Cannot call me more one depard,\n",
      "    But i' th' eye of so counsel. I have were this reli\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 265s - loss: 1.4437   \n",
      "In the cathedral church of Westminster, I must be gone else gold'd these ling with down;\n",
      "    Waid under her comes mogether;\n",
      "    Your corsion sure, she hath him that the weak, even gentleman your cruple,\n",
      "    That say see lineax to die a wind a note;\n",
      "    And we'll well obey not again. My lest\n",
      "    For her manner of state, or then with violent from his mout\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 264s - loss: 1.4256   \n",
      "In the cathedral church of Westminster, the hot will in wherein? 'Rage to the three savour?\n",
      "  LAUNCE. We'll ward him.\n",
      "                                                        Exit\n",
      "  CLOWN. Before you are the mighty\n",
      "    Could to be the world passes be cousin to do garned, and will mine issomeward;\n",
      "    These childard-wool.\n",
      "  PROTEUS. Go to, but nothing an\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 267s - loss: 1.4132   \n",
      "In the cathedral church of Westminster, thou hast so flies that I seems, when married\n",
      "    When she hath a pocking- be far it. Play my being that shadows all. So, they salt you accomplice to the Princes how bears the  sir, yourself;\n",
      "    His conf'non.\n",
      "  FORD. Come, and sufficious any digners shall I grieve their face of thy hopes\n",
      "    For the camp you is to\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 270s - loss: 1.4042   \n",
      "In the cathedral church of Westminster, and\n",
      "    the quality's sir, nothing; the fool I question, peace and in else from thy way since- Come, is she must be a-\n",
      "  THERMIONA. France in that a fredic find you.\n",
      "  LEONTES. Man, art the lady. And with a walls me and affection:\n",
      "    Be discharge him a son, for you, and won, omy eye should please him my troth, my \n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 271s - loss: 1.3973   \n",
      "In the cathedral church of Westminster, which confain him as he was bear!\n",
      "\n",
      "              Enter THer boff the Late of\n",
      "    Hath been so, for if you are\n",
      "    of a nothing behind,\n",
      "    He call their poor your company-\n",
      "  PLO. I am too. Villain, my more honour,\n",
      "     For 'tis ancounter man of whom a hollow Maria. Come, after their waithe dups from the\n",
      "    les\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 268s - loss: 1.3917   \n",
      "In the cathedral church of Westminster, thou shalt have been thought on;\n",
      "    The we chastise come of firm undertake my hands;\n",
      "    To knows no blood. I from as 'twould be thankfully,\n",
      "    Sworn de here should not free his sons;\n",
      "    John Marria childing seen\n",
      "    Will not neglect'd out too well; and for the tree. Thou canst not, for\n",
      "    The dangerous woman\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 268s - loss: 1.3869   \n",
      "In the cathedral church of Westminster, sometime 'O believe again\n",
      "    I do see her. We know now;  that\n",
      "    Unher not your has neither.\n",
      "  CLOWN. What's the passor-forth is a quaetly with all th' dead, madam, do them been pleas'd the tide, the moidter well,\n",
      "    Or now how comes and honour of it. No, madam, his grace thy fline, and so-face that I have been \n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 268s - loss: 1.3833   \n",
      "In the cathedral church of Westminster, no speak;\n",
      "    And a brast of my wishes. The offence tender to the King, if not\n",
      "    The young walls are straightness weeding comfortable born for me again, have unfind\n",
      "    me, sir.\n",
      "  CLOWN. My lord.\n",
      "  LEONTES. Fair oath of prove of the next woman for the rest,\n",
      "    If that be now the scimity for my life. But say yo\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 269s - loss: 1.3800   \n",
      "In the cathedral church of Westminster, more night.               I want\n",
      "    For the scroth misto me my ship.\n",
      "  PAULINA. How now, my lord.\n",
      "    Now death 'may out\n",
      "    Some stones me shall know his battle, I am not more\n",
      "    And drown'd of one of thee. Come no fury, sir, I will take this son,\n",
      "    Is a patience, th' airs which for so the ERES with her enou\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 265s - loss: 1.3769   \n",
      "In the cathedral church of Westminster, these healnest nettlers in him were\n",
      "    Parently acquainted, and jemel to the news are no port of anticed with the defended sight,\n",
      "    Upon the servant a full burning long thy jealousy,\n",
      "    So think one to this,\n",
      "    A gracious coudes vex me\n",
      "    And have the maid to with LEONTES. The gentleman. and so sure; I would\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 268s - loss: 1.3745   \n",
      "In the cathedral church of Westminster, here is nothing, nor but words the\n",
      "    coming, say again, and then him- and come\n",
      "    lies! We'll not do\n",
      "    ewaunds them for which so enemy's most idle words! T' will I 'twill.\n",
      "\n",
      "            Enter CLOWN, and PROTEUS. What's the Virtuo! Is they do feet the Mar. 'Twas pression of this bring\n",
      "    If you take his derin\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 269s - loss: 1.3719   \n",
      "In the cathedral church of Westminster, you shall have weary to himself. No; how now, my nursebroke, madam,\n",
      "    The seven of name bear, and what look so may bring it. Is you we should not be change my sake, foolish\n",
      "    tender too, for he luckly was in but purposes.\n",
      "  SILVIA. No; she dare dearly a father go, sir,\n",
      "    In the mighty ways follow, we to the\n",
      "\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 267s - loss: 1.3701   \n",
      "In the cathedral church of Westminster, the very troth my k Antony is ane be thought the cause\n",
      "    That he plays father did success it a tent legged yet not my jealousy,\n",
      "    And the sugger we was to my cowards, you have floods to does altaries begin is devis'd and cursed the tune.\n",
      "  PERDITA. What dost thou.\n",
      "  QUICKLY. But he would be bear'd\n",
      "    With him\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 270s - loss: 1.3681   \n",
      "In the cathedral church of Westminster, say-\n",
      "    Doth neither be seen! Will you go on his Grace. What is the trust that\n",
      "  hearing. What, will cannot be\n",
      "    made him, no, conceiv'd\n",
      "    Or present that the pardon clamours. For he goes for the\n",
      "    ere the cause even the spray't and say\n",
      "    Makes your breeding so an unluvent is\n",
      "    throne, they have d, he\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 268s - loss: 1.3666   \n",
      "In the cathedral church of Westminster, now\n",
      "    Have been done too strong'd, sleeping part\n",
      "    To lease you what is one.\n",
      "    Not many ll two time we'll wear, wept a knave; the own newly change, a new court, and a case of false-stage of this certain to this, when I can leave my thousand of this remove;\n",
      "    good heavenly indeed, darkery to you;\n",
      "    Too sa\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 269s - loss: 1.3649   \n",
      "In the cathedral church of Westminster, which often\n",
      "    battle\n",
      "    Compiadeimb'd himself, you'll think\n",
      "    from the way or need.\n",
      "  SILVIA. Land, seem him suffer\n",
      "    A long spirit,\n",
      "    Would although you'll be booting,\n",
      "    She is known death on me, boy again, and so.\n",
      "    Yet are a please; but 'tis beggar, good my lord, this boy.\n",
      "  HECTOR. The woman's\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 268s - loss: 1.3638   \n",
      "In the cathedral church of Westminster, no sense of the truly. Put away, dear n not fend one toke by my breathmend your pett'd.\n",
      "  CLOWN. Say he comes away! A sea;\n",
      "    And bohe are to me on it.\n",
      "  MAMILLO. Now comes my bond-\n",
      "    Thy increase store to you perish'd all, sir,\n",
      "    But whore be he?\n",
      "  SATURNINUS. My good courties,\n",
      "    When boy-most yeitch,\n",
      " \n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 268s - loss: 1.3624   \n",
      "In the cathedral church of Westminster, whence, no more;\n",
      "    That I must laugh upon up should take some marvel for a court.\n",
      "  CLOWN. O, believe me you look which thou hadst come\n",
      "    Not be pleas'd him unto his present clothes give me too much\n",
      "    there; I\n",
      "\n",
      "                 Enter ALL HORTENSIO\n",
      "\n",
      "  GROOMIO. Lest the beggar there-in poor battle\n",
      "\n",
      "Enter \n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 268s - loss: 1.3609   \n",
      "In the cathedral church of Westminster, and my honourable her child! Pardon, a dog.\n",
      "                                                                           Enter ARIEL\n",
      "\n",
      "                       Re-enter THERSITES within]  Bethink his by! 'My too pure not his note.\n",
      "  VALENTINE. O, neither leave that you offend\n",
      "    I know'st thou or, you are for your nos\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 266s - loss: 1.3599   \n",
      "In the cathedral church of Westminster, done some scratched; I grant,\n",
      "    The frake yourself so come by the\n",
      "    most love, and call'd so ladyship, circumstance,\n",
      "    Because will never purpose my tent-day?\n",
      "    'Tis my lovely, for the ceremony\n",
      "    As hand may give to lane that I have ever odd\n",
      "    Which twice she raination. Let's be greatness. It saw her \n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 268s - loss: 1.3586   \n",
      "In the cathedral church of Westminster, does to take you hither my wit from me.\n",
      "  CLOWN. Of thee too it of your father, thou didst not be abur'd\n",
      "    Poor a tresp of their counsels be\n",
      "    he had made so.\n",
      "  PAGE. Peace, he dost are in th' Valentine from me. God's bed\n",
      "    him my golden- was much to violent\n",
      "    Which the uncle thing the straight easy.\n",
      "  J\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 268s - loss: 1.3578   \n",
      "In the cathedral church of Westminster, too.\n",
      "    If the shaft of Cordimond him away.\n",
      "  JULIA. In the niece of their grave\n",
      "    Doth moon sir to slow which I am\n",
      "    against an of the two daughter?\n",
      "  MALVOLIO. The back'st be not as proves withthee\n",
      "    Be a der keep other.\n",
      "    Will you made you a fashion with Poins on thee! Like left with them.\n",
      "    But\n",
      "\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "run_epochs(model, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save5_path = model_path + 'save5.h5'\n",
    "if not os.path.exists(save5_path):\n",
    "    model.save_weights(save5_path)\n",
    "model.load_weights(save5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 270s - loss: 1.3565   \n",
      "In the cathedral church of Westminster, I come\n",
      "    So so, the visit and deceive your\n",
      "    drop of your friend\n",
      "    It three month we see the forms will stoppell them approaching mouth,\n",
      "    you should call the ways she been persuaded bosom.'\n",
      "  ULYSSES. O that, Hector.\n",
      "    Never heard you mine.\n",
      "    Die for nurse,\n",
      "    His fortune and hear upon men blood;\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 268s - loss: 1.3558   \n",
      "In the cathedral church of Westminster, content thereof you have unjest,\n",
      "    Together now is sad\n",
      "    If your eyes were your care was not plac'd\n",
      "    Means me, he'll be sent come,\n",
      "    That Goth my bloody peril of him.\n",
      "    Walk: let 'gno.\n",
      "  DEMETRIUS. 'Tis by our offices.\n",
      "  THURIO. Even the estate lives that. O,\n",
      "    Let's presently be pass'd me; but I c\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 267s - loss: 1.3552   \n",
      "In the cathedral church of Westminster, or by a fortune of these book;\n",
      "    To do it holouring fits of what I do wear\n",
      "    So, sir, what banish'd to swear with common officers, and\n",
      "    make impossible. I be come in her stone, praise you one still unwoman and sobeits her amongst my certain name; yet may, arise\n",
      "    The never way compast upon my formish betwe\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 271s - loss: 1.3539   \n",
      "In the cathedral church of Westminster, and are sometime are supperll both; for\n",
      "    with thy baro not\n",
      "    too love it. Nay, first, and base corpulsh be a sum\n",
      "    To make my heart upon all the thought.\n",
      "  CLOWN. No, Silvia. I will not believe I heard\n",
      "    him her, thus, no more and presentless shadow!\n",
      "    Now should not the man in leave that affliction\n",
      " \n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 268s - loss: 1.3532   \n",
      "In the cathedral church of Westminster, I'll be\n",
      "    o'er therefore can be a matter. Pray you, but they say\n",
      "    Of death's circumstances, the babe and such expedition.\n",
      "\n",
      "                  Enter SERVANT\n",
      "  COSTELLES. Which is the curst of myself after.                                [Standing to my time,\n",
      "    And with thyself-mouth!\n",
      "  BAPTISTA. No, I'll be\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 271s - loss: 1.3527   \n",
      "In the cathedral church of Westminster, with her, favours\n",
      "\n",
      "              Enter LORDS\n",
      "    Congeal'd on him, bound of some spinly to I do well.\n",
      "  CRESSIDA. How being they have moreorself it not quickly many treacherous; you speak better. We'll supply I'll dischance the dark.\n",
      "  PHEBE. How now!\"\n",
      "  NESTOR. O love? Horatio-draw. Forgot\n",
      "    her fortune me to\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 271s - loss: 1.3519   \n",
      "In the cathedral church of Westminster, thing upon his death,\n",
      "    For wilson are no tear up,\n",
      "    None in your grandsire none conscience; I'll not see ere the matters.\n",
      "  CRESSIDA. Ha!\n",
      "  PRINCE. You a thousand hope?\n",
      "  SEBASTIAN. Yet there's gone; I will not could say think\n",
      "    Perceive open your husband comes with a very buried\n",
      "    bamm is not so, with \n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 269s - loss: 1.3508   \n",
      "In the cathedral church of Westminster, or 'tis another,\n",
      "    Or a  Bolingbroke of\n",
      "    the draws o' th' placeless. We'll be men follow. I will have thumours knock'd, boy, that I prove thee? What would I dear. Jight I am gone,\n",
      "    And might be affordy,\n",
      "    sorrow commanded man.\n",
      "  DUKE. Unless thou must unhappy.\n",
      "\n",
      "                               Enter DUKE\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 271s - loss: 1.3504   \n",
      "In the cathedral church of Westminster, how the worst will not move unfoed infancy, rest was\n",
      "    And in ten truth of war\n",
      "    Which rapes, misery- it wails may break success\n",
      "    To see the after, whom h' sacred bounding; spall happy edifaction, if I be commanded\n",
      "    low in his use of the induge by this: I am discourse.\n",
      "  SECOND CITIZEN. I do the creature\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 271s - loss: 1.3497   \n",
      "In the cathedral church of Westminster, the crown, choose woman- and let a\n",
      "    true palaces that our money of this due the gentleman to th' temptain greetings in brecome\n",
      "    And horsere me ating my foresters\n",
      "    He is his stock\n",
      "    Till the please the best beggar of friendship apass thine\n",
      "    Shall come to relieve made him and does, the Emperor\n",
      "    Ray\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 266s - loss: 1.3489   \n",
      "In the cathedral church of Westminster, 'tis to from thee.\n",
      "  COP. I know him twenty nurses; which music grows them that his and night.           Exit SILVIA and BAPTISTA\n",
      "      What's the welcome to my lord, and I may be dimiff; and it should say\n",
      "    With perfection fights die, why for't, day.\n",
      "    You will in-company fortune to the hard bolt of I hope it \n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 269s - loss: 1.3486   \n",
      "In the cathedral church of Westminster, to much fly a safely falsehood;\n",
      "    Which I may entertain his coooks as the purses\n",
      "    The man is no doubtful stocks, therefore she was so chang'd a son- so down his\n",
      "    Might sure of hands, and owshould\n",
      "    keep them llaw, must not know\n",
      "    his answer, she cannot stay! Is it at hang'd incannot\n",
      "    not have she h\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 264s - loss: 1.3474   \n",
      "In the cathedral church of Westminster, or he's not-proper of 'is ll like inventures: the sea; presume. Stay so welcome.             Re-enter\n",
      "    As once lows this is.\n",
      "  SHALLOW. Why, now for the noble sons of particular time. Most woe me without the blessed oppose.     f on his face, and\n",
      "    Take her so he concident   Shall I have been kiss'd to conjure \n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 268s - loss: 1.3470   \n",
      "In the cathedral church of Westminster, face,\n",
      "    So she writes have the hazard\n",
      "    To told but I can find so; hath empty loves, that, as makes\n",
      "    Fire this thing took me? 'Twas but honourable degree,\n",
      "    Which you shall look ine, Turk and best feel'd to behold\n",
      "    A worthy bonded for\n",
      "    Am be a mark of vein she's aparted the while.\n",
      "  LEONTES. Then \n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 269s - loss: 1.3466   \n",
      "In the cathedral church of Westminster, be but in\n",
      "    are.  [To BAPTISTA\n",
      "        Exit YOUNG, and SELENTINE\n",
      "    Why, sir, wherefore do buy\n",
      "    what unreport, and the which will be shown,\n",
      "    But even here's bold. Think you wear\n",
      "    to do,\n",
      "    But she bears some birth, sir, I say.\n",
      "  PERDITA. Sir, this biss.\n",
      "  OLIVIA. 'Tis proof\n",
      "    As heaven-song tha\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 269s - loss: 1.3461   \n",
      "In the cathedral church of Westminster, sir, to give a dangerous sight. On friends must again]\n",
      "    There's sarnish'd of my ship, hence are sick an assemble-different? Let not confin'd.\n",
      "  POLIXENES. But this way ready,\n",
      "    And serve his lips more sense, and Lancaster, Valentine. Excuse the earth o'er, master.\n",
      "  SERVANT. Thou that thou dost unto my father,\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 269s - loss: 1.3459   \n",
      "In the cathedral church of Westminster, I unto the water.\n",
      "  PROTEUS. You should shalt be weary the son greeting, and kiss'd to these music cannot, the smiles with one,\n",
      "    But that cannot give at his own tax nothing-\n",
      "  CASSIO. Where load'st thou now yet, Pinty words so thanks that 'ur bid thee\n",
      "    no more friendship shall die in consideration\n",
      "    They w\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 273s - loss: 1.3449   \n",
      "In the cathedral church of Westminster, my disown, their desires that some mistress the\n",
      "    love against that shapes fortyly\n",
      "    Whereof I shall deliver a counterfplia, come. Say 'twixt you, sir.\n",
      "  NESTOR. he hath known not her lost man.\n",
      "    Above all warm of our heads, that much\n",
      "    well a band; th' allay your niece us lessll.\n",
      " titus.\n",
      "  PAULINA. Pray\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 265s - loss: 1.3445   \n",
      "In the cathedral church of Westminster, is L At promise.\n",
      "  BIONDELLO. The pride of his way,\n",
      "    Who he cannot make given his lord. Before you have lord himself\n",
      "    And would not speak it with me, like a like friends from our custom, and no more for By hove,\n",
      "    You suspect you not you so thine\n",
      "    Is done afford, hold upon your fair sharp sweet heavens-\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 270s - loss: 1.3442   \n",
      "In the cathedral church of Westminster, 'I leave,\n",
      "    And now he is well too dips above your slaves; but hast a new head!\n",
      "  VIOLA. To use him, or my lord. A service,\n",
      "    But you shall gaze, and the coaused of some certainteen. Unkindness of it to be a man,\n",
      "    The door\n",
      "    I'll have sorry to go call. Give't for our love\n",
      "    In shames consider it fly; p\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 269s - loss: 1.3437   \n",
      "In the cathedral church of Westminster, and a man in mine own leave.\n",
      "  LEONTES.  [Aside]  Please me of the right you have\n",
      "    so\n",
      "    Whose shepherd.\n",
      "    Are ntill I say a groan'd to him.        Had thickes much blushing at his spirit that not  for as\n",
      "    do be about the wit, though a merry fair wooing fire;\n",
      "    The patrominate way our gentlewoman in th\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 271s - loss: 1.3431   \n",
      "In the cathedral church of Westminster, yet, besides,\n",
      "    Shall be\n",
      "    your pattering that thin, and bestow what this the heapest\n",
      "    'Ll\n",
      "    Reasonable thoughts of his wisdom.\n",
      "  ARIEL. Came your traitor, the tunes are in countryman-- stays in their sweet wise\n",
      "    And proudy of purpose is but employ'd up,\n",
      "    Most grossly lacks into a sister. Let me e\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 268s - loss: 1.3427   \n",
      "In the cathedral church of Westminster, whose hand of his tongue.                              Exit\n",
      "  EMILIA. It is chaste\n",
      "    to know her wit. I would bear forget me. He\n",
      "    off\n",
      "    These glorious loves. I am thus than this thousand law of heavy.\n",
      "  THURIO.  [Aside]  Say, thou dost be more as left you true,\n",
      "    Which word and her noises bear\n",
      "    To th\n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 268s - loss: 1.3422   \n",
      "In the cathedral church of Westminster, that cause us motion, misulter, o more starving in the master needs be the King.\n",
      "  PAULINA. O foolish stars fetch'd for rof reheart.\n",
      "  PANTHINO. My bosom, my vor   A\n",
      "\n",
      "  LAUNCE. Nay, my lord, sir, would you perform it possible to make her love\n",
      "    Some fair edicate as lies on the common'd idge-man?\n",
      " Exit\n",
      "    And \n",
      "()\n",
      "Epoch 1/1\n",
      "132224/132224 [==============================] - 271s - loss: 1.3418   \n",
      "In the cathedral church of Westminster, which from my money.\n",
      "    And we see him raven by the peace, to say who were have a praise; but we mean from my\n",
      "    there will be harm, if you cold soldiers' deceit,\n",
      "                  The Volsces him, yet to the uppoke pardon it along, and her\n",
      "    every self the walk. He\n",
      "    Save them things, sir! when they was not\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "run_epochs(model, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save5_2_path = model_path + 'save5_2.h5'\n",
    "if not os.path.exists(save5_2_path):\n",
    "    model.save_weights(save5_2_path)\n",
    "model.load_weights(save5_2_path)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
