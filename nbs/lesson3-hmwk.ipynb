{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Farm Distracted Driver Detection: Take 3\n",
    "_Can computer vision spot distracted drivers?_\n",
    "\n",
    "---\n",
    "\n",
    "## Lesson 3 Homework Assignment\n",
    "\n",
    "Dataset: https://www.kaggle.com/c/state-farm-distracted-driver-detection\n",
    "\n",
    "### Dealing with Overfitting\n",
    "In [`lesson2-hmwk.ipynb`](https://github.com/iconix/fast.ai/blob/master/nbs/lesson2-hmwk.ipynb), my final results (after 5 epochs) were as follows:\n",
    "> `loss: 0.6260 - acc: 0.7907 - val_loss: 1.6719 - val_acc: 0.4978`\n",
    "\n",
    "When `val_acc >> acc`, this is a clear indicator of overfitting on the training data.\n",
    "\n",
    "On the bright side, this means that my neural net architecture is complex enough to model the data. The next step is to generalize my architecture a bit more.\n",
    "\n",
    "Here is the prioritized list of approaches to reducing overfitting provided during class:\n",
    "1. Add more data\n",
    "2. Use data augmentation\n",
    "3. Use architectures that generalize well\n",
    "4. Add regularization (dropout, L2/L1 regularization)\n",
    "5. Reduce architecture complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and creating the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, I am skipping the download, split, and create of training, validation, test, and sample datasets, relying instead on the data split from last week. See [`lesson2-hmwk.ipynb`](https://github.com/iconix/fast.ai/blob/master/nbs/lesson2-hmwk.ipynb) for those steps, if needed.\n",
    "\n",
    "Additionally, I will be starting with the weights from last week below (`finetune2.h5`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "LESSON_HOME_DIR = current_dir\n",
    "DATA_HOME_DIR = current_dir + '/data/statefarm/'\n",
    "\n",
    "# point to your training images\n",
    "train_dir = DATA_HOME_DIR + 'train'\n",
    "\n",
    "# point to the 'driver_imgs_list.csv'\n",
    "lookup = DATA_HOME_DIR + 'driver_imgs_list.csv'\n",
    "\n",
    "# point to the validation directory, which will be created in the next block\n",
    "val_dir = DATA_HOME_DIR + 'valid'\n",
    "\n",
    "sample_dir = DATA_HOME_DIR + 'sample'\n",
    "\n",
    "test_dir = DATA_HOME_DIR + 'test'\n",
    "\n",
    "#path = DATA_HOME_DIR + 'sample/'\n",
    "path = DATA_HOME_DIR\n",
    "model_path = path + 'models/'\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import utils; reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = vgg_ft(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finetune2_path = model_path+'finetune2.h5'\n",
    "\n",
    "model.load_weights(finetune2_path);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_2 (Lambda)                (None, 3, 224, 224)   0           lambda_input_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_14 (ZeroPadding2D) (None, 3, 226, 226)   0           lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_14 (Convolution2D) (None, 64, 224, 224)  0           zeropadding2d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_15 (ZeroPadding2D) (None, 64, 226, 226)  0           convolution2d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_15 (Convolution2D) (None, 64, 224, 224)  0           zeropadding2d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_6 (MaxPooling2D)    (None, 64, 112, 112)  0           convolution2d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_16 (ZeroPadding2D) (None, 64, 114, 114)  0           maxpooling2d_6[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_16 (Convolution2D) (None, 128, 112, 112) 0           zeropadding2d_16[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_17 (ZeroPadding2D) (None, 128, 114, 114) 0           convolution2d_16[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_17 (Convolution2D) (None, 128, 112, 112) 0           zeropadding2d_17[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_7 (MaxPooling2D)    (None, 128, 56, 56)   0           convolution2d_17[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_18 (ZeroPadding2D) (None, 128, 58, 58)   0           maxpooling2d_7[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_18 (Convolution2D) (None, 256, 56, 56)   0           zeropadding2d_18[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_19 (ZeroPadding2D) (None, 256, 58, 58)   0           convolution2d_18[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_19 (Convolution2D) (None, 256, 56, 56)   0           zeropadding2d_19[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_20 (ZeroPadding2D) (None, 256, 58, 58)   0           convolution2d_19[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_20 (Convolution2D) (None, 256, 56, 56)   0           zeropadding2d_20[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_8 (MaxPooling2D)    (None, 256, 28, 28)   0           convolution2d_20[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_21 (ZeroPadding2D) (None, 256, 30, 30)   0           maxpooling2d_8[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_21 (Convolution2D) (None, 512, 28, 28)   0           zeropadding2d_21[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_22 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_21[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_22 (Convolution2D) (None, 512, 28, 28)   0           zeropadding2d_22[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_23 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_22[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_23 (Convolution2D) (None, 512, 28, 28)   0           zeropadding2d_23[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_9 (MaxPooling2D)    (None, 512, 14, 14)   0           convolution2d_23[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_24 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_9[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_24 (Convolution2D) (None, 512, 14, 14)   0           zeropadding2d_24[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_25 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_24[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_25 (Convolution2D) (None, 512, 14, 14)   0           zeropadding2d_25[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_26 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_25[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_26 (Convolution2D) (None, 512, 14, 14)   0           zeropadding2d_26[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_10 (MaxPooling2D)   (None, 512, 7, 7)     0           convolution2d_26[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 25088)         0           maxpooling2d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 4096)          0           flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 4096)          0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 4096)          0           dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 4096)          0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 10)            40970       dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 40970\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "# Dense layers - also known as fully connected or 'FC' layers\n",
    "fc_layers = layers[last_conv_idx+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a generator that includes data augmentation (convenient feature of Keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator(rotation_range=10, width_shift_range=0.1, \n",
    "       height_shift_range=0.1, shear_range=0.15, zoom_range=0.1, \n",
    "       channel_shift_range=10., horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18298 images belonging to 10 classes.\n",
      "Found 4126 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "trn_batches = get_batches(path+'train', gen, batch_size=batch_size)\n",
    "# NB: We don't want to augment or shuffle the validation set\n",
    "val_batches = get_batches(path+'valid', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeremy's [explanation](http://forums.fast.ai/t/lesson-3-discussion/186/33) as to why we aren't training the convolutional layers here:\n",
    "> The early layers are so general (e.g. remember Zeiler's visualizations - layer 1 just finds edges and gradients) that it's extremely unlikely that you'll need to change them, unless you're looking at very different kinds of images. e.g. if you're classifying line drawings, instead of photos, you'll probably need to retrain many conv layers too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for layer in layers[:last_conv_idx+1]: layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Updating slowly because it is finely tuned\n",
    "K.set_value(model.optimizer.lr, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "18298/18298 [==============================] - 590s - loss: 2.3606 - acc: 0.4514 - val_loss: 1.6972 - val_acc: 0.5264\n",
      "Epoch 2/8\n",
      "18298/18298 [==============================] - 590s - loss: 2.2040 - acc: 0.4697 - val_loss: 1.7198 - val_acc: 0.5201\n",
      "Epoch 3/8\n",
      "18298/18298 [==============================] - 590s - loss: 2.1467 - acc: 0.4716 - val_loss: 1.7177 - val_acc: 0.5208\n",
      "Epoch 4/8\n",
      "18298/18298 [==============================] - 591s - loss: 2.1106 - acc: 0.4792 - val_loss: 1.7157 - val_acc: 0.5223\n",
      "Epoch 5/8\n",
      "18298/18298 [==============================] - 591s - loss: 2.0800 - acc: 0.4773 - val_loss: 1.7235 - val_acc: 0.5182\n",
      "Epoch 6/8\n",
      "18298/18298 [==============================] - 590s - loss: 2.0187 - acc: 0.4837 - val_loss: 1.7172 - val_acc: 0.5182\n",
      "Epoch 7/8\n",
      "18298/18298 [==============================] - 590s - loss: 1.9916 - acc: 0.4862 - val_loss: 1.7573 - val_acc: 0.5107\n",
      "Epoch 8/8\n",
      "18298/18298 [==============================] - 590s - loss: 1.9278 - acc: 0.5002 - val_loss: 1.7463 - val_acc: 0.5099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faa1047f6d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(trn_batches, samples_per_epoch=trn_batches.nb_sample, nb_epoch=8, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 8 epochs, we are overfitting much much less, which is great!\n",
    "> `loss: 1.9278 - acc: 0.5002 - val_loss: 1.7463 - val_acc: 0.5099`\n",
    "\n",
    "In fact, now we're underfitting very slightly. This seems like a good time to try decreasing dropout a smidge, just to see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our intermediate weights first, just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finetune_hw3_1_path = model_path+'finetune_hw3_1.h5'\n",
    "if not os.path.exists(finetune_hw3_1_path):\n",
    "    model.save_weights(finetune_hw3_1_path)\n",
    "model.load_weights(finetune_hw3_1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intermediate Kaggle submission\n",
    "\n",
    "Interestingly, despite the significant decrease in overfitting, `val_acc` only improved a small amount from `0.4978`... I'd like to try submitting these results to Kaggle, just to see how this result reflects in the rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4126 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "subm_name = 'subm_hmwk3_1.gz'\n",
    "subm_path = path + 'results/' + subm_name\n",
    "\n",
    "#Get the classes from the validation batch\n",
    "val_preprocess = get_batches(path+'valid', shuffle=False, batch_size=1)\n",
    "classes = sorted(val_preprocess.class_indices, key=val_preprocess.class_indices.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data = get_data(path+'test')\n",
    "preds = model.predict(test_data, batch_size = batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (number of classes - 1) = 9 (http://forums.fast.ai/t/moving-up-the-ncfm-leaderboard-by-100-positions-do-clip/1035/6)\n",
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/9, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = do_clip(preds, 0.51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'img', [a[8:] for a in test_filenames])\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(subm_name, index=False, compression='gzip')\n",
    "FileLink(subm_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase Dropout\n",
    "We're going to experiment with increasing the dropout rate from Vgg16's 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)\n",
    "fc_model = Sequential(fc_layers)\n",
    "\n",
    "def get_fc_model():\n",
    "    model = Sequential([\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.),\n",
    "        Dense(2, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    for l1,l2 in zip(model.layers, fc_layers): l1.set_weights(proc_wgts(l2))\n",
    "\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
