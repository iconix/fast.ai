{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Farm Distracted Driver Detection: Take 3\n",
    "_Can computer vision spot distracted drivers?_\n",
    "\n",
    "---\n",
    "\n",
    "## Lesson 3 Homework Assignment\n",
    "\n",
    "Dataset: https://www.kaggle.com/c/state-farm-distracted-driver-detection\n",
    "\n",
    "### Dealing with Overfitting\n",
    "In [`lesson2-hmwk.ipynb`](https://github.com/iconix/fast.ai/blob/master/nbs/lesson2-hmwk.ipynb), my final results (after 5 epochs) were as follows:\n",
    "> `loss: 0.6260 - acc: 0.7907 - val_loss: 1.6719 - val_acc: 0.4978`\n",
    "\n",
    "When `val_acc >> acc`, this is a clear indicator of overfitting on the training data.\n",
    "\n",
    "On the bright side, this means that my neural net architecture is complex enough to model the data. The next step is to generalize my architecture a bit more.\n",
    "\n",
    "Here is the prioritized list of approaches to reducing overfitting provided during class:\n",
    "1. Add more data\n",
    "2. Use data augmentation\n",
    "3. Use architectures that generalize well\n",
    "4. Add regularization (dropout, L2/L1 regularization)\n",
    "5. Reduce architecture complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and creating the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, I am skipping the download, split, and create of training, validation, test, and sample datasets, relying instead on the data split from last week. See [`lesson2-hmwk.ipynb`](https://github.com/iconix/fast.ai/blob/master/nbs/lesson2-hmwk.ipynb) for those steps, if needed.\n",
    "\n",
    "Additionally, I will be starting with the weights from last week below (`finetune2.h5`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "LESSON_HOME_DIR = current_dir\n",
    "DATA_HOME_DIR = current_dir + '/data/statefarm/'\n",
    "\n",
    "# point to your training images\n",
    "train_dir = DATA_HOME_DIR + 'train'\n",
    "\n",
    "# point to the 'driver_imgs_list.csv'\n",
    "lookup = DATA_HOME_DIR + 'driver_imgs_list.csv'\n",
    "\n",
    "# point to the validation directory, which will be created in the next block\n",
    "val_dir = DATA_HOME_DIR + 'valid'\n",
    "\n",
    "sample_dir = DATA_HOME_DIR + 'sample'\n",
    "\n",
    "test_dir = DATA_HOME_DIR + 'test'\n",
    "\n",
    "#path = DATA_HOME_DIR + 'sample/'\n",
    "path = DATA_HOME_DIR\n",
    "model_path = path + 'models/'\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import utils; reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_week2_model():\n",
    "    model = vgg_ft(10)\n",
    "    finetune2_path = model_path+'finetune2.h5'\n",
    "    model.load_weights(finetune2_path);\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = get_week2_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                    if type(layer) is Convolution2D][-1]\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "# Dense layers - also known as fully connected or 'FC' layers\n",
    "fc_layers = layers[last_conv_idx+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a generator that includes data augmentation (convenient feature of Keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator(rotation_range=10, width_shift_range=0.1, \n",
    "       height_shift_range=0.1, shear_range=0.15, zoom_range=0.1, \n",
    "       channel_shift_range=10., horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18298 images belonging to 10 classes.\n",
      "Found 4126 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "trn_batches = get_batches(path+'train', gen, batch_size=batch_size)\n",
    "# NB: We don't want to augment or shuffle the validation set\n",
    "val_batches = get_batches(path+'valid', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeremy's [explanation](http://forums.fast.ai/t/lesson-3-discussion/186/33) as to why we aren't training the convolutional layers here:\n",
    "> The early layers are so general (e.g. remember Zeiler's visualizations - layer 1 just finds edges and gradients) that it's extremely unlikely that you'll need to change them, unless you're looking at very different kinds of images. e.g. if you're classifying line drawings, instead of photos, you'll probably need to retrain many conv layers too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for layer in layers[:last_conv_idx+1]: layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Updating slowly because it is finely tuned\n",
    "K.set_value(model.optimizer.lr, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit_generator(trn_batches, samples_per_epoch=trn_batches.nb_sample, nb_epoch=8, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 8 epochs, we are overfitting much much less, which is great!\n",
    "> `loss: 1.9278 - acc: 0.5002 - val_loss: 1.7463 - val_acc: 0.5099`\n",
    "\n",
    "In fact, now we're underfitting very slightly. This seems like a good time to try decreasing dropout a smidge, just to see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our intermediate weights first, just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finetune_hw3_1_path = model_path+'finetune_hw3_1.h5'\n",
    "if not os.path.exists(finetune_hw3_1_path):\n",
    "    model.save_weights(finetune_hw3_1_path)\n",
    "model.load_weights(finetune_hw3_1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intermediate Kaggle submission\n",
    "\n",
    "Interestingly, despite the significant decrease in overfitting, `val_acc` only improved a small amount from `0.4978`... I'd like to try submitting these results to Kaggle, just to see how this result reflects in the rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "subm_name = 'subm_hmwk3_1.gz'\n",
    "subm_path = path + 'results/' + subm_name\n",
    "\n",
    "#Get the classes from the validation batch\n",
    "val_preprocess = get_batches(path+'valid', shuffle=False, batch_size=1)\n",
    "classes = sorted(val_preprocess.class_indices, key=val_preprocess.class_indices.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on `.predict` vs `.predict_generator`: \"the precomputed data will be large. Then it is likely that you will encounter errors such as OOM or **kernel death** during training. In this case, you might want to use model.fit_generator() instead of model.fit()\" ([source](http://forums.fast.ai/t/fine-tuning-vgg-taking-very-long/3825/11)) - I kept running into kernel death with `.predict`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_batches = get_batches(path+'test', shuffle=False, batch_size = batch_size)\n",
    "preds = model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (number of classes - 1) = 9 (http://forums.fast.ai/t/moving-up-the-ncfm-leaderboard-by-100-positions-do-clip/1035/6)\n",
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/9, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = do_clip(preds, 0.9) # still unsure how to set mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'img', [a[8:] for a in test_batches.filenames])\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission.to_csv(subm_path, index=False, compression='gzip')\n",
    "FileLink(subm_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Private Score: `1.41527` ... comparable to my week 2 results.\n",
    "\n",
    "And interestingly, weeks ago, my Public Score tended to be the lower/better score, or at least comparable; now, it is `1.48239` in comparison. See [this Quora post](https://www.quora.com/What-is-the-difference-between-public-and-private-leaderboard-in-Kaggle/answer/Giuliano-Janson) for an explanation:\n",
    "> The public LB is computed on a portion of the test set, the private is computed on the remainder of the test set (not the whole test set).\n",
    "\"Fitting the LB\" is a Kaggle term used to describe when you're tuning your models to perform well on the public LB. There is an art and a science in doing so and experience Kagglers are able to make the most out of it without overfitting. If not done well, that usually lends itself to worse scores on the private LB, sometimes disasters. In general the key is to build a model that generalizes well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation + Decrease Dropout\n",
    "We're going to experiment with decreasing the dropout rate from Vgg16's 50% to 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# will start again with conv_layers from last week's model\n",
    "model = get_week2_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_fc_model(fcl, conv_input_shape):\n",
    "    m = Sequential([\n",
    "        MaxPooling2D(input_shape=conv_input_shape),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    for l1,l2 in zip(m.layers, fcl): l1.set_weights(l2.get_weights())\n",
    "\n",
    "    m.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                    if type(layer) is Convolution2D][-1]\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "fc_layers = layers[last_conv_idx+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)\n",
    "fc_model = get_fc_model(fc_layers, conv_layers[-1].output_shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the new `fc_model` with less dropout to the original `conv_model` from last week..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in conv_model.layers: layer.trainable=False\n",
    "conv_model.add(fc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then compile the new combined model and fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model.compile(optimizer=Adam(0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "18298/18298 [==============================] - 555s - loss: 0.6000 - acc: 0.8050 - val_loss: 1.7193 - val_acc: 0.5325\n",
      "Epoch 2/8\n",
      "18298/18298 [==============================] - 553s - loss: 0.2425 - acc: 0.9205 - val_loss: 1.5269 - val_acc: 0.6008\n",
      "Epoch 3/8\n",
      "18298/18298 [==============================] - 553s - loss: 0.1587 - acc: 0.9478 - val_loss: 1.4940 - val_acc: 0.6146\n",
      "Epoch 4/8\n",
      "18298/18298 [==============================] - 553s - loss: 0.1221 - acc: 0.9608 - val_loss: 1.5573 - val_acc: 0.6105\n",
      "Epoch 5/8\n",
      "18298/18298 [==============================] - 552s - loss: 0.1010 - acc: 0.9681 - val_loss: 1.5101 - val_acc: 0.6408\n",
      "Epoch 6/8\n",
      "18298/18298 [==============================] - 553s - loss: 0.0878 - acc: 0.9719 - val_loss: 1.4931 - val_acc: 0.6275\n",
      "Epoch 7/8\n",
      "18298/18298 [==============================] - 553s - loss: 0.0644 - acc: 0.9802 - val_loss: 1.4614 - val_acc: 0.6534\n",
      "Epoch 8/8\n",
      "18298/18298 [==============================] - 554s - loss: 0.0625 - acc: 0.9781 - val_loss: 1.5901 - val_acc: 0.6241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f947ea4e650>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.fit_generator(trn_batches, samples_per_epoch=trn_batches.nb_sample, nb_epoch=8, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "18298/18298 [==============================] - 553s - loss: 0.0536 - acc: 0.9825 - val_loss: 1.6007 - val_acc: 0.6299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f947260f710>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.optimizer.lr = 0.001 # back to default learning rate\n",
    "conv_model.fit_generator(trn_batches, samples_per_epoch=trn_batches.nb_sample, nb_epoch=1, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drumroll...\n",
    "\n",
    "    `loss: 0.0554 - acc: 0.9819 - val_loss: 1.3206 - val_acc: 0.7038`\n",
    "\n",
    "Wow, back to massive overfitting. But I was expecting this since I reduced dropout in a model that was already pretty well balanced in fit thanks to data augmentation in the last section.\n",
    "\n",
    "Fortunately as I was also hoping, there has been a _huge_ increase in the upper bounds of both `acc` and `val_acc` - dropout was stunting the model's learning ability! So I'm hoping this means that my next final attempt at reducing overfitting will work again, but with much higher accuracies than the `~0.5` I was seeing earlier.\n",
    "\n",
    "Let's save weights again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finetune_hw3_2_path = model_path+'finetune_hw3_2.h5'\n",
    "if not os.path.exists(finetune_hw3_2_path):\n",
    "    conv_model.save_weights(finetune_hw3_2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start with the model just trained above\n",
    "model = conv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4126/4126 [==============================] - 98s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6006800007369411, 0.62990790118711182]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that we have the correct model loaded\n",
    "val_data = load_array(model_path+'valid_data.bc')\n",
    "model.evaluate(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(1000, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_fc_weights_from_vgg16bn(m):\n",
    "    \"Load weights for model from the dense layers of the Vgg16BN model.\"\n",
    "    # See imagenet_batchnorm.ipynb for info on how the weights for\n",
    "    # Vgg16BN can be generated from the standard Vgg16 weights.\n",
    "    from vgg16bn import Vgg16BN\n",
    "    vgg16_bn = Vgg16BN()\n",
    "    _, fcl = split_at(vgg16_bn.model, Convolution2D)\n",
    "    copy_weights(fcl, m.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_fc_model = Sequential(get_bn_layers(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_fc_weights_from_vgg16bn(bn_fc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_fc_model.pop()\n",
    "for layer in bn_fc_model.layers: layer.trainable=False\n",
    "bn_fc_model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_fc_model.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_fc_layers = get_bn_layers(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bn_fc_layers.pop()\n",
    "bn_fc_layers.append(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_model = Sequential(conv_layers)\n",
    "for layer in final_model.layers: layer.trainable = False\n",
    "for layer in bn_fc_layers: final_model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for l1,l2 in zip(bn_fc_model.layers, bn_fc_layers):\n",
    "    l2.set_weights(l1.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model.compile(optimizer=Adam(0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "18298/18298 [==============================] - 557s - loss: 7.9419 - acc: 0.3832 - val_loss: 9.5732 - val_acc: 0.3303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9434a28390>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit_generator(trn_batches, samples_per_epoch=trn_batches.nb_sample, nb_epoch=1, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "18298/18298 [==============================] - 557s - loss: 5.3783 - acc: 0.5222 - val_loss: 7.3743 - val_acc: 0.4152\n",
      "Epoch 2/3\n",
      "18298/18298 [==============================] - 556s - loss: 3.3244 - acc: 0.6433 - val_loss: 5.1134 - val_acc: 0.5325\n",
      "Epoch 3/3\n",
      "18298/18298 [==============================] - 556s - loss: 2.1664 - acc: 0.7283 - val_loss: 4.2172 - val_acc: 0.5926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9452f5d150>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit_generator(trn_batches, samples_per_epoch=trn_batches.nb_sample, nb_epoch=3, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "18298/18298 [==============================] - 557s - loss: 1.4044 - acc: 0.7876 - val_loss: 3.0287 - val_acc: 0.6544\n",
      "Epoch 2/2\n",
      "18298/18298 [==============================] - 556s - loss: 0.9522 - acc: 0.8315 - val_loss: 2.8282 - val_acc: 0.6597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f94343e9cd0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit_generator(trn_batches, samples_per_epoch=trn_batches.nb_sample, nb_epoch=2, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "18298/18298 [==============================] - 557s - loss: 0.7423 - acc: 0.8605 - val_loss: 2.4889 - val_acc: 0.6801\n",
      "Epoch 2/2\n",
      "18298/18298 [==============================] - 556s - loss: 0.5927 - acc: 0.8781 - val_loss: 2.3082 - val_acc: 0.7087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f94343e9bd0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.optimizer.lr = 0.001\n",
    "final_model.fit_generator(trn_batches, samples_per_epoch=trn_batches.nb_sample, nb_epoch=2, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "18298/18298 [==============================] - 557s - loss: 0.4997 - acc: 0.8909 - val_loss: 2.2943 - val_acc: 0.6837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f94343e9ad0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit_generator(trn_batches, samples_per_epoch=trn_batches.nb_sample, nb_epoch=1, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was feeling disappointed after Epoch 1 - it looked like the accuracy upper bound had regressed from the awesome results after reducing dropout in the last section.\n",
    "    \n",
    "    `loss: 7.9419 - acc: 0.3832 - val_loss: 9.5732 - val_acc: 0.3303`\n",
    "Thankfully after Epoch 9, things are looking much better.\n",
    "    \n",
    "    `loss: 0.4997 - acc: 0.8909 - val_loss: 2.2943 - val_acc: 0.6837`\n",
    "Although we're still overfitting a ton (but not as much as before: the `acc - val_acc` delta dropped from `~0.36` to `~0.21` after 9 epochs).\n",
    "\n",
    "I bet I could increase dropout again (somewhere between the current `0.25` and the original `0.5`) to reduce overfitting more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finetune_hw3_3_path = model_path+'finetune_hw3_3.h5'\n",
    "if not os.path.exists(finetune_hw3_3_path):\n",
    "    final_model.save_weights(finetune_hw3_3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissions to Kaggle\n",
    "\n",
    "I'm really interested to compare Kaggle performance between Data Augmentation + Decrease Dropout results and Add Batch Normalization results. The former had better accuracy results overall with more overfitting; the latter had less overfitting with a hit to the accuracy upper bound (although `val_acc` improved, which is likely what really matters).\n",
    "\n",
    "My guess is that Add Batch Normalization results will be the better of the two (which is not that brave of a guess, considering the contents of this week's lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm_model_2 = model\n",
    "subm_model_3 = final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches = get_batches(path+'test', shuffle=False, batch_size = batch_size)\n",
    "preds_2 = subm_model_2.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_3 = subm_model_3.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "subm_name_2 = 'subm_hmwk3_2.gz'\n",
    "subm_path_2 = path + 'results/' + subm_name_2\n",
    "\n",
    "subm_name_3 = 'subm_hmwk3_3.gz'\n",
    "subm_path_3 = path + 'results/' + subm_name_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm_2 = do_clip(preds_2, 0.9) # still unsure how to set mx\n",
    "subm_3 = do_clip(preds_3, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4126 images belonging to 10 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_81601.jpg</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_14887.jpg</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.893773</td>\n",
       "      <td>0.104057</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_62885.jpg</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_45125.jpg</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_22633.jpg</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             img        c0        c1        c2        c3        c4        c5  \\\n",
       "0  img_81601.jpg  0.011111  0.011111  0.011111  0.011111  0.011111  0.011111   \n",
       "1  img_14887.jpg  0.011111  0.011111  0.011111  0.011111  0.011111  0.011111   \n",
       "2  img_62885.jpg  0.011111  0.011111  0.011111  0.011111  0.011111  0.011111   \n",
       "3  img_45125.jpg  0.011111  0.011111  0.011111  0.011111  0.011111  0.011111   \n",
       "4  img_22633.jpg  0.011111  0.900000  0.011111  0.011111  0.011111  0.011111   \n",
       "\n",
       "         c6        c7        c8        c9  \n",
       "0  0.900000  0.011111  0.011111  0.011111  \n",
       "1  0.011111  0.893773  0.104057  0.011111  \n",
       "2  0.011111  0.011111  0.900000  0.011111  \n",
       "3  0.011111  0.011111  0.011111  0.900000  \n",
       "4  0.011111  0.011111  0.011111  0.011111  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preprocess = get_batches(path+'valid', shuffle=False, batch_size=1)\n",
    "classes = sorted(val_preprocess.class_indices, key=val_preprocess.class_indices.get)\n",
    "\n",
    "submission2 = pd.DataFrame(subm_2, columns=classes)\n",
    "submission2.insert(0, 'img', [a[8:] for a in test_batches.filenames])\n",
    "submission2.head()\n",
    "\n",
    "submission3 = pd.DataFrame(subm_3, columns=classes)\n",
    "submission3.insert(0, 'img', [a[8:] for a in test_batches.filenames])\n",
    "submission3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/hmwk/nbs/data/statefarm/results/subm_hmwk3_2.gz' target='_blank'>/home/ubuntu/hmwk/nbs/data/statefarm/results/subm_hmwk3_2.gz</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/hmwk/nbs/data/statefarm/results/subm_hmwk3_2.gz"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission2.to_csv(subm_path_2, index=False, compression='gzip')\n",
    "FileLink(subm_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/hmwk/nbs/data/statefarm/results/subm_hmwk3_3.gz' target='_blank'>/home/ubuntu/hmwk/nbs/data/statefarm/results/subm_hmwk3_3.gz</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/hmwk/nbs/data/statefarm/results/subm_hmwk3_3.gz"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission3.to_csv(subm_path_3, index=False, compression='gzip')\n",
    "FileLink(subm_path_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not going to submit to Kaggle with Jupyter anymore because it caused `OSError: [Errno 12] Cannot allocate memory` last time. But this is the command:\n",
    "\n",
    "    `kg submit $subm_path -c state-farm-distracted-driver-detection`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So! My guess appears to be wrong... but I'm pretty sure I must have a bug somewhere.\n",
    "\n",
    "`subm_hmwk3_2.gz: Private Score: 1.07994, Public Score: 1.34504`\n",
    "\n",
    "^ that's awesome! best score yet (558/1440= top 38.75%)\n",
    "\n",
    "`subm_hmwk3_3.gz: Private Score: 4.00094, Public Score: 3.98079`\n",
    "\n",
    "^ that is one of my worst scores to date.\n",
    "\n",
    "I find it hard to believe that the moral here is, Batch Normalization makes things terrible. Let's evaluate the models again to make sure I didn't accidently overwrite something and submit the wrong model for `subm_hmwk3_3.gz`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4126/4126 [==============================] - 97s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6006800007369411, 0.62990790118711182]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm_model_2.evaluate(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4126/4126 [==============================] - 97s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2942806485204286, 0.6837130393643317]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm_model_3.evaluate(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting... `subm_hmwk3_3.gz` gave me my highest validation accuracy so far, but simultaneously a pretty terrible `categorical_crossentropy` loss score. How can that be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's say you have a label [0 0 1] and your first prediction is [0 0.5 0.1]. Low accuracy and low loss. Then later you predict [0.3 0.3 0.4]. Higher loss and higher accuracy. Of course it depends on how you calculate loss.. - [forums](http://forums.fast.ai/t/kaggle-intel-mobileodt-cervical-cancer-screening-competition/2355/28)\n",
    "\n",
    "This makes sense... meanwhile, my training loss is so low! Perhaps even though it doesn't look like my accuracy is overfitting as badly, my loss is telling another story. If I want to improve this, I'd like to mess with dropout some more, I think.\n",
    "\n",
    "When I stopped training `final_model` last, my loss was still trending down. Let's run a few more epochs just to see if that continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "18298/18298 [==============================] - 557s - loss: 0.4191 - acc: 0.9042 - val_loss: 2.4053 - val_acc: 0.6898\n",
      "Epoch 2/2\n",
      "18298/18298 [==============================] - 556s - loss: 0.3848 - acc: 0.9105 - val_loss: 2.3611 - val_acc: 0.6701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f944a0b2d90>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit_generator(trn_batches, samples_per_epoch=trn_batches.nb_sample, nb_epoch=2, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "18298/18298 [==============================] - 557s - loss: 0.3127 - acc: 0.9243 - val_loss: 2.2028 - val_acc: 0.6983\n",
      "Epoch 2/2\n",
      "  640/18298 [>.............................] - ETA: 443s - loss: 0.2279 - acc: 0.9453"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-5b4621fdfee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m final_model.fit_generator(trn_batches, samples_per_epoch=trn_batches.nb_sample, nb_epoch=2, \n\u001b[0;32m----> 3\u001b[0;31m                         validation_data=val_batches, nb_val_samples=val_batches.nb_sample)\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1442\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1444\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0m_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "final_model.optimizer.lr = 0.01\n",
    "final_model.fit_generator(trn_batches, samples_per_epoch=trn_batches.nb_sample, nb_epoch=2, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
