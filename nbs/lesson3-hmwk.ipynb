{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Farm Distracted Driver Detection: Take 3\n",
    "_Can computer vision spot distracted drivers?_\n",
    "\n",
    "---\n",
    "\n",
    "## Lesson 3 Homework Assignment\n",
    "\n",
    "Dataset: https://www.kaggle.com/c/state-farm-distracted-driver-detection\n",
    "\n",
    "### Dealing with Overfitting\n",
    "In [`lesson2-hmwk.ipynb`](https://github.com/iconix/fast.ai/blob/master/nbs/lesson2-hmwk.ipynb), my final results (after 5 epochs) were as follows:\n",
    "> `loss: 0.6260 - acc: 0.7907 - val_loss: 1.6719 - val_acc: 0.4978`\n",
    "\n",
    "When `val_acc >> acc`, this is a clear indicator of overfitting on the training data.\n",
    "\n",
    "On the bright side, this means that my neural net architecture is complex enough to model the data. The next step is to generalize my architecture a bit more.\n",
    "\n",
    "Here is the prioritized list of approaches to reducing overfitting provided during class:\n",
    "1. Add more data\n",
    "2. Use data augmentation\n",
    "3. Use architectures that generalize well\n",
    "4. Add regularization (dropout, L2/L1 regularization)\n",
    "5. Reduce architecture complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and creating the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, I am skipping the download, split, and create of training, validation, test, and sample datasets, relying instead on the data split from last week. See [`lesson2-hmwk.ipynb`](https://github.com/iconix/fast.ai/blob/master/nbs/lesson2-hmwk.ipynb) for those steps, if needed.\n",
    "\n",
    "Additionally, I will be starting with the weights from last week below (`finetune2.h5`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "LESSON_HOME_DIR = current_dir\n",
    "DATA_HOME_DIR = current_dir + '/data/statefarm/'\n",
    "\n",
    "# point to your training images\n",
    "train_dir = DATA_HOME_DIR + 'train'\n",
    "\n",
    "# point to the 'driver_imgs_list.csv'\n",
    "lookup = DATA_HOME_DIR + 'driver_imgs_list.csv'\n",
    "\n",
    "# point to the validation directory, which will be created in the next block\n",
    "val_dir = DATA_HOME_DIR + 'valid'\n",
    "\n",
    "sample_dir = DATA_HOME_DIR + 'sample'\n",
    "\n",
    "test_dir = DATA_HOME_DIR + 'test'\n",
    "\n",
    "#path = DATA_HOME_DIR + 'sample/'\n",
    "path = DATA_HOME_DIR\n",
    "model_path = path + 'models/'\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import utils; reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_week2_model():\n",
    "    model = vgg_ft(10)\n",
    "    finetune2_path = model_path+'finetune2.h5'\n",
    "    model.load_weights(finetune2_path);\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = get_week2_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_3 (Lambda)                (None, 3, 224, 224)   0           lambda_input_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_27 (ZeroPadding2D) (None, 3, 226, 226)   0           lambda_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_27 (Convolution2D) (None, 64, 224, 224)  0           zeropadding2d_27[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_28 (ZeroPadding2D) (None, 64, 226, 226)  0           convolution2d_27[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_28 (Convolution2D) (None, 64, 224, 224)  0           zeropadding2d_28[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_11 (MaxPooling2D)   (None, 64, 112, 112)  0           convolution2d_28[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_29 (ZeroPadding2D) (None, 64, 114, 114)  0           maxpooling2d_11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_29 (Convolution2D) (None, 128, 112, 112) 0           zeropadding2d_29[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_30 (ZeroPadding2D) (None, 128, 114, 114) 0           convolution2d_29[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_30 (Convolution2D) (None, 128, 112, 112) 0           zeropadding2d_30[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_12 (MaxPooling2D)   (None, 128, 56, 56)   0           convolution2d_30[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_31 (ZeroPadding2D) (None, 128, 58, 58)   0           maxpooling2d_12[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_31 (Convolution2D) (None, 256, 56, 56)   0           zeropadding2d_31[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_32 (ZeroPadding2D) (None, 256, 58, 58)   0           convolution2d_31[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_32 (Convolution2D) (None, 256, 56, 56)   0           zeropadding2d_32[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_33 (ZeroPadding2D) (None, 256, 58, 58)   0           convolution2d_32[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_33 (Convolution2D) (None, 256, 56, 56)   0           zeropadding2d_33[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_13 (MaxPooling2D)   (None, 256, 28, 28)   0           convolution2d_33[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_34 (ZeroPadding2D) (None, 256, 30, 30)   0           maxpooling2d_13[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_34 (Convolution2D) (None, 512, 28, 28)   0           zeropadding2d_34[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_35 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_34[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_35 (Convolution2D) (None, 512, 28, 28)   0           zeropadding2d_35[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_36 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_35[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_36 (Convolution2D) (None, 512, 28, 28)   0           zeropadding2d_36[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_14 (MaxPooling2D)   (None, 512, 14, 14)   0           convolution2d_36[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_37 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_14[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_37 (Convolution2D) (None, 512, 14, 14)   0           zeropadding2d_37[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_38 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_37[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_38 (Convolution2D) (None, 512, 14, 14)   0           zeropadding2d_38[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_39 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_38[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_39 (Convolution2D) (None, 512, 14, 14)   0           zeropadding2d_39[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_15 (MaxPooling2D)   (None, 512, 7, 7)     0           convolution2d_39[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 25088)         0           maxpooling2d_15[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 4096)          0           flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 4096)          0           dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 4096)          0           dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 4096)          0           dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 10)            40970       dropout_6[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 40970\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                    if type(layer) is Convolution2D][-1]\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "# Dense layers - also known as fully connected or 'FC' layers\n",
    "fc_layers = layers[last_conv_idx+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a generator that includes data augmentation (convenient feature of Keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator(rotation_range=10, width_shift_range=0.1, \n",
    "       height_shift_range=0.1, shear_range=0.15, zoom_range=0.1, \n",
    "       channel_shift_range=10., horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18298 images belonging to 10 classes.\n",
      "Found 4126 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "trn_batches = get_batches(path+'train', gen, batch_size=batch_size)\n",
    "# NB: We don't want to augment or shuffle the validation set\n",
    "val_batches = get_batches(path+'valid', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeremy's [explanation](http://forums.fast.ai/t/lesson-3-discussion/186/33) as to why we aren't training the convolutional layers here:\n",
    "> The early layers are so general (e.g. remember Zeiler's visualizations - layer 1 just finds edges and gradients) that it's extremely unlikely that you'll need to change them, unless you're looking at very different kinds of images. e.g. if you're classifying line drawings, instead of photos, you'll probably need to retrain many conv layers too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for layer in layers[:last_conv_idx+1]: layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Updating slowly because it is finely tuned\n",
    "K.set_value(model.optimizer.lr, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit_generator(trn_batches, samples_per_epoch=trn_batches.nb_sample, nb_epoch=8, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 8 epochs, we are overfitting much much less, which is great!\n",
    "> `loss: 1.9278 - acc: 0.5002 - val_loss: 1.7463 - val_acc: 0.5099`\n",
    "\n",
    "In fact, now we're underfitting very slightly. This seems like a good time to try decreasing dropout a smidge, just to see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our intermediate weights first, just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finetune_hw3_1_path = model_path+'finetune_hw3_1.h5'\n",
    "if not os.path.exists(finetune_hw3_1_path):\n",
    "    model.save_weights(finetune_hw3_1_path)\n",
    "model.load_weights(finetune_hw3_1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intermediate Kaggle submission\n",
    "\n",
    "Interestingly, despite the significant decrease in overfitting, `val_acc` only improved a small amount from `0.4978`... I'd like to try submitting these results to Kaggle, just to see how this result reflects in the rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4126 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "subm_name = 'subm_hmwk3_1.gz'\n",
    "subm_path = path + 'results/' + subm_name\n",
    "\n",
    "#Get the classes from the validation batch\n",
    "val_preprocess = get_batches(path+'valid', shuffle=False, batch_size=1)\n",
    "classes = sorted(val_preprocess.class_indices, key=val_preprocess.class_indices.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on `.predict` vs `.predict_generator`: \"the precomputed data will be large. Then it is likely that you will encounter errors such as OOM or **kernel death** during training. In this case, you might want to use model.fit_generator() instead of model.fit()\" ([source](http://forums.fast.ai/t/fine-tuning-vgg-taking-very-long/3825/11)) - I kept running into kernel death with `.predict`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches = get_batches(path+'test', shuffle=False, batch_size = batch_size)\n",
    "preds = model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (number of classes - 1) = 9 (http://forums.fast.ai/t/moving-up-the-ncfm-leaderboard-by-100-positions-do-clip/1035/6)\n",
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/9, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = do_clip(preds, 0.9) # still unsure how to set mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_81601.jpg</td>\n",
       "      <td>0.011417</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.463585</td>\n",
       "      <td>0.020937</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.045846</td>\n",
       "      <td>0.443723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_14887.jpg</td>\n",
       "      <td>0.346917</td>\n",
       "      <td>0.121693</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.098112</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.061683</td>\n",
       "      <td>0.047707</td>\n",
       "      <td>0.017239</td>\n",
       "      <td>0.234232</td>\n",
       "      <td>0.060724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_62885.jpg</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.423249</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.147754</td>\n",
       "      <td>0.044589</td>\n",
       "      <td>0.280264</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.083560</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_45125.jpg</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.256628</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.133316</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.593049</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_22633.jpg</td>\n",
       "      <td>0.031959</td>\n",
       "      <td>0.011320</td>\n",
       "      <td>0.084662</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.034428</td>\n",
       "      <td>0.477008</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.323893</td>\n",
       "      <td>0.026945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             img        c0        c1        c2        c3        c4        c5  \\\n",
       "0  img_81601.jpg  0.011417  0.011111  0.011111  0.011111  0.011111  0.463585   \n",
       "1  img_14887.jpg  0.346917  0.121693  0.011111  0.098112  0.011111  0.061683   \n",
       "2  img_62885.jpg  0.011111  0.011111  0.423249  0.015064  0.147754  0.044589   \n",
       "3  img_45125.jpg  0.011111  0.011111  0.256628  0.011111  0.011111  0.011111   \n",
       "4  img_22633.jpg  0.031959  0.011320  0.084662  0.011111  0.011111  0.034428   \n",
       "\n",
       "         c6        c7        c8        c9  \n",
       "0  0.020937  0.011111  0.045846  0.443723  \n",
       "1  0.047707  0.017239  0.234232  0.060724  \n",
       "2  0.280264  0.011111  0.083560  0.011111  \n",
       "3  0.133316  0.011111  0.593049  0.011111  \n",
       "4  0.477008  0.011111  0.323893  0.026945  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'img', [a[8:] for a in test_batches.filenames])\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/hmwk/nbs/data/statefarm/results/subm_hmwk3_1.gz' target='_blank'>/home/ubuntu/hmwk/nbs/data/statefarm/results/subm_hmwk3_1.gz</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/hmwk/nbs/data/statefarm/results/subm_hmwk3_1.gz"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.to_csv(subm_path, index=False, compression='gzip')\n",
    "FileLink(subm_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Private Score: `1.41527` ... comparable to my week 2 results.\n",
    "\n",
    "And interestingly, weeks ago, my Public Score tended to be the lower/better score, or at least comparable; now, it is `1.48239` in comparison. See [this Quora post](https://www.quora.com/What-is-the-difference-between-public-and-private-leaderboard-in-Kaggle/answer/Giuliano-Janson) for an explanation:\n",
    "> The public LB is computed on a portion of the test set, the private is computed on the remainder of the test set (not the whole test set).\n",
    "\"Fitting the LB\" is a Kaggle term used to describe when you're tuning your models to perform well on the public LB. There is an art and a science in doing so and experience Kagglers are able to make the most out of it without overfitting. If not done well, that usually lends itself to worse scores on the private LB, sometimes disasters. In general the key is to build a model that generalizes well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation + Decrease Dropout\n",
    "We're going to experiment with decreasing the dropout rate from Vgg16's 50% to 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# will start again with conv_layers from last week's model\n",
    "model = get_week2_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_fc_model(fcl, conv_input_shape):\n",
    "    m = Sequential([\n",
    "        MaxPooling2D(input_shape=conv_input_shape),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    for l1,l2 in zip(m.layers, fcl): l1.set_weights(l2.get_weights())\n",
    "\n",
    "    m.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                    if type(layer) is Convolution2D][-1]\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "fc_layers = layers[last_conv_idx+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)\n",
    "fc_model = get_fc_model(fc_layers, conv_layers[-1].output_shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the new `fc_model` with less dropout to the original `conv_model` from last week..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in conv_model.layers: layer.trainable=False\n",
    "conv_model.add(fc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then compile the new combined model and fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model.compile(optimizer=Adam(0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "18298/18298 [==============================] - 549s - loss: 0.5896 - acc: 0.8057 - val_loss: 1.5074 - val_acc: 0.5880\n",
      "Epoch 2/8\n",
      "18298/18298 [==============================] - 549s - loss: 0.2387 - acc: 0.9200 - val_loss: 1.3920 - val_acc: 0.6156\n",
      "Epoch 3/8\n",
      "18298/18298 [==============================] - 549s - loss: 0.1575 - acc: 0.9501 - val_loss: 1.2786 - val_acc: 0.6512\n",
      "Epoch 4/8\n",
      "18298/18298 [==============================] - 550s - loss: 0.1248 - acc: 0.9601 - val_loss: 1.4503 - val_acc: 0.6006\n",
      "Epoch 5/8\n",
      "18298/18298 [==============================] - 549s - loss: 0.0931 - acc: 0.9692 - val_loss: 1.5553 - val_acc: 0.6156\n",
      "Epoch 6/8\n",
      "18298/18298 [==============================] - 550s - loss: 0.0815 - acc: 0.9750 - val_loss: 1.2619 - val_acc: 0.6854\n",
      "Epoch 7/8\n",
      "18298/18298 [==============================] - 550s - loss: 0.0675 - acc: 0.9787 - val_loss: 1.4714 - val_acc: 0.6432\n",
      "Epoch 8/8\n",
      "18298/18298 [==============================] - 550s - loss: 0.0599 - acc: 0.9797 - val_loss: 1.4871 - val_acc: 0.6333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6651faafd0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.fit_generator(trn_batches, samples_per_epoch=trn_batches.nb_sample, nb_epoch=8, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "18298/18298 [==============================] - 550s - loss: 0.0554 - acc: 0.9819 - val_loss: 1.3206 - val_acc: 0.7038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f66505615d0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.optimizer.lr = 0.001 # back to default learning rate\n",
    "conv_model.fit_generator(trn_batches, samples_per_epoch=trn_batches.nb_sample, nb_epoch=1, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drumroll...\n",
    "\n",
    "    `loss: 0.0554 - acc: 0.9819 - val_loss: 1.3206 - val_acc: 0.7038`\n",
    "\n",
    "Wow, back to massive overfitting. But I was expecting this since I reduced dropout in a model that was already pretty well balanced in fit thanks to data augmentation in the last section.\n",
    "\n",
    "Fortunately as I was also hoping, there has been a _huge_ increase in the upper bounds of both `acc` and `val_acc` - dropout was stunting the model's learning ability! So I'm hoping this means that my next final attempt at reducing overfitting will work again, but with much higher accuracies than the `~0.5` I was seeing earlier.\n",
    "\n",
    "Let's save weights again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finetune_hw3_2_path = model_path+'finetune_hw3_2.h5'\n",
    "if not os.path.exists(finetune_hw3_2_path):\n",
    "    conv_model.save_weights(finetune_hw3_2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start with the model just trained above\n",
    "model = vgg_ft(10)\n",
    "model.load_weights(finetune_hw3_2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm now going to assume that the convolution layers are well-trained enough that I can pre-calculate them. This means that Data Augmentation will not affect the conv layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_classes = val_batches.classes\n",
    "trn_classes = trn_batches.classes\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_features = conv_model.predict_generator(batches, batches.nb_sample)\n",
    "val_features = conv_model.predict_generator(val_batches, val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_convlayer_feats_path = model_path+'hmwk3_train_convlayer_features.bc'\n",
    "if not os.path.exists(train_convlayer_feats_path):\n",
    "    save_array(train_convlayer_feats_path, trn_features)\n",
    "    \n",
    "valid_convlayer_feats_path = model_path+'hmwk3_valid_convlayer_features.bc'\n",
    "if not os.path.exists(valid_convlayer_feats_path):\n",
    "    save_array(valid_convlayer_feats_path, val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(1000, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_fc_weights_from_vgg16bn(m):\n",
    "    \"Load weights for model from the dense layers of the Vgg16BN model.\"\n",
    "    # See imagenet_batchnorm.ipynb for info on how the weights for\n",
    "    # Vgg16BN can be generated from the standard Vgg16 weights.\n",
    "    from vgg16bn import Vgg16BN\n",
    "    vgg16_bn = Vgg16BN()\n",
    "    _, fcl = split_at(vgg16_bn.model, Convolution2D)\n",
    "    copy_weights(fcl, m.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_fc_model = Sequential(get_bn_layers(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_fc_weights_from_vgg16bn(bn_fc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_fc_model.pop()\n",
    "for layer in bn_fc_model.layers: layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_fc_model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_fc_model.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_fc_model.fit(trn_features, trn_labels, nb_epoch=8, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finetune_hw3_3_path = model_path+'finetune_hw3_3.h5'\n",
    "if not os.path.exists(finetune_hw3_3_path):\n",
    "    bc_fc_model.save_weights(finetune_hw3_3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model = Sequential(conv_layers)\n",
    "for layer in final_model.layers: layer.trainable = False\n",
    "final_model.add(bn_fc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
